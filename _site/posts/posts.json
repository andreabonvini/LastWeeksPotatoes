[
  {
    "path": "posts/2021-07-20-support-vector-machines/",
    "title": "Support Vector Machines",
    "description": "The math behind the Support Vector Machines algorithm.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-07-20",
    "categories": [
      "Machine Learning",
      "Classification"
    ],
    "contents": "\nIntroduction & Brief History\nIn this blog-post we are gonna talk about one of the most powerful and fascinating techniques in Machine Learning: Support Vector Machines.\nIn the field of Statistical Learning the Support Vector Machine technique is a binary classification algorithm which aims to find the hyperplane which is able to separate the data with the largest margin possible. The concept of margin is illustrated in the following images.\nSuppose we have a set of points in \\(\\mathbb{R}^2\\), each point belongs to a class \\(\\in\\{-1,+1\\}\\)\n\n\nWe want to find the best hyperplane (in this case a line) which is able to correctly separate the data.\n\n\nWe identify this hyperplane by maximizing the margin, i.e. the distance from the hyperplane to the closest points of both classes, we call this points support vectors.\n\n\nIn this case we identified two support vectors, they are called like that because they support the dashed lines, which represent the set of points equidistant from the separating hyperplane.\n\n\nThe margins from the support vectors to the hyperplane are drawed in red\n\n\nBefore diving into the theory of the algorithm let’s have a look at the history behind it.\nThe birth of SVMs dates back to \\(1963\\) in Russia, when Vladimir Vapnik and Aleksondr Lerner introduced the Generalized Portrait algorithm.\nAfter almost \\(30\\) years, at the end of \\(1990\\), Vapnik moved to the USA and joined Bernhard Boser and Isabelle Guyen at the Adaptive Systems Research Department at AT&T Bell Labs in New Jersey, where the algorithm was refined.\n\n“The invention of SVMs happened when Bernhard decided to implement Vladimir’s algorithm in the three months we had left before we moved to Berkeley. After some initial success of the linear algorithm, Vladimir suggested introducing products of features. I proposed to rather use the kernel trick of the ‘potential function’ algorithm. Vladimir initially resisted the idea because the inventors of the ‘potential functions’ algorithm (Aizerman, Braverman, and Rozonoer) were from a competing team of his institute back in the 1960’s in Russia! But Bernhard tried it anyways, and the SVMs were born!”\n\nIsabelle Guyen\nPremise on linear classifiers\nFor a binary classification problem, one can visualize the operation of a linear classifier as splitting a high-dimensional input space of dimension \\(d\\) with an hyperplane of dimension \\(d\\) (which, as you’ll see in a minute, corresponds to a \\(d-1\\) dimensional space): all points on one side of the hyperplane are classified as \\(+1\\) (or \\(-1\\)), while the others are classified as \\(-1\\) (or \\(+1\\)). In case you doubt the power of linear classifiers just observe that we’re always able to transform (or enrich) our input space by means of some basis functions, if we “guess” the right transformation maybe we are able to correctly classify our samples with a linear classifier.\nIf, for instance, we have the following unseparable data in the 2D space\n\n\nthere’s nothing stopping us from enriching the input space with some new coordinates which depend on the old features, e.g. by adding a ne w dimension \\(x_3 = \\sqrt{x_1^2+x_2^2}\\).\nThis way, in the new 3D input space, we are able to correctly classify the data by means of a 2D plane.\nDerivation\nFirst of all, we should be familiar with the equation of a generic \\(D\\)-dimensional hyperplane:\n\\[\n\\text{hyperplane: }\\\\\n\\mathbf{w}^T\\mathbf{x}=0\\\\\n\\mathbf{w} = [w_0,w_1,\\dots,w_D]^T\\\\\n\\mathbf{x} = [1,x_1,\\dots,x_D]^T\n\\]\nIf \\(D=2\\) we have that\n\\[\n\\text{hyperplane: }\\\\\nw_0+w_1x_1+w_2x_2=0\\\\\n\\mathbf{w} = [w_0,w_1,w_2]^T\\\\\n\\mathbf{x} = [1,x_1,x_2]^T\n\\]\nLet \\(\\mathbf{x}_N\\) be the nearest data point to the hyperplane \\(\\mathbf{w}^T\\mathbf{x} = 0\\) , before finding the distance we just have to state two observations:\nLet’s say I multiply the vector \\(\\mathbf{w}\\) by \\(1000000\\) , I get the same hyperplane! So any formula that takes \\(\\mathbf{w}\\) and produces the margin will have to have built-in scale-invariance, we do that by normalizing \\(\\mathbf{w}\\) , requiring that for the nearest data point \\(\\mathbf{x}_N\\): \\[\n  |\\mathbf{w}^T\\mathbf{x}_N|=1\n  \\] ( So I just scale \\(\\mathbf{w}\\) up and down in order to fulfill the condition stated above, we just do it because it’s mathematically convenient! By the way remember that \\(1\\) does not represent the Euclidean distance)\nWhen you solve for the margin, the \\(w_1\\) to \\(w_d\\) will play a completely different role from the role of \\(w_0\\) , so it is no longer convenient to have them on the same vector. We pull out \\(w_0\\) from \\(\\mathbf{w}\\) and rename \\(w_0\\) with \\(b\\) (for bias).\n\\[\n\\mathbf{w} = (w_1,\\dots,w_d)\\\\w_0=b\n\\]\nSo now our notation is changed:\nThe hyperplane is represented by\n\\[\n\\mathbf{w}^T\\mathbf{x} +b= 0\n\\]\nand our constraint becomes\n\\[\n|\\mathbf{w}^T\\mathbf{x}_N+b|=1\n\\]\nIt’s trivial to demonstrate that the vector \\(\\mathbf{w}\\) is orthogonal to the hyperplane, just suppose to have two point \\(\\mathbf{x}'\\) and \\(\\mathbf{x''}\\) belonging to the hyperplane , then \\(\\mathbf{w}^T\\mathbf{x}' +b= 0\\) and \\(\\mathbf{w}^T\\mathbf{x}'' +b= 0\\).\nAnd of course \\(\\mathbf{w}^T\\mathbf{x}'' +b - (\\mathbf{w}^T\\mathbf{x}' +b)=\\mathbf{w}^T(\\mathbf{x}''-\\mathbf{x}') = 0\\)\nSince \\(\\mathbf{x}''-\\mathbf{x}'\\) is a vector which lays on the hyperplane , we deduce that \\(\\mathbf{w}\\) is orthogonal to the hyperplane.\n\n\nThen the distance from \\(\\mathbf{x}_N\\) to the hyperplane can be expressed as a dot product between \\(\\mathbf{x}_N-\\mathbf{x}\\) (where \\(\\mathbf{x}\\) is any point belonging to the plane) and the unit vector \\(\\hat{\\mathbf{w}}\\) where \\(\\hat{\\mathbf{w}} = \\frac{\\mathbf{w}}{\\vert\\vert\\mathbf{w}\\vert\\vert}\\)\n(the distance is just the projection of \\(\\mathbf{x}_N-\\mathbf{x}\\) in the direction of \\(\\hat{\\mathbf{w}}\\)!)\n\\[\ndistance = |\\hat{\\mathbf{w}}^T(\\mathbf{x}_N-\\mathbf{x})|\n\\]\nWe take the absolute value since we don’t know if \\(\\mathbf{w}\\) is facing \\(\\mathbf{x}_N\\) or is facing the other direction\n\n\nWe’ll now try to simplify our notion of distance.\n\\[\n\\text{distance} = |\\hat{\\mathbf{w}}^T(\\mathbf{x}_N-\\mathbf{x})\\;| = \\frac{1}{||\\mathbf{w}||}|\\;\\mathbf{w}^T\\mathbf{x}_N-\\mathbf{w}^T\\mathbf{x}|\n\\]\nThis can be simplified if we add and subtract the missing term \\(b\\).\n\\[\ndistance = \\frac{1}{||\\mathbf{w}||}|\\;\\mathbf{w}^T\\mathbf{x}_N+b-\\mathbf{w}^T\\mathbf{x}-b\\;| = \\frac{1}{||\\mathbf{w}||}|\\;\\mathbf{w}^T\\mathbf{x}_N+b-(\\mathbf{w}^T\\mathbf{x}+b)\\;|\n\\]\nWell, \\(\\mathbf{w}^T\\mathbf{x}+b\\) is just the value of the equation of the plane…for a point on the plane. So without any doubt \\(\\mathbf{w}^T\\mathbf{x}+b= 0\\) , our notion of distance becomes\n\\[\ndistance = \\frac{1}{||\\mathbf{w}||}|\\;\\mathbf{w}^T\\mathbf{x}_N+b\\;|\n\\]\nBut wait… what is \\(\\vert\\mathbf{w}^T\\mathbf{x}_N+b\\vert\\) ? It is the constraint that we defined at the beginning of our derivation!\n\\[\n\\vert\\mathbf{w}^T\\mathbf{x}_N+b\\vert=1\n\\]\nSo we end up with the formula for the distance being just\n\\[\ndistance = \\frac{1}{\\vert\\vert\\mathbf{w}\\vert\\vert}\n\\]\nLet’s now formulate the optimization problem, we have:\n\\[\n\\underset{w}{\\operatorname{max}}\\frac{1}{||\\mathbf{w}||}\\\\\\text{subject to}\\;\\underset{n=1,2,\\dots,N}{\\operatorname{min}}|\\mathbf{w}^T\\mathbf{x}_n+b|=1\n\\]\nSince this is not a friendly optimization problem (the constraint is characterized by a minimum and an absolute, which are annoying) we are going to find an equivalent problem which is easier to solve. Our optimization problem can be rewritten as\n\\[\n\\underset{w}{\\operatorname{min}}\\frac{1}{2}\\mathbf{w}^T\\mathbf{w}\n\\\\\n\\text{subject to} \\ \\ \\ \\  y_n \\cdot(\\mathbf{w}^T\\mathbf{x}_n+b)\\ge1 \\;\\;\\;\\;\\text{for $n = 1,2,\\dots,N$}\n\\]\nwhere \\(y_n\\) is a variable that we introduce that will be equal to either \\(+1\\) or \\(-1\\) accordingly to its real target value (remember that this is a supervised learning technique and we know the real target value of each sample). One could argue that the new constraint is actually different from the former one, since maybe the \\(\\mathbf{w}\\) that we’ll find will allow the constraint to be strictly greater than \\(1\\) for every possible point in our dataset [ \\(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)> 1 \\;\\;\\forall{n}\\) ] while we’d like it to be exactly equal to \\(1\\) for at least one value of \\(n\\). But that’s actually not true! Since we’re trying to minimize \\(\\frac{1}{2}\\mathbf{w}^T\\mathbf{w}\\) our algorithm will try to scale down \\(\\mathbf{w}\\) until \\(\\mathbf{w}^T\\mathbf{x}_n+b\\) will touch \\(1\\) for some specific point \\(n\\) of the dataset.\nSo how can we solve this? This is a constraint optimization problem with inequality constraints, we have to derive the Lagrangian and apply the KKT (Karush–Kuhn–Tucker) conditions.\nObjective Function:\nWe have to minimize\n\\[\n\\mathcal{L}(\\mathbf{w},b,\\mathbf{\\alpha}) = \\frac{1}{2}\\mathbf{w}^T\\mathbf{w}-\\sum_{n=1}^{N}\\alpha_n(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1)\\\\\n\\]\nw.r.t. to \\(\\mathbf{w}\\) and \\(b\\) and maximize it w.r.t. the Lagrange Multipliers \\(\\alpha_n\\)\nWe can easily get the two conditions for the unconstrained part:\n\\[\n\\nabla_{\\mathbf{w}}\\mathcal{L}=\\mathbf{w}-\\sum_{n=1}^{N}\\alpha_n y_n\\mathbf{x}_n = 0 \\;\\;\\;\\;\\;\\;\\;\\; \\mathbf{w}=\\sum_{n=1}^{N}\\alpha_n y_n\\mathbf{x}_n\\\\\n\\frac{\\partial\\mathcal{L}}{\\partial b} = -\\sum_{n=1}^{N}\\alpha_n y_n = 0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sum_{n=1}^{N}\\alpha_n y_n=0\n\\]\nAnd list the other KKT conditions:\n\\[\ny_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1\\ge0\\;\\;\\;\\;\\;\\;\\forall{n}\\\\\n\\alpha_n\\ge0\\;\\;\\;\\;\\;\\;\\;\\forall{n}\\\\\n\\alpha_n(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1)=0\\;\\;\\;\\;\\;\\;\\forall{n}\n\\]\nAlert : the last condition is called the KKT dual complementary condition and will be key for showing that the SVM has only a small number of “support vectors”, and will also give us our convergence test when we’ll talk about the SMO algorithm.\nNow we can reformulate the Lagrangian by applying some substitutions\n\\[\n\\mathcal{L}(\\mathbf{w},b,\\mathbf{\\alpha}) = \\frac{1}{2}\\mathbf{w}^T\\mathbf{w}-\\sum_{n=1}^{N}\\alpha_n(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1)\\\\\n\\mathcal{L}(\\mathbf{\\alpha}) =\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n y_m\\alpha_n\\alpha_m\\mathbf{x}_n^T\\mathbf{x}_m\n\\]\n(if you have doubts just go to minute 36.50 of this excellent lecture by professor Yaser Abu-Mostafa at Caltech )\nWe end up with the dual formulation of the problem\n\\[\n\\underset{\\alpha}{\\operatorname{argmax}}\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n y_m\\alpha_n\\alpha_m\\mathbf{x}_n^T\\mathbf{x}_m\\\\\n\\;\\\\\ns.t. \\;\\;\\;\\;\\;\\;\\;\\;\\alpha_n\\ge0\\;\\;\\;\\forall{n}\\\\\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sum_{n=1}^{N}\\alpha_n y_n=0\n\\]\nWe can notice that the old constraint \\(\\mathbf{w}=\\sum_{n=1}^{N}\\alpha_n y_n\\mathbf{x}_n\\) doesn’t appear in the new formulation since it is not a constraint on \\(\\alpha\\) , it was a constraint on \\(\\mathbf{w}\\) which is not part of our formulation anymore.\nHow do we find the solution? we throw this objective (which btw happens to be a convex function) to a quadratic programming package.\nOnce the quadratic programming package gives us back the solution we find out that a whole bunch of \\(\\alpha\\) are just \\(0\\) ! All the \\(\\alpha\\) which are not \\(0\\) are the ones associated with the so-called support vectors ! ( which are just samples from our dataset )\nThey are called support vectors because they are the vectors that determine the width of the margin , this can be noted by observing the last KKT condition\n\\[\n\\big\\{\\alpha_n(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1)=0\\;\\;\\;\\forall{n}\\big\\}\n\\]\nin fact either a constraint is active, and hence the point is a support vector, or its multiplier is zero.\nNow that we solved the problem we can get both \\(\\mathbf{w}\\) and \\(b\\).\n\\[\n\\mathbf{w} = \\sum_{\\mathbf{x}_n \\in \\text{ SV}}\\alpha_ny_n\\mathbf{x}_n\\\\\ny_n(\\mathbf{w}^T\\mathbf{x}_{n\\in\\text{SV}}+b)=1\n\\]\nwhere \\(\\mathbf{x}_{n\\in\\text{SV}}\\) is any support vector. (you’d find the same \\(b\\) for every support vector)\nBut the coolest thing about SVMs is that we can rewrite our objective functions from\n\\[\n\\mathcal{L}(\\mathbf{\\alpha}) =\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n y_m\\alpha_n\\alpha_m\\mathbf{x}_n^T\\mathbf{x}_m\n\\]\nto\n\\[\n\\mathcal{L}(\\mathbf{\\alpha}) =\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n y_m\\alpha_n\\alpha_mk(\\mathbf{x}_n,\\mathbf{x}_m)\n\\]\nWe can use kernels ! (if you don’t know what I’m talking about just check this one)\nFinally we end up with the following equation for classifying new points:\n\\[\n\\hat{y}(\\mathbf{x}) = sign\\left(\\sum_{n=1}^{N}\\alpha_n y_n k(\\mathbf{x},\\mathbf{x}_n)+b\\right)\n\\]\nSoft-margin Formulation\nThe method described so far is called hard-margin SVM since the margin has to be satisfied strictly, it can happen that the points are not linearly separable in any way, or we just want to handle noisy data to avoid overfitting, so now we’re going to briefly define another version of it, which is called soft-margin SVM that allows for few errors and penalizes for them.\nWe introduce slack variables \\(\\xi_n\\) , this way we allow to violate the margin constraint but we add a penalty expressed by the distance of the misclassified samples from the hyperplane ( samples correctly classified have \\(\\xi_n=0\\)).\nWe now have to\n\\[\n\\text{Minimize}\\ \\ ||\\mathbf{w}||_2^2+C\\sum_n \\xi_n \\\\\n\\text{s.t.}\\\\ \n\\ y_n(\\mathbf{w}^Tx_n+b)\\ge1-\\xi_n\\ ,\\ \\ \\ \\forall{n}\\\\\n\\xi_n\\ge0\\ ,\\ \\ \\ \\forall{n}\n\\]\n\\(C\\) is a coefficient that allows to trade-off bias-variance and is chosen by cross-validation.\nAnd obtain the Dual Representation\n\\[\n\\text{Maximize}\\ \\ \\ \\mathcal{L}(\\mathbf{\\alpha}) =\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n y_m\\alpha_n\\alpha_mk(\\mathbf{x}_n\\mathbf{x}_m)\\\\\n  \\text{s.t.}\\\\\n  0\\le\\alpha_n\\le C\\ \\ \\ \\ \\ \\forall{n}\\\\\n  \\sum_{n=1}^N\\alpha_n y_n = 0\n\\]\nif \\(\\alpha_n\\le0\\) the point \\(x_n\\) is just correctly classified.\nif \\(0<\\alpha_n<C\\) the points lies on the margin. They are indeed Support Vectors.\nif \\(\\alpha_n = C\\) the point lies inside the margin, and it can be either correctly classified (\\(\\xi_n \\le 1\\)) or misclassified (\\(\\xi_n>1\\))\nFun fact: When \\(C\\) is large, larger slacks penalize the objective function of SVM’s more than when \\(C\\) is small. As \\(C\\) approaches infinity, this means that having any slack variable set to non-zero would have infinite penalty. Consequently, as \\(C\\) approaches infinity, all slack variables are set to \\(0\\) and we end up with a hard-margin SVM classifier.\nError bounds\nAnd what about generalization? Can we compute an Error bound in order to see if our model is overfitting?\nAs Vapnik said :\n\n“In the support-vectors learning algorithm the complexity of the construction does not depend on the dimensionality of the feature space, but on the number of support vectors.”\n\nIt’s reasonable to define an upper bound of the error as:\n\\[\nL_h\\le\\frac{\\mathbb{E}[\\text{number of support vectors}]}{N}\n\\]\nWhere \\(N\\) is the total number of samples in the dataset. The good thing is that this bound can be easily computed and we don’t need to run SVM multiple times.\n\n\n\n",
    "preview": "posts/2021-07-20-support-vector-machines/media/outer_points2D-white.png",
    "last_modified": "2021-07-22T11:57:31+02:00",
    "input_file": {},
    "preview_width": 1675,
    "preview_height": 1662
  },
  {
    "path": "posts/2021-07-20-the-hazard-function/",
    "title": "The Hazard Function",
    "description": "What is the Hazard Function in the context of Survival Analysis? How can we apply it in Computational Neuroscience?",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-07-10",
    "categories": [
      "Computational Neuroscience"
    ],
    "contents": "\nIn Survival Analysis we are interested in understanding the risk of an event happening at a particular point in time, where time is a continuous variable.\nFor example, let’s consider the event firing of a neuron: we define the time of firing as \\(X\\), and time in general as \\(t\\).\nThe hazard function, which is a function of time, is defined as:\n\\[\nh(t) = \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t|X>t)}{\\Delta t}\n\\]\nWe are conditioning on \\(X>t\\) because we want to condition our probability on the fact that the event hasn’t occurred yet.\nIs there a way to rewrite \\(h(t)\\) in a different way?\n\\[\nh(t) = \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t|X>t)}{\\Delta t}\\\\\nh(t) = \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t,X>t)}{P(X>t)\\Delta t}\\\\\n\\]\nIt is easy to see that \\((t<X<t+\\Delta t)\\) is just a subset of \\(X>t\\)\n    O---------------------- {     X > t    }\n    |       o-------------- { t < X < t+Δt }\n    |       |       \n----.-------.----.---------\n    t       X   t+Δt\n\\[\n  h(t) = \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t)}{P(X>t)\\Delta t}\n\\]\n\\(P(X>t)\\) is called the survival function and is just \\(1\\) minus the cumulative distribution function (CDF):\n\\[\n  P(X>t) = 1-F(t)=1-\\int_{t_0}^tp(t)dt\n\\]\nThe remaining part is the definition of the derivative of the CDF, which is just the probability density function (PDF) at time \\(t\\)\n\\[\n  \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t)}{\\Delta t}= \\lim_{\\Delta t\\to0}\\frac{P(X<t+\\Delta t)-P(X <t)}{\\Delta t}=\\\\\n  \\lim_{\\Delta t\\to0}\\frac{F(t+\\Delta t)-F(t)}{\\Delta t}=p(t)\n\\]\nSo, finally we can rewrite the hazard function as:\n\\[\n  h(t) = \\frac{p(t)}{1-\\int_{t_0}^tp(t)dt}\n\\]\n\n\n\n",
    "preview": "posts/2021-07-20-the-hazard-function/neuron.jpg",
    "last_modified": "2021-07-22T14:35:33+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-22-a-quick-overview-of-logistic-regression/",
    "title": "A quick overview of Logistic Regression.",
    "description": "A pretty basic technique for binary classification.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-05-21",
    "categories": [
      "Machine Learning",
      "Classification"
    ],
    "contents": "\nAlthough the name might confuse, please note that this is a classiﬁcation algorithm.\nIn Logistic Regression, we define a set weights \\(\\mathbf{w}\\) that should be combined (through a trivial dot product) with some features \\(\\phi\\). Considering a problem of two-class classiﬁcation, the posterior probability of class \\(C_1\\) can be written as a logistic sigmoid function:\n\\[\np(C_1\\vert\\phi) = \\frac{1}{1+e^{-\\mathbf{w}^T\\phi}}=\\sigma(\\mathbf{w}^T\\phi)\n\\]\n\nand \\(p(C_2\\vert\\phi) = 1 - p(C_1\\vert\\phi)\\)\nApplying the Maximum Likelihood approach…\nGiven a dataset \\(\\mathcal{D} = \\{(\\mathbf{\\phi}_n,t_n)\\ \\forall n\\in[1,N]\\}\\), \\(t_n \\in \\{0,1\\}\\), we have to maximize the probability of getting the right label:\n\\[\nP(\\mathbf{t}\\vert\\mathbf{\\Phi},\\mathbf{w}) = \\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n},\\ \\ y_n = \\sigma(\\mathbf{w}^T\\phi_n)\n\\]\nTaking the negative log of the likelihood, the cross-entropy error function can be deﬁned and it has to be minimized:\n\\[\nL(\\mathbf{w}) = -\\ln P(\\mathbf{t}\\vert\\mathbf{\\Phi},\\mathbf{w}) = -\\sum_{n=1}^{N}(t_n\\ln y_n+(1-t_n)\\ln(1-y_n))=\\sum_{n}^NL_n\n\\]\nDifferentiating and using the chain rule:\n\\[\n\\frac{\\partial L_n}{\\partial y_n}= \\frac{y_n-t_n}{y_n(1-y_n)},\\ \\ \\ \\ \\frac{\\partial y_n}{\\partial\\mathbf{w}}=y_n(1-y_n)\\phi_n\\\\\n\\frac{\\partial L_n}{\\partial \\mathbf{w}}= \\frac{\\partial L_n}{\\partial y_n}\\frac{\\partial y_n}{\\partial\\mathbf{w}}=(y_n-t_n)\\phi\n\\]\nThe gradient of the loss function is\n\\[\n\\nabla L(\\mathbf{w}) = \\sum_{n=1}^{N}(y_n-t_n)\\phi_n\n\\]\nIt has the same form as the gradient of the sum-of-squares error function for linear regression. But in this case \\(y\\) is not a linear function of \\(\\mathbf{w}\\) and so, there is no closed form solution. The error function is convex (only one optimum) and can be optimized by standard gradient-based optimization techniques. It is, hence, easy to adapt to the online learning setting.\nTalking about Multiclass Logistic Regression…\nFor the multiclass case, the posterior probabilities can be represented by a softmax transformation of linear functions of feature variables:\n\\[\np(C_k\\vert\\phi)=y_k(\\phi)=\\frac{e^{\\mathbf{w}_k^T\\phi}}{\\sum_j e^{\\mathbf{w}_j^T\\phi}}\n\\]\n\\(\\phi(\\mathbf{x})\\) has been abbreviated with \\(\\phi\\) for simplicity.\nMaximum Likelihood is used to directly determine the parameters\n\\[\np(\\mathbf{T}\\vert\\Phi,\\mathbf{w}_1,\\dots,\\mathbf{w}_K)=\\prod_{n=1}^{N}{\\underset{\\text{Term for correct class$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\,\\,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$}}{\\underbrace{\\left(\\prod_{k=1}^{K}p(C_k\\vert\\phi_n)^{t_{nk}}\\right)}=\\prod_{n=1}^{N}\\left(\\prod_{k=1}^{K}y_{nk}^{t_{nk}}\\right)}}\\\\\n\\]\nwhere \\(y_{nk}=p(C_k\\vert\\phi_n)=\\frac{e^{\\mathbf{w}_k^T\\phi_n}}{\\sum_j e^{\\mathbf{w}_j^T\\phi_n}}\\)\nThe cross-entropy function is:\n\\[\nL(\\mathbf{w}_1,\\dots,\\mathbf{w}_K)=-\\ln p(\\mathbf{T}\\vert\\Phi,\\mathbf{w}_1,\\dots,\\mathbf{w}_K)=-\\sum_{n=1}^{N}\\left(\\sum_{k=1}^{K}t_{nk}\\ln y_{nk}\\right)\n\\]\nTaking the gradient\n\\[\n\\nabla L_{\\mathbf{w}_j}(\\mathbf{w}_1,\\dots,\\mathbf{w}_K) =\\sum_{n=1}^{N}(y_{nj}-t_{nj})\\phi_n\n\\]\n\n\n\n",
    "preview": "posts/2021-07-22-a-quick-overview-of-logistic-regression/media/sigmoid.png",
    "last_modified": "2021-07-22T14:18:04+02:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 898
  },
  {
    "path": "posts/2021-07-22-the-kernel-trick/",
    "title": "The Kernel Trick.",
    "description": "What is the kernel trick? What's the main advantage of this technique?",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-05-18",
    "categories": [
      "Machine Learning"
    ],
    "contents": "\nTraditionally, theory and algorithms of machine learning and statistics have been very well developed for the linear case. Real world data analysis problems, on the other hand, often require nonlinear methods to detect the kind of dependencies that allow successful prediction of properties of interest. By using a positive definite kernel, one can sometimes have the best of both worlds. The kernel corresponds to a dot product in a usually high-dimensional (possibly infinite) feature space. In this space, our estimation methods are linear, but as long as we can formulate everything in terms of kernel evaluations, we never explicitly have to compute in the high dimensional feature space! (this is called the Kernel Trick)\nSuppose we have a mapping \\(\\varphi : \\mathbb{R}^d \\to \\mathbb{R}^m\\) that brings our vectors in to some feature space \\(\\mathbb{R}^m\\). Then the dot product of \\(\\textbf{x}\\) and \\(\\textbf{y}\\) in this space is \\(\\varphi (\\textbf{x})^T\\varphi (\\textbf{y})\\).\nA kernel is a function \\(k\\) that corresponds to this dot product, i.e. $k(,)=()^T() $ .\nWhy is this useful? Kernels give a way to compute dot products in some feature space without even knowing what this space is and what is \\(\\varphi\\) .\nFor example, consider a simple polynomial kernel \\(k(\\textbf{x},\\textbf{y})=(1+\\textbf{x}^T\\textbf{y})^2\\) with \\(\\textbf{x},\\textbf{y} \\in \\mathbb{R}^2\\).\nThis doesn’t seem to correspond to any mapping function \\(\\varphi\\) , it’s just a function that returns a real number. Assuming that \\(\\textbf{x} = (x_1,x_2)\\) and \\(\\textbf{y} = (y_1,y_2)\\), let’s expand this expression:\n\\[\nk(\\textbf{x},\\textbf{y})=(1+\\textbf{x}^T\\textbf{y})^2 = (1+x_1y_1 + x_2y_2)^2=\\\\1+x_1^2y_1^2+x_2^2y_2^2+2x_1y_1+2x_2y_2+2x_1x_2y_1y_2\n\\]\nNote that this is nothing else but a dot product between two vectors:\n\\[\\varphi(\\mathbf x) = \\varphi(x_1, x_2) = (1, x_1^2, x_2^2, \\sqrt{2} x_1, \\sqrt{2} x_2, \\sqrt{2} x_1 x_2)\\]\nand\n\\[\\varphi(\\mathbf y) = \\varphi(y_1, y_2) = (1, y_1^2, y_2^2, \\sqrt{2} y_1, \\sqrt{2} y_2, \\sqrt{2} y_1 y_2)\\]\nSo the kernel \\(k(\\mathbf x, \\mathbf y) = (1 + \\mathbf x^T \\mathbf y)^2 = \\varphi(\\mathbf x)^T \\varphi(\\mathbf y)\\) computes a dot product in a 6-dimensional space without explicitly visiting this space.\nAnother example is the Gaussian kernel \\(k(\\mathbf x, \\mathbf y) = \\exp\\big(- \\gamma \\, \\|\\mathbf x - \\mathbf y\\|^2 \\big)\\). If we Taylor-expand this function, we’ll see that it corresponds to an infinite-dimensional codomain of \\(\\varphi\\).\nInstead, the simplest kernel is the linear kernel which corresponds to an identity mapping in the feature space: \\(k(\\mathbf{x},\\mathbf{x'}) = \\varphi(\\mathbf{x})^T\\varphi(\\mathbf{x'}) = \\mathbf{x}^T\\mathbf{x}\\)\nMoreover, the kernel is a symmetric function of its arguments: \\(k(\\mathbf{x},\\mathbf{x'}) = k(\\mathbf{x'},\\mathbf{x})\\)\nMany linear models for regression and classiﬁcation can be reformulated in terms of dual representation in which the kernel function arises naturally ! For example if we consider a linear ridge regression model we know that we obtain the best parameters by minimizing the regularized sum of squares error function (ridge):\n\\[\nL_{\\mathbf{w}} = \\frac{1}{2}\\sum_{n=1}^{N}(\\mathbf{w}^T\\varphi(\\mathbf{x_n})-t_n)^2+\\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}=\\\\\n\\frac{1}{2}\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)^T\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)+\\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}\n\\]\nWhere \\(\\Phi\\) is the design matrix whose \\(n^{th}\\) row is \\(\\varphi(\\mathbf{x}_n)^T\\) (remember that in \\(L_{\\mathbf{w}}\\) all the vectors are column vectors) and \\(\\mathbf{t} = (t_1,...,t_N)^T\\) is the target vector.\nSetting the gradient of \\(L_{\\mathbf{w}}\\) w.r.t. \\(\\mathbf{w}\\) equal to \\(0\\) we obtain the following:\n\\[\n\\frac{\\partial L_\\mathbf{w}}{\\partial \\mathbf{w}}=0\\\\\n\\frac{\\partial \\left(\\frac{1}{2}\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)^T\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)+\\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}\\right)}{\\partial \\mathbf{w}}=0\\\\\n\\mathbf{\\Phi}^T\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)+\\lambda\\mathbf{w} = 0\\\\\n\\mathbf{w} = -\\frac{1}{\\lambda}\\Phi^T\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)\\\\\n\\mathbf{w} = \\Phi^T\\mathbf{a}\n\\]\nWhere \\(\\mathbf{a}=-\\frac{1}{\\lambda}\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)\\) is a \\(N\\times 1\\) vector.\nWe observe that the coefficients \\(a_n\\) are functions of \\(\\mathbf{w}\\). So our definition of \\(\\mathbf{w}\\) is function of \\(\\mathbf{w}\\) itself…which is surely weird, just wait for it…\nWe now define the Gram Matrix \\(\\mathbf{K} = \\Phi \\times \\Phi^T\\), an \\(N \\times N\\) matrix, with elements:\n\\[\nK_{nm} = \\varphi(\\mathbf{x_n})^T\\varphi(\\mathbf{x_m})=k(\\mathbf{x}_n,\\mathbf{x}_m)\n\\]\nSo, given \\(N\\) samples, the Gram Matrix is the matrix of all inner products\n\\[\nK =\n\\begin{bmatrix}\nk(\\mathbf{x}_1,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_1,\\mathbf{x}_N) \\\\\n\\vdots &\\ddots & \\vdots\\\\\nk(\\mathbf{x}_N,\\mathbf{x}_1)  & \\dots&  k(\\mathbf{x}_N,\\mathbf{x}_N) \n\\end{bmatrix}\n\\]\nThis will come in handy in a few seconds…\nIf we substitute \\(\\mathbf{w} = \\Phi^T\\mathbf{a}\\) into \\(L_{\\mathbf{w}}\\) we get\n\\[\nL_{\\mathbf{w}} =\n\\frac{1}{2}\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)^T\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)+\\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}\n\\]\n\\[\nL_{\\mathbf{w}} =\n\\frac{1}{2}\\left(\\mathbf{\\Phi\\Phi^T\\mathbf{a}}-\\mathbf{t}\\right)^T\\left(\\mathbf{\\Phi\\Phi^T\\mathbf{a}}-\\mathbf{t}\\right)+\\frac{\\lambda}{2}\\left(\\Phi^T\\mathbf{a}\\right)^T\\left(\\Phi^T\\mathbf{a}\\right)\n\\]\n\\[\nL_{\\mathbf{a}} = \\frac{1}{2}\\mathbf{a}^T\\Phi\\Phi^T\\Phi\\Phi^T\\mathbf{a}-\\mathbf{a}^T\\Phi\\Phi^T\\mathbf{t}+\\frac{1}{2}\\mathbf{t}^T\\mathbf{t}+\\frac{\\lambda}{2}\\mathbf{a}^T\\Phi\\Phi^T\\mathbf{a}\n\\]\nGuess what? we can rewrite the Loss function in terms of the Gram Matrix !\n\\[\nL_{\\mathbf{a}} = \\frac{1}{2}\\mathbf{a}^TKK\\mathbf{a}-\\mathbf{a}^TK\\mathbf{t}+\\frac{1}{2}\\mathbf{t}^T\\mathbf{t}+\\frac{\\lambda}{2}\\mathbf{a}^TK\\mathbf{a}\n\\]\nBy combining \\(\\mathbf{w} = \\Phi^T\\mathbf{a}\\) and \\(a_n = -\\frac{1}{\\lambda}(\\mathbf{w}^T\\varphi(\\mathbf{x}_n)-t_n)\\), setting the gradient w.r.t \\(\\mathbf{a}\\) equal to \\(0\\) and isolating \\(\\mathbf{a}\\) we obtain:\n\\[\n\\mathbf{a}=(K+\\lambda\\mathbf{I}_N)^{-1}\\mathbf{t}\n\\]\nWhere \\(I_N\\) is the identity matrix of dimension \\(N\\). Consider that \\(K = N\\times N\\) and \\(\\mathbf{t} = N\\times 1\\), so \\(\\mathbf{a} = N \\times 1\\).\nSo we can make our prediction for a new input \\(\\mathbf{x}\\) by substituting back into our linear regression model:\n\\[\ny(\\mathbf{x}) = \\mathbf{w}^T\\varphi(\\mathbf{x}) = (\\Phi^T\\mathbf{a})^T\\varphi(\\mathbf{x}) = \\mathbf{a}^T\\Phi\\varphi(\\mathbf{x})= \\mathbf{k}(\\mathbf{x})^T(K+\\lambda\\mathbf{I}_N)^{-1}\\mathbf{t}\n\\]\nwhere \\(\\mathbf{k}(\\mathbf{x})\\) is an \\(N\\)-dimensional column vector with elements \\(k_n(\\mathbf{x}) = k(\\mathbf{x}_n,\\mathbf{x})\\).\nThe good thing is that instead of inverting an \\(M\\times M\\) matrix, we are inverting an \\(N\\times N\\) matrix! This allows us to work with very high or infinite dimensionality of \\(\\mathbf{x}\\).\nBut how can we build a valid kernel?\nWe have mainly two ways to do it:\nBy construction: we choose a feature space mapping \\(\\varphi(\\mathbf{x})\\) and use it to ﬁnd the corresponding kernel.\nIt is possible to test whether a function is a valid kernel without having to construct the basis function explicitly. The necessary and suﬃcient condition for a function \\(k(\\mathbf{x},\\mathbf{x}')\\) to be a kernel is that the Gram matrix \\(K\\) is positive semi-deﬁnite for all possible choices of the set \\(\\{x_n\\}\\). It means that \\(\\mathbf{x}^TK\\mathbf{x}\\ge 0\\) for non-zero vectors \\(\\mathbf{x}\\) with real entries, i.e.\\(\\sum_n\\sum_m K_{n,m}x_nx_m \\ge 0\\) for any real number \\(x_n,x_m\\).\n\\(\\implies\\)Mercer’s Theorem : Any continuous, symmetric, positive semi-deﬁnite kernel function \\(k(\\mathbf{x},\\mathbf{y})\\) can be expressed as a dot product in a high-dimensional space.\nNew kernels can be constructed from simpler kernels as building blocks; given valid kernels \\(k_1(\\mathbf{x},\\mathbf{x})\\) and \\(k_2(\\mathbf{x},\\mathbf{x})\\) the following new kernels will be valid:\n\\(k(\\mathbf{x},\\mathbf{x}^{'})=c \\cdot k_1(\\mathbf{x},\\mathbf{x}^{'})\\)\n\\(k(\\mathbf{x},\\mathbf{x}^{'})=f(x)\\cdot k_1(\\mathbf{x},\\mathbf{x}^{'})\\cdot f(x)\\)\n\\(k(\\mathbf{x},\\mathbf{x}^{'})=q\\left(k_1(\\mathbf{x},\\mathbf{x}^{'})\\right)\\) where \\(q()\\) is a polynomial with non-negative coefficients.\n\\(k(\\mathbf{x},\\mathbf{x}^{'})=e^{k_1(\\mathbf{x},\\mathbf{x}^{'})}\\)\n\\(k(\\mathbf{x},\\mathbf{x}^{'})=k_1(\\mathbf{x},\\mathbf{x}^{'})+k_2(\\mathbf{x},\\mathbf{x}^{'})\\)\n\\(k(\\mathbf{x},\\mathbf{x}^{'})=k_1(\\mathbf{x},\\mathbf{x}^{'})k_2(\\mathbf{x},\\mathbf{x}^{'})\\)\n\n\n\n",
    "preview": "posts/2021-07-22-the-kernel-trick/images/preview.png",
    "last_modified": "2021-07-22T14:18:58+02:00",
    "input_file": {},
    "preview_width": 700,
    "preview_height": 614
  },
  {
    "path": "posts/2021-07-21-how-to-call-a-c-function-from-python/",
    "title": "How to call a C function from Python.",
    "description": "Need to speed up things by calling a C function from your Python script? Check this out.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-03-03",
    "categories": [
      "Coding",
      "Performance Optimization",
      "C",
      "Python"
    ],
    "contents": "\nThis will be a really short blogpost which mainly serves as a personal reminder on how to call a C function from Python… the title is pretty self-explanatory.\nI will show how to call some really simple c functions wich return a double or an array (which will be a np.array in Python).\nHere we have our simple c file cfunctions.c:\n\n#include<stdlib.h>\n\nlong factorial(int n){\n    // \n    long res = 1;\n    if (n <= 0) {\n        return -1;\n    }\n    else {\n        for (long i = 1; i <= n; i++) {\n           res *= i;\n        }\n    }\n    return res;\n}\n\ndouble dotproduct(int dim, double a[dim], double b[dim]){\n    // Compute the dot product between two vectors...\n    // e.g. a = [1,2,3,4] , b = [4,3,2,1]\n    // res = 1*4 + 2*3 + 3*2 + 4*1 = 4 + 6 + 6 + 4 = 20\n    double res = 0;   \n    for(int i = 0; i < dim; i++){\n        res = res + a[i]*b[i];\n    }\n    return res;\n}\n\ndouble * elementwiseproduct(int dim, double a[dim], double b[dim]){\n    // Compute the elementiwise product between two vectors...\n    // e.g. a = [1,2,3,4] , b = [4,3,2,1]\n    // res will point to an array such as [1*4, 2*3, 3*2, 4*1] = [4,6,6,4]\n    double * res = (double *) malloc(sizeof(double) * dim);\n    for(int i = 0; i < dim; i++){\n        res[i] = a[i]*b[i];\n    }    \n    return res;\n}\n\nWe then create a shared object witht the following command:\n\ncc -fPIC -shared -o cfunctions.so cfunctions.c\n\nThen we can write a Python script py_cfunctions.py such as:\n\nimport ctypes\nimport numpy as np\n\ndef factorial(num: int):\n    py_cfunctions.factorial.restype = ctypes.c_long\n    return py_cfunctions.factorial(num)\n\ndef dotproduct(dim: int, a: np.array, b: np.array):\n    # Convert np.array to ctype doubles\n    a_data = a.astype(np.double)\n    a_data_p = a_data.ctypes.data_as(c_double_p)\n    b_data = b.astype(np.double)\n    b_data_p = b_data.ctypes.data_as(c_double_p)\n    py_cfunctions.dotproduct.restype = ctypes.c_double\n    # Compute result...\n    return py_cfunctions.dotproduct(dim,a_data_p,b_data_p)\n    \ndef elementwiseproduct(dim: int, a: np.array, b: np.array):\n    # Convert np.array to ctype doubles\n    a_data = a.astype(np.double)\n    a_data_p = a_data.ctypes.data_as(c_double_p)\n    b_data = b.astype(np.double)\n    b_data_p = b_data.ctypes.data_as(c_double_p)\n\n    py_cfunctions.elementwiseproduct.restype = np.ctypeslib.ndpointer(dtype=ctypes.c_double,shape=(dim,))\n    # Compute result...\n    return py_cfunctions.elementwiseproduct(dim,a_data_p,b_data_p)\n\n# so_file genreated with:\n# cc -fPIC -shared -o cfunctions.so cfunctions.c\n\nso_file = 'MY_PATH/cfunctions.so'\npy_cfunctions = ctypes.CDLL(so_file)\nc_double_p = ctypes.POINTER(ctypes.c_double)\npy_cfunctions.factorial.argtypes = [ctypes.c_int] \npy_cfunctions.elementwiseproduct.argtypes = [ctypes.c_int, c_double_p, c_double_p]\npy_cfunctions.dotproduct.argtypes= [ctypes.c_int, c_double_p, c_double_p]\n\nAnd that’s it! You now can import py_cfunctions and call factorial(), dotproduct() and elementwiseproduct().\n\n\n\n",
    "preview": "posts/2021-07-21-how-to-call-a-c-function-from-python/c-python.png",
    "last_modified": "2021-07-22T00:49:43+02:00",
    "input_file": {},
    "preview_width": 1600,
    "preview_height": 1200
  },
  {
    "path": "posts/2021-07-22-the-vc-dimension/",
    "title": "The VC dimension.",
    "description": "A quick explanation of the VC dimension.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-02-14",
    "categories": [
      "Machine Learning",
      "Classification"
    ],
    "contents": "\nWhen talking about binary classification, an hypothesis is a function that maps an input from the entire input space to a result: \\[\nh:\\mathcal{X}\\to\\{-1,+1\\}\n\\] The number of hypotheses \\(\\vert\\mathcal{H}\\vert\\) can be infinite.\nA dichotomy is a hypothesis that maps from an input from the sample size to a result:\n\\[\nh:\\{\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_N\\}\\to\\{-1,+1\\}\n\\]\nThe number of dichotomies \\(\\vert\\mathcal{H}(\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_N )\\vert\\) is at most \\(2^N\\), where \\(N\\) is the sample size.\ne.g. for a sample size \\(N = 3\\) we have at most \\(8\\) possible dichotomies:\n        x1 x2 x3\n1       -1 -1 -1\n2       -1 -1 +1\n3       -1 +1 -1\n4       -1 +1 +1\n5       +1 -1 -1 \n6       +1 -1 +1\n7       +1 +1 -1\n8       +1 +1 +1\n\nThe growth function is a function that counts the most dichotomies on any \\(N\\) points. \\[\nm_{\\mathcal{H}}(N)=\\underset{\\mathbf{x}_1,\\dots,\\mathbf{x}_N\\in\\mathcal{X}}{max}\\vert\\mathcal{H}(\\mathbf{x}_1,\\dots,\\mathbf{x}_N)\\vert\n\\] This translates into choosing any \\(N\\) points and laying them out in any fashion in the input space. Determining \\(m\\) is equivalent to looking for such a layout of the \\(N\\) points that yields the most dichotomies.\nThe growth function satisfies: \\[\nm_{\\mathcal{H}}(N)\\le 2^N\n\\] This can be applied to the perceptron. For example, when \\(N=4\\), we can lay out the points so that they are easily separated. However, given a layout, we must then consider all possible configurations of labels on the points, one of which is the following:\n\n\nThis is where the perceptron breaks down because it cannot separate that configuration, and so \\(m_{\\mathcal{H}}(4)=14\\) because two configurations—this one and the one in which the left/right points are blue and top/bottom are red—cannot be represented. For this reason, we have to expect that for perceptrons, \\(m\\) can’t be \\(2^4\\).\nThe VC ( Vapnik-Chervonenkis ) dimension of a hypothesis set \\(\\mathcal{H}\\) , denoted by \\(d_{VC}(\\mathcal{H})\\) is the largest value of \\(N\\) for which \\(m_{\\mathcal{H}}(N)=2^N\\) , in other words is “the most points \\(\\mathcal{H}\\) can shatter”\nWe can say that the VC dimension is one of many measures that characterize the expressive power, or capacity, of a hypothesis class.\nYou can think of the VC dimension as “how many points can this model class memorize/shatter?” (a ton? \\(\\to\\) BAD! not so many? \\(\\to\\) GOOD!).\nWith respect to learning, the effect of the VC dimension is that if the VC dimension is finite, then the hypothesis will generalize:\n\\[\nd_{vc}(\\mathcal H)\\ \\Longrightarrow\\ g \\in \\mathcal H \\text { will generalize }\n\\]\nThe key observation here is that this statement is independent of:\nThe learning algorithm\nThe input distribution\nThe target function\nThe only things that factor into this are the training examples, the hypothesis set, and the final hypothesis.\nThe VC dimension for a linear classifier (i.e. a line in 2D, a plane in 3D etc…) is \\(d+1\\) (a line can shatter at most \\(2+1=3\\) points, a plane can shatter at most \\(3+1=4\\) points etc…)\nProof: here\nHow many randomly drawn examples sufﬁce to guarantee error of at most \\(\\epsilon\\) with probability at least (1−\\(\\delta\\))?\n\\[\nN\\ge\\frac{1}{\\epsilon}\\left(4\\log\\left(\\frac{2}{\\delta}\\right)+8VC(H)\\log_2\\left(\\frac{13}{\\epsilon}\\right)\\right)\n\\]\nPAC BOUND using VC dimension: \\[\nL_{true}(h)\\le L_{train}(h)+\\sqrt{\\frac{VC(H)\\left(\\ln\\frac{2N}{VC(H)}+1\\right)+\\ln\\frac{4}{\\delta}}{N}}\n\\]\n\n\n\n",
    "preview": "posts/2021-07-22-the-vc-dimension/media/perc.png",
    "last_modified": "2021-07-22T12:51:42+02:00",
    "input_file": {},
    "preview_width": 474,
    "preview_height": 372
  },
  {
    "path": "posts/2021-07-22-generative-adversarial-networks-original-formulation/",
    "title": "Generative Adversarial Networks (Original Formulation)",
    "description": "\"Generative Adversarial Networks\" (Goodfellow et al.) Paper overview.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2020-12-14",
    "categories": [
      "Computer Vision",
      "Machine Learning",
      "Generative Adversarial Networks"
    ],
    "contents": "\nLet’s start with a question: what is Generative Modeling?\nGenerative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.\nGANs are a clever way of training a generative model by framing the problem as a supervised learning problem with two sub-models: the generator model that we train to generate new examples, and the discriminator model that tries to classify examples as either real (from the domain) or fake (generated).\nFormulation\nGenerator \\(\\mathcal{G}\\): produces realistic samples e.g. taking as input some random noise. \\(\\mathcal{G}\\) tries to fool the discriminator\nDiscriminator \\(\\mathcal{D}\\) that takes as input an image and assess whether it is real or generated by \\(\\mathcal{G}\\)\nBoth \\(\\mathcal{D}\\) and \\(\\mathcal{G}\\) are conveniently chosen as MLPs. The generative process depends on two networks:\n\\(\\mathcal{D} =\\mathcal{D}(\\mathbf{x},\\theta_d)\\)\n\\(\\mathcal{G} =\\mathcal{G}(\\mathbf{z},\\theta_g)\\)\n\\(\\theta_g\\) and \\(\\theta_d\\) are the network parameters, \\(\\mathbf{x}\\in\\mathbb{R}^n\\) is an input image (either real or generated by \\(\\mathcal{G}\\)) and \\(\\mathbf{z}\\in\\mathbb{R}^d\\) is some random noise to be fed to the generator. We suppose that \\(\\mathbf{x}\\) is sampled from a distribution \\(p_{data}\\) \\((\\)i.e. \\(\\mathbf{x}\\sim p_{data}(\\mathbf{x}))\\) and \\(\\mathbf{z}\\) is sampled from a distribution \\(p_z\\) \\((\\)i.e. \\(\\mathbf{z}\\sim p_z(\\mathbf{z}))\\). Our Discriminator’s output is to be seen as the probability that the input image comes from the data and not from the generator: \\[\n\\mathcal{D}(\\cdot,\\theta_d):\\mathbb{R}^n \\to [0,1]\n\\]\nThe generator gives as output a generated image: \\[\n\\mathcal{G}(\\cdot,\\theta_d):\\mathbb{R}^d \\to \\mathbb{R}^n\n\\] A good discriminator is such that:\n\\(\\mathcal{D}(\\mathbf{x},\\theta_d)\\) is maximum when \\(\\mathbf{x} \\in X\\) (i.e. \\(\\mathbf{x}\\) is sampled for the real images dataset \\(X\\))\n\\(1 - \\mathcal{D}(\\mathbf{x},\\theta_d)\\) is maximum when \\(\\mathbf{x}\\) was generated by \\(\\mathcal{G}\\)\n\\(1 - \\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)\\) is maximum when \\(\\mathbf{z} \\sim p_z(\\mathbb{z})\\)\nTraining \\(\\mathcal{D}\\) consists in maximizing the binary cross-entropy:\n\\[\n\\underset{\\theta_d}{\\text{max}}\n\\left(\\ \n\\mathbb{E}_{\\mathbf{x}\\sim p_{data}(\\mathbf{x})}\n[\\log\\mathcal{D}(\\mathbf{x},\\theta_d)]\n+\n\\mathbb{E}_{\\mathbf{z}\\sim p_z(\\mathbf{z})}\n\\log[1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)]\n\\ \\right)\n\\]\nWhere\n\\(\\mathcal{D}(\\mathbf{x},\\theta_d)\\) has to be \\(1\\) since \\(\\mathbf{x} \\sim p_{data}(\\mathbf{x})\\), namely images are real.\n\\(\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)\\) has to be \\(0\\) since \\(\\mathcal{G}(\\mathbf{z},\\theta_g)\\) is a generated (fake) image.\nA good generator \\(\\mathcal{G}\\) is one that makes \\(\\mathcal{D}\\) fail:\n\\[\n\\underset{\\theta_g}{\\text{min}}\n\\left(\n\\underset{\\theta_d}{\\text{max}}\n\\left(\\ \n\\mathbb{E}_{\\mathbf{x}\\sim p_{data}(\\mathbf{x})}\n[\\log\\mathcal{D}(\\mathbf{x},\\theta_d)]\n+\n\\mathbb{E}_{\\mathbf{z}\\sim p_z(\\mathbf{z})}\n\\log[1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)]\n\\ \n\\right)\n\\right)\n\\]\nOptimizing \\(\\mathcal{D}\\) to completion in the inner loop of training is computationally prohibitive, and on finite datasets would result in overfitting. Instead, we alternate between \\(k\\) steps of optimizing \\(\\mathcal{D}\\) and one step of optimizing \\(\\mathcal{G}\\). This results in \\(\\mathcal{D}\\) being maintained near its optimal solution, as long as \\(\\mathcal{G}\\) changes slowly enough.\nLet’s schematize it:\nWe need to solve by an iterative numerical approach the min max game shown at \\((4)\\). In order to do so we alternate:\n\\(k\\)-steps of Stochastic Gradient Ascent w.r.t. \\(\\theta_d\\) to solve\n\\[\n  \\underset{\\theta_d}{\\text{max}}\n  \\left(\\ \n  \\mathbb{E}_{\\mathbf{x}\\sim p_{data}(\\mathbf{x})}\n  [\\log\\mathcal{D}(\\mathbf{x},\\theta_d)]\n  +\n  \\mathbb{E}_{\\mathbf{z}\\sim p_z(\\mathbf{z})}\n  \\log[1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)]\n  \\ \n  \\right)\n  \\]\n\\(1\\)-step of Stochastic Gradient Descent w.r.t. \\(\\theta_g\\) being \\(\\theta_d\\) fixed:\n\\[\n  \\underset{\\theta_g}{\\text{min}}\n  \\left(\\ \n  \\mathbb{E}_{\\mathbf{x}\\sim p_{data}(\\mathbf{x})}\n  [\\log\\mathcal{D}(\\mathbf{x},\\theta_d)]\n  +\n  \\mathbb{E}_{\\mathbf{z}\\sim p_z(\\mathbf{z})}\n  \\log[1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)]\n  \\ \n  \\right)\n  \\]\ni.e. (the removed term does not depend on \\(\\theta_g\\))\n\\[\n  \\underset{\\theta_g}{\\text{min}}\n  \\left(\\ \n  \\mathbb{E}_{\\mathbf{z}\\sim p_z(\\mathbf{z})}\n  [\\log(1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d))]\n  \\ \n  \\right)\n  \\]\ni.e.\n\\[\n  \\underset{\\theta_g}{\\text{max}}\n  \\left(\\ \n  \\mathbb{E}_{\\mathbf{z}\\sim\\phi_z}\n  [\\log\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)]\n  \\ \n  \\right)\n  \\]\nThere is a reason why Goodfellow proposed to optimize \\(\\log(\\mathcal{D}(\\cdot))\\) instead of \\(\\log(1-\\mathcal{D}(\\cdot))\\). If we try to descend the gradient of \\(\\log(1-\\mathcal{D}(x))\\), we notice that at the beginning of the training process, when the generated samples would be easily classified as “fake” (i.e. \\(\\mathcal{D}(x) \\sim 0\\)), there would be too few gradient in order to learn properly!\n\n\n\nWe have the following value function for our min-max problem:\n\\[\nV(\\mathcal{G},\\mathcal{D})\n=\n\\mathbb{E}_{\\mathbf{x}\\sim p_{\\text{data}}(\\mathbf{x})}\\log(\\mathcal{D}(\\mathbf{x}))\\mathbf{x}+\\mathbb{E}_{\\mathbf{z}\\sim p_{z}(\\mathbf{z})}\\log(1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z}))d\\mathbf{z}\n\\]\n\\[\n=\\int_{\\mathbf{x}}p_{\\text{data}}(\\mathbf{x})\\log(\\mathcal{D}(\\mathbf{x}))d\\mathbf{x}\n+\n\\int_{\\mathbf{z}}p_{\\mathbf{z}}(\\mathbf{z})\\log(1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z}))d\\mathbf{z}\n\\]\n\\[\n=\\int_{\\mathbf{x}}\n\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\n\\right)\nd\\mathbf{x}\n\\]\nThis last equality comes from the Radon-Nikodym Theorem of measure theory and it’s sometimes referred as the Law Of The Unconscious Statistician (or LOTUS Theorem) since students have been accused of using the identity without realizing that it must be treated as the result of a rigorously proved theorem, not merely a definition (if you want the full proof check this out! )\n\\[\nV(\\mathcal{G},\\mathcal{D})\n=\\int_{\\mathbf{x}}\n\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\n\\right)\nd\\mathbf{x}\n\\]\nLet’s first consider the optimal discriminator \\(\\mathcal{D}\\) for any given generator \\(\\mathcal{G}\\). The training criterion for the discriminator \\(\\mathcal{D}\\), given any generator \\(\\mathcal{G}\\), is to maximize the quantity defined below:\n\\[\n\\underset{\\mathcal{D}}{\\text{argmax}}\\left(V(\\mathcal{G},\\mathcal{D})\\right)\n=\\underset{\\mathcal{D}}{\\text{argmax}}\\left(\\int_{\\mathbf{x}}\n\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\n\\right)\nd\\mathbf{x}\n\\right)\n\\]\nFor the individual sample \\(\\mathbf{x}\\) we derive \\(V(\\mathcal{G},\\mathcal{D})\\) w.r.t. \\(\\mathcal{D}(\\mathbf{x})\\) and we equal this quantity to \\(0\\) in order to find the optimal discriminator \\(\\mathcal{D}(\\mathbf{x})^{*}\\)\n\\[\n\\frac{d}{d\\mathcal{D}(\\mathbf{x})}\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\n\\right)\n=0\n\\]\n\\[\n\\frac{p_{data}(\\mathbf{x})}{\\mathcal{D}^{*}(\\mathbf{x})}\n-\n\\frac{p_{g}(\\mathbf{x})}{1-\\mathcal{D}^{*}(\\mathbf{x})}=0\n\\]\n\\[\n\\frac{p_{data}(\\mathbf{x})(1-\\mathcal{D}^{*}(\\mathbf{x}))-p_{g}(\\mathbf{x})\\mathcal{D}^{*}(\\mathbf{x})}{\\mathcal{D}^{*}(\\mathbf{x})(1-\\mathcal{D}^{*}(\\mathbf{x}))}\n=0\n\\]\n\\[\n\\frac{p_{data}(\\mathbf{x})-p_{data}(\\mathbf{x})\\mathcal{D}^{*}(\\mathbf{x})-p_{g}(\\mathbf{x})\\mathcal{D}^{*}(\\mathbf{x})}{\\mathcal{D}^{*}(\\mathbf{x})(1-\\mathcal{D}^{*}(\\mathbf{x}))}\n= 0\n\\]\n\\[\np_{data}(\\mathbf{x})-p_{data}(\\mathbf{x})\\mathcal{D}^{*}(\\mathbf{x})-p_{g}(\\mathbf{x})\\mathcal{D}^{*}(\\mathbf{x})\n= 0\n\\]\n\\[\np_{data}(\\mathbf{x})-\\mathcal{D}^{*}(\\mathbf{x})\\left(p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})\\right)\n= 0\n\\]\n\\[\n\\color{blue}{\\mathcal{D}^{*}(\\mathbf{x})\n=\n\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\n}\n\\]\nDoes this point represent a maximum? we have to check if the second derivative calculated in \\(\\mathcal{D}^{\\star}\\) is negative.\n\\[\n\\frac{d}{d\\mathcal{D}(\\mathbf{x})}\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\n\\right)\n=\n\\frac{p_{data}(\\mathbf{x})}{\\mathcal{D}(\\mathbf{x})}\n-\n\\frac{p_{g}(\\mathbf{x})}{1-\\mathcal{D}(\\mathbf{x})}\n\\]\n\\[\n\\frac{d^2}{d^2\\mathcal{D}(\\mathbf{x})}\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\n\\right)\n=\n\\frac{d}{d\\mathcal{D}(\\mathbf{x})}\\left(\\frac{p_{data}(\\mathbf{x})}{\\mathcal{D}(\\mathbf{x})}\n-\n\\frac{p_{g}(\\mathbf{x})}{1-\\mathcal{D}(\\mathbf{x})}\\right)\n\\]\n\\[\n\\frac{d}{d\\mathcal{D}(\\mathbf{x})}\\left(\\frac{p_{data}(\\mathbf{x})}{\\mathcal{D}(\\mathbf{x})}\n-\n\\frac{p_{g}(\\mathbf{x})}{1-\\mathcal{D}(\\mathbf{x})}\\right)\n= \n-\\frac{p_{data}(\\mathbf{x})}{\\mathcal{D}^2(\\mathbf{x})}\n-\n\\frac{p_{g}(\\mathbf{x})}{\\left(1-\\mathcal{D}(\\mathbf{x})\\right)^2} < 0\n\\]\nThe quantity above is negative for every \\(\\mathcal{D}\\), \\(\\mathcal{D}^*\\) included, since \\(p_{data}(\\mathbf{x})\\) and \\(p_g(\\mathbf{x})\\) are between \\(0\\) and \\(1\\).\nWe then can plug \\(\\mathcal{D^{\\star}}\\) into \\(\\mathcal{V(G,D)}\\) and find the optimal generator \\(\\mathcal{G^{\\star}}\\) as:\n\\[\n\\int_{\\mathbf{x}}\\left(p_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}^{\\star}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}^{\\star}(\\mathbf{x}))\\right)d\\mathbf{x}\n\\]\n\\[\n\\mathcal{G}^{\\star} =\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\\int_{\\mathbf{x}}\n\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log\\left(\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\n+\np_{g}(\\mathbf{x})\n\\log\\left(1-\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\n\\right)\nd\\mathbf{x}\n\\right)\n\\]\n\\[\n\\mathcal{G}^{\\star} =\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\\int_{\\mathbf{x}}\n\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log\\left(\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\n+\np_{g}(\\mathbf{x})\n\\log\\left(\\frac{p_{g}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\n\\right)\nd\\mathbf{x}\n\\right)\n\\]\n\\[\n\\mathcal{G}^{\\star}=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\n\\int_{\\mathbf{x}}\n\\left(\np_{\\text{data}}(\\mathbf{x})\n\\left(\\log 2 - \\log 2\\right) +\np_{\\text{data}}(\\mathbf{x})\n\\log\\left(\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\n\\\\\n+\np_{\\text{g}}(\\mathbf{x})\n\\left(\\log 2 - \\log 2\\right) +\np_{g}(\\mathbf{x})\n\\log\\left(\\frac{p_{g}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\n\\right)\nd\\mathbf{x}\n\\right)\n\\]\n\\[\n\\mathcal{G}^{\\star}=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\n-\\log 2\\int_{\\mathbf{x}}\\left(p_g(\\mathbf{x})+p_{data}(\\mathbf{x})\\right)d\\mathbf{x}\n+\n\\int_{\\mathbf{x}}\np_{\\text{data}}(\\mathbf{x})\n\\left(\\log 2  + \n\\log\\left(\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\n\\right)\\right)d\\mathbf{x}\\\\\n +\n\\int_{\\mathbf{x}}\np_{\\text{g}}(\\mathbf{x})\n\\left(\\log 2  + \n\\log\\left(\\frac{p_{g}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\\right)d\\mathbf{x}\n\\right)\n\\]\n\\[\n\\mathcal{G}^{\\star}=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\n-\\log 2 \\cdot (2)\n+\n\\int_{\\mathbf{x}}\np_{\\text{data}}(\\mathbf{x}) \n\\log\\left(2\\cdot\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\n\\right)d\\mathbf{x}\\\\\n +\n\\int_{\\mathbf{x}}\np_{\\text{g}}(\\mathbf{x})\n\\log\\left(2\\cdot\\frac{p_{g}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)d\\mathbf{x}\n\\right)\n\\]\n\\[\n\\mathcal{G}^{\\star}=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\n-\\log 2^2\n+\n\\int_{\\mathbf{x}}\np_{\\text{data}}(\\mathbf{x}) \n\\log\\left(\\frac{p_{data}(\\mathbf{x})}{\\frac{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}{2}}\n\\right)d\\mathbf{x}\\\\\n +\n\\int_{\\mathbf{x}}\np_{\\text{g}}(\\mathbf{x})\n\\log\\left(\\frac{p_{g}(\\mathbf{x})}{\\frac{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}{2}}\\right)d\\mathbf{x}\n\\right)\n\\]\n\\[\n\\mathcal{G}^{\\star}=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\n-\\log 4\n+\n\\mathit{KL}\\left(p_{data}||\\frac{p_g+p_{data}}{2}\\right)\n+\n\\mathit{KL}\\left(p_{g}||\\frac{p_g+p_{data}}{2}\\right)\n\\right)\n\\]\n\\[\n\\color{blue}{\\mathcal{G}^{\\star}=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\n-\\log 4\n+\n2\\cdot\\mathit{JSD}(p_{data}||p_g)\\right)}\n\\]\nWhere the Kullback-Leibler divergence (KL) and the Jenson-Shannon divergence (JSD) are quantities that measure the difference between two distributions and we know that \\(\\mathit{JSD}(p_{data}\\vert\\vert p_g)=0\\) only when \\(p_{data} = p_g\\) !\n\\[\n\\implies V(\\mathcal{D}^{\\star}_{\\mathcal{G}},\\mathcal{G}) = -\\log 4\n\\]\nTheorem \\(1\\):\nThe global minimum of the virtual training criterion \\[V(\\mathcal{D}^{\\star}_{\\mathcal{G}},\\mathcal{G})\\] is achieved if and only if \\(p_{g}=p_{data}\\). At that point, \\(V(\\mathcal{D}^{\\star}_{\\mathcal{G}},\\mathcal{G})\\) achieves the value \\(−\\log 4\\).\nBesides, that was what we expected! We wanted our generator to learn the same distribution which generated the data. If we know that \\(p_{data} = p_g\\) then it’s trivial to observe that at the end of the training process the optimal discriminator will be forced to output \\(0.5\\) since it won’t be able to distinguish between real and fake samples anymore.\n\\[\n\\color{blue}{\\mathcal{D}^{\\star}(\\mathbf{x})\n=\n\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\n= \\frac{1}{2}}\n\\]\nBut does this converge?\nWell, as stated in the original Paper:\nIf \\(\\mathcal{G}\\) and \\(\\mathcal{D}\\) have enough capacity, and at each step of our algorithm, the discriminator is allowed to reach its optimum given \\(\\mathcal{G}\\) and \\(p_g\\) is updated so as to improve the criterion \\[\n\\mathbb{E}_{\\mathbf{x}\\sim p_{data}}\\left[\\log\\mathcal{D}^{\\star}_{G}(\\mathbf{x})\\right] + \\mathbb{E}_{\\mathbf{x}\\sim p_{g}}\\left[\\log(1-\\mathcal{D}^{\\star}_{G}(\\mathbf{x}))\\right]\n\\]\nthen \\(p_g\\) converges to \\(p_{data}\\).\nProof:\nConsider \\(V(\\mathcal{G,D})= U(p_g,\\mathcal{D})\\) as a function of \\(p_g\\) as done in the above criterion. Note that \\(U(p_g,\\mathcal{D})\\) is convex in \\(p_g\\).\nThe subderivatives of a supremum of convex functions include the derivative of the function at the point where the maximum is attained. In other words, if \\(f(x)=\\sup_{\\alpha\\in\\mathcal{A}}f_\\alpha(x)\\) and \\(f_\\alpha(x)\\) is convex in \\(x\\) for every \\(\\alpha\\) , then \\(\\partial f_\\beta(x) \\in \\partial f\\) if \\(\\beta=\\arg\\sup_{\\alpha\\in\\mathcal{A}}f_\\alpha(x)\\). This is equivalent to computing a gradient descent update for \\(p_g\\) at the optimal \\(\\mathcal{D}\\) given the corresponding \\(\\mathcal{G}\\). \\(\\sup_\\mathcal{D}U(p_g,\\mathcal{D})\\) is convex in \\(p_g\\) with a unique global optima as proven in Theorem \\(1\\), therefore with sufficiently small updates of \\(p_g\\), \\(p_g\\) converges to \\(p_x\\), concluding the proof.\nIn practice, adversarial nets represent a limited family of \\(p_g\\) distributions via the function \\(\\mathcal{G}(\\mathbf{z};\\theta_g)\\), and we optimise \\(\\theta_g\\) rather than \\(p_g\\) itself. Using a multilayer perceptron to define \\(\\mathcal{G}\\) introduces multiple critical points in parameter space. However, the excellent performance of multilayer perceptrons in practice suggests that they are a reasonable model to use despite their lack of theoretical guarantees.\n\n\n\n",
    "preview": "posts/2021-07-22-generative-adversarial-networks-original-formulation/media/preview.png",
    "last_modified": "2021-07-22T19:52:43+02:00",
    "input_file": {},
    "preview_width": 590,
    "preview_height": 390
  },
  {
    "path": "posts/2021-07-22-lotus-theorem-in-original-gans-formulation/",
    "title": "LOTUS Theorem in original GANs formulation.",
    "description": "How do we apply the Law Of The Unconcious Statistician to Ian Goodfellow's original GANs value function formulation?",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2020-12-11",
    "categories": [
      "Computer Vision",
      "Machine Learning",
      "Generative Adversarial Networks"
    ],
    "contents": "\nIn this brief we want to prove a passage of the original GANs Paper by Ian Goodfellow. Specifically, we want to prove that the following equation is satisfied:\n\\[\n\\int_{\\mathbf{z}}p_{z}(\\mathbf{z})\\log(1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z})))d\\mathbf{z}\n=\n\\int_{\\mathbf{x}}p_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\nd\\mathbf{x}\n\\] For a continuous random variable \\(\\mathbf{z}\\), let \\(\\mathbf{x} = \\mathcal{G}(\\mathbf{z})\\) and \\(\\mathbf{z}=\\mathcal{G}^{-1}(\\mathbf{x})\\) suppose that \\(\\mathcal{G}\\) is differentiable and that its inverse \\(\\mathcal{G}^{-1}\\) is monotonic. By the formula for inverse functions and differentiation we have that \\[\n\\frac{d\\mathbf{z}}{d \\mathbf{x}}\\cdot\n\\frac{d \\mathbf{x}}{d \\mathbf{z}}\n=\n1\n\\] \\[\n\\frac{d\\mathbf{z}}{d \\mathbf{x}}\n=\n\\frac{1}{\\frac{d \\mathbf{x}}{d \\mathbf{z}}}\\\\\n\\] \\[\n\\frac\n{d\\mathbf{z}}\n{d\\mathbf{x}}\n=\n\\frac\n{1}\n{\\frac{d\\mathcal{G}(\\mathcal{G}^{-1}(\\mathbf{x}))}{d(\\mathcal{G}^{-1}(\\mathbf{x}))}}\n\\] \\[\n\\frac\n{d\\mathbf{z}}\n{d\\mathbf{x}}\n=\n\\frac\n{1}\n{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}\n\\] \\[\nd\\mathbf{z}\n=\n\\frac\n{1}\n{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}d\\mathbf{x}\n\\] so that by a change of variables we can rewrite everything in function of \\(\\mathbf{x}\\), \\[\n\\int_{-\\infty}^{\\infty}\np_{z}(\\mathbf{z})\n\\log\n\\left(\n1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z}))\n\\right)\nd\\mathbf{z}\n=\n\\] \\[\n\\int_{-\\infty}^{\\infty}\np_{z}(\\mathbf{z})\n\\log\n\\left(\n1-\\mathcal{D}(\\mathbf{x})\n\\right)\n\\frac{1}{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}d\\mathbf{x}=\n\\] \\[\n\\int_{-\\infty}^{\\infty}\n\\color{blue}{p_{z}(\\mathbf{z})}\n\\log\n\\left(\n1-\\mathcal{D}(\\mathbf{x})\n\\right)\n\\color{blue}{\n\\frac{1}{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}}d\\mathbf{x}=\n\\] \\[\n\\int_{-\\infty}^{\\infty}\n\\color{blue}{p_{z}(\\mathbf{z})}\n\\color{blue}{\n\\frac{1}{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}}\n\\log\n\\left(\n1-\\mathcal{D}(\\mathbf{x})\n\\right)\nd\\mathbf{x}=\n\\] Now, notice that the cumulative distribution function \\(P_\\mathbf{X}(\\mathbf{x}):\\mathbb{R}^n\\to[0,1]\\) is \\(P_\\mathbf{X}(\\mathbf{x})=Pr(\\mathbf{X}<\\mathbf{x})=Pr(X_1\\leq x_1,X_2\\leq x_2,\\dots,X_n\\leq x_n)\\), we observe that:\n\\[\nP_\\mathbf{X}(\\mathbf{x})=\n\\] \\[\nPr(\\mathbf{X}<\\mathbf{x})=\n\\] \\[\nPr(\\mathcal{G}(\\mathbf{Z})\\leq\\mathbf{x})=\n\\] \\[\nPr(\\mathbf{Z}\\leq\\mathcal{G}^{-1}(\\mathbf{x}))=\n\\] \\[\nP_{\\mathbf{Z}}(\\mathcal{G}^{-1}(\\mathbf{x}))\n\\] From here \\[\nP_\\mathbf{X}(\\mathbf{x})=P_{\\mathbf{Z}}(\\mathcal{G}^{-1}(\\mathbf{x}))\\\\\nP_\\mathbf{X}(\\mathbf{x})=P_{\\mathbf{Z}}(\\mathbf{z})\n\\] We take the derivative w.r.t \\(\\mathbf{x}\\): \\[\n\\frac{dP_\\mathbf{X}(\\mathbf{x})}{d\\mathbf{x}} =\\frac{P_{\\mathbf{Z}}(\\mathbf{z})}{d\\mathbf{x}}\n\\] \\[\n\\frac{dP_\\mathbf{X}(\\mathbf{x})}{d\\mathbf{x}} =\\frac{P_{\\mathbf{Z}}(\\mathbf{z})}{d\\mathbf{z}}\\frac{d\\mathbf{z}}{d\\mathbf{x}}\n\\] \\[\np_{X}(\\mathbf{x}) = p_{\\mathbf{z}}(\\mathbf{z})\\frac{1}{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}\n\\] For clarity we rename \\(p_{X}(\\mathbf{x})\\) as \\(p_{g}(\\mathbf{x})\\) since it represents the distribution learned from our generator \\(\\mathcal{G}\\), we have \\[\n\\color{blue}{p_{g}(\\mathbf{x}) = p_{\\mathbf{z}}(\\mathbf{z})\\frac{1}{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}}\n\\]\n\n\n\n",
    "preview": "posts/2021-07-22-lotus-theorem-in-original-gans-formulation/media/preview.png",
    "last_modified": "2021-07-22T14:33:36+02:00",
    "input_file": {},
    "preview_width": 576,
    "preview_height": 216
  },
  {
    "path": "posts/2021-07-20-a-boring-post-on-linear-regression/",
    "title": "A boring Post on Linear Regression",
    "description": "A visual and mathematical overview of Linear Regression.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2020-09-12",
    "categories": [
      "Machine Learning",
      "Regression"
    ],
    "contents": "\nRegression is one of the main problems tackled by machine learning.\nWe define regression analysis as a set of statistical processes for estimating the relationship between a dependent variable (often called the target variable) and one or more independent variables (often called features). In Linear Regression we aim to find the best hyperplane that is able to predict the outcome variable starting from a set of features.\nOne trivial example could be estimating the price (target variable) of a house starting from the house’s dimension expressed in squared meters (\\(m^2\\)). In this case, since we are just considering one feature (the dimension of the house expressed in \\(m^2\\)) our hyperplane will consist of a simple line.\n\n\nEach dot in the plot corresponds to a real data sample, namely a real correspondence among area and price. This is how our dataset would look like\n\n\nOur goal consists in finding the line that better explains the data we have been provided with.\n\n\nThis traduces in finding the best weights \\(\\textbf{w} = \\begin{bmatrix}w_0 \\\\ w_1\\end{bmatrix}\\) such that \\(w_0+w_1\\cdot\\left(\\text{dimension in }m^2\\right) \\simeq\\text{Price}\\)\nNow it’s time to introduce some formalism.\nFor the \\(i_{th}\\) data sample we call our target variable \\(t_i\\) , our features \\(\\mathbf{x}_i\\) and the weights that we apply to our features \\(\\mathbf{w}\\). Note that \\(\\mathbf{x}_i\\) and \\(\\mathbf{w}\\) are column vectors , while \\(t_i\\) is just a scalar.\nSuppose we have \\(n\\) data samples and we consider \\(m\\) features, then we define:\nThe target vector\n\\[\n\\mathbf{t} = \\begin{bmatrix}t_1\\\\\nt_2\\\\\n\\vdots\n\\\\\nt_n\n\\end{bmatrix}\n\\]\nThe dataset\n\\[\n\\mathbf{X}=\\begin{bmatrix}\n\\mathbf{x_1}^T\n\\\\ \n\\mathbf{x_2}^T\n\\\\ \n\\vdots\n\\\\ \n\\mathbf{x}_n^T\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 && x_{11} && \\cdots && x_{1m}\n\\\\ \n1 && x_{21} && \\cdots && x_{2m}\n\\\\ \n\\vdots && \\vdots && \\ddots &&\\vdots\n\\\\ 1 &&\\cdots && \\cdots &&x_{nm}\n\\end{bmatrix}\n\\]\nNote that the \\(1\\) at the beginning of each row are just to take in account the bias \\(w_0\\) (that would be the intercept in our trivial example)\nAnd the weights\n\\[\n\\mathbf{w}=\n\\begin{bmatrix}\nw_0\n\\\\\nw_1\n\\\\\nw_2\n\\\\\n\\vdots\n\\\\\nw_m\n\\end{bmatrix}\n\\]\nThen our prediction (that from now on we’ll call \\(y\\) ) for the \\(i_{th}\\) sample will be\n\\[\ny_i = \\mathbf{w}^T\\mathbf{x}_i = \\begin{bmatrix} \nw_0\n&&\nw_1\n&&\nw_2\n&&\n\\cdots\n&&\nw_m\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1\n\\\\\nx_{i1}\n\\\\\nx_{i2}\n\\\\\n\\vdots\n\\\\\nx_{im}\n\\end{bmatrix}\n\\]\nAnd our prediction vector (which will have dimension \\(n\\times 1\\)) will be computed as\n\\[\n\\mathbf{y}=\\begin{bmatrix}\n1 && x_{11} && \\cdots && x_{1m}\n\\\\\n1 && x_{21} && \\cdots && x_{2m}\n\\\\\n\\vdots && \\vdots && \\ddots &&\\vdots\n\\\\\n1 &&\\cdots && \\cdots &&x_{nm}\n\\end{bmatrix}\\cdot\\begin{bmatrix}\nw_0\n\\\\\nw_1\n\\\\\n\\vdots\n\\\\\nw_m\n\\end{bmatrix}=\\mathbf{X}\\mathbf{w}\n\\]\nBut how can we find the optimal weights? We do that by minimizing the so-called Mean Squared Error \\(J(\\mathbf{w})\\).\n\\[\nJ(\\mathbf{w}) =\\\\\n\\frac{1}{N}\\left(\\mathbf{t}-\\mathbf{y}\\right)^T\\left(\\mathbf{t}-\\mathbf{y}\\right)=\\\\\\frac{1}{N}\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)^T\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)\n\\]\nWhere \\(\\epsilon_i\\) is just the difference between the target values \\(t_i\\) and our predictions \\(y_i\\),\n\\[\n\\begin{bmatrix}\nt_1-y_1\n\\\\\nt_2-y_2\n\\\\\nt_3-y_3\n\\\\\n\\vdots\n\\\\\nt_n-y_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\epsilon_1\n\\\\\n\\epsilon_2\n\\\\\n\\epsilon_3\n\\\\\n\\vdots\n\\\\\n\\epsilon_n\n\\end{bmatrix}\n\\]\nTo have a visual understanding of what we’re talking about, the various \\(\\epsilon_i\\) corresponds to the green segments in the image below.\n\nOur cost function is just the Mean Squared Error\n\\[\nJ(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^n\\epsilon_i^2\n\\]\nSince we would like to minimize this quantity, we derive with respect to \\(\\mathbf{w}\\) and set the derivative equal to \\(0\\) .\n\\[\nJ\\left(\\mathbf{w}\\right) =\n\\frac{1}{N}\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)^T\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)\\\\\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} =\n-\\frac{2}{N}\\mathbf{X}^T\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)\n=0\n\\]\nWhich is equivalent to\n\\[\n\\mathbf{X}^T\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)=0\n\\]\nWe then isolate the weights\n\\[\n\\mathbf{X}^T\\mathbf{t}=\\mathbf{X}^T\\mathbf{X}\\mathbf{w}\\\\\n\\mathbf{w}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n\\]\nAnd that’s all!\nNow, before tackling the problems relative with such closed form solution, it is useful to introduce the parameters-space, i.e. the space representing all the possible solutions \\((w)\\) of our problem: in our trivial example this space corresponds to all the possible points \\(w:(w_0,w_1) \\in \\mathbb{R}^2\\), each of this points traduces in a different predictor (line) in the features-space as you can see in the animation below.\nLet’s talk now about some problems that can arise from the closed form solution\nThe inverse of \\(\\left(\\mathbf{X}^T\\mathbf{X}\\right)\\) is computationally expensive when the number of features is high, being the temporal complexity of such inversion \\(\\mathcal{O}(m^3)\\) (Note that \\(\\left(\\mathbf{X}^T\\mathbf{X}\\right)\\) has dimensionality equal to \\((m+1)\\times (m+1)\\) )\nThe matrix \\(\\left(\\mathbf{X}^T\\mathbf{X}\\right)\\) isn’t always invertible since \\(\\mathbf{X}^T\\mathbf{X}\\) could contain linearly dependent rows, which implies a null determinant. In such case we’d have infinite solutions for \\(\\mathbf{w}\\).\nIn order to show this last drawback we’ll use a toy example:\nWe have to find a valid predictor for a dataset which contains just three samples.\n\n\nWe solve by means of the closed form solution:\n\\[\n\\mathbf{X}\\mathbf{w}=\\mathbf{t}\n\\]\n\\[\n\\begin{bmatrix}1 && 1\\\\1 && 0\\\\1 && -1\\\\\\end{bmatrix}\\cdot\\begin{bmatrix}\\mathbf{w}_0\\\\\\mathbf{w}_1\\\\\\end{bmatrix}=\\begin{bmatrix}4.1\\\\1.9\\\\0\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n\\]\n\\[\n\\mathbf{w} = \n\\begin{bmatrix}\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 && 1\\\\\n1 && 0\\\\\n1 && -1\\\\\n\\end{bmatrix}\n\\end{bmatrix}\n^{-1}\n\\cdot\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \n\\begin{bmatrix}\n3 && 0\\\\\n0 && 2\n\\end{bmatrix}\n^{-1}\n\\cdot\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \n\\frac{1}{6}\n\\begin{bmatrix}\n2 && 0\\\\\n0 && 3\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \n\\begin{bmatrix}\n\\frac{1}{3} && 0\\\\\n0 && \\frac{1}{2} \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \n\\begin{bmatrix}\n \\frac{1}{3} &&  \\frac{1}{3}  && \\frac{1}{3}\\\\\n \\frac{1}{2} &&  0  && -\\frac{1}{2}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \n\\begin{bmatrix}\n2.00\\\\\n2.05\n\\end{bmatrix}\n\\]\nIn this case \\(\\mathbf{X}^T\\mathbf{X}\\) is invertible and, if we plot the cost function \\(J(\\mathbf{w})\\) in the parameter space, we can see that \\(J(\\mathbf{w})\\) is a convex function with one single minimum and a well defined bowl shape. This minimum corresponds to the point \\(\\mathbf{w} = \\begin{bmatrix} 2.00\\\\ 2.05 \\end{bmatrix}\\), i.e. the blue dot which appears at the base of the bowl.\nWhich corresponds to the following predictor line:\n\n\nThe above example is the ideal scenario, but it is not always the one you’ll be dealing with. In fact there are some observations to be done regarding the dataset to be used:\nNow let’s compute the closed form solution for another example:\n\\[\n\\begin{bmatrix}m^2 && price\\\\2 && 2\\\\2 && 4\\\\2 && 6\\\\\\end{bmatrix}\n\\]\n\n\nHere our data samples are pretty bad since there doesn’t seem to be any correlation between the independent variable \\((m^2)\\) and the dependent variable \\((price)\\). We will observe that the inversion of the matrix \\(\\mathbf{X}^T\\mathbf{X}\\) becomes problematic.\n\\[\n\\mathbf{X}\\mathbf{w}=\\mathbf{t}\n\\]\n\\[\n\\begin{bmatrix}1 && 2\\\\1 && 2\\\\1 && 2\\\\\\end{bmatrix}\\cdot\\begin{bmatrix}\\mathbf{w}_0\\\\\\mathbf{w}_1\\\\\\end{bmatrix}=\\begin{bmatrix}2\\\\4\\\\6\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n\\]\n\\[\n\\mathbf{w} =\\begin{bmatrix}\\begin{bmatrix}1 && 1 && 1\\\\2 && 2 && 2\\\\\\end{bmatrix}\\cdot\\begin{bmatrix}1 && 2\\\\1 && 2\\\\1 && 2\\\\\\end{bmatrix}\\end{bmatrix}^{-1}\\cdot\\begin{bmatrix}1 && 1 && 1\\\\2 && 2 && 2\\\\\\end{bmatrix}\\begin{bmatrix}2\\\\4\\\\6\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \\begin{bmatrix}  3&& 6\\\\6 && 12\\end{bmatrix}^{-1}\\cdot\\begin{bmatrix}1 && 1 && 1\\\\2 && 2 && 2\\\\\\end{bmatrix}\\cdot\\begin{bmatrix}2\\\\4\\\\6\\end{bmatrix}\n\\]\nAs you can see here we can’t invert \\[\n\\begin{bmatrix}\n3\n&&\n6\n\\\\\n6\n&&\n12\n\\end{bmatrix}\n\\] since its determinant would be \\(0\\) !\nBy plotting the cost function \\(J(\\mathbf{w})\\) we would obtain a sort of parabolic cylinder\nwhose minimum has infinite solutions (that we can find by gradient-based techniques).\nLastly it is opportune to remember that our dataset needs to have a number of samples greater than the number of parameters to be learnt. Being \\(m\\) the number of features of each sample and \\(n\\) the number of samples in our dataset, we need to satisfy the constraint \\(n>m+1\\). Let’s reason why through a simple example. Suppose that you have been asked to find the model that best explains a dataset made of \\(2\\) samples \\((n=2)\\). Suppose that each sample is represented by \\(2\\) features \\((m=2)\\) and that we need to predict a value \\(t\\). In this scenario we are not satisfying the constraint since \\(2\\not>2+1\\). What are the consequences? Of course we will get a perfect fit! There are infinite planes \\((w_0+w_1x_1+w_2x_2=0)\\) which pass through two points!\nWe’ll find at least a model that passes through all the data, more specifically we’d find one solution in the \\(n=m+1\\) case, and infinite solutions in the \\(n<m+1\\) case. We have found a very bad family of models because they are an exact representation of the training data, which translates into a very obvious overfitting problem. Generally we want \\(n >> m\\).\n\n\n\n",
    "preview": "posts/2021-07-20-a-boring-post-on-linear-regression/preview.png",
    "last_modified": "2021-07-20T22:40:44+02:00",
    "input_file": {},
    "preview_width": 2478,
    "preview_height": 1738
  }
]

[
  {
    "path": "posts/2021-07-20-support-vector-machines/",
    "title": "Support Vector Machines",
    "description": "The math behind the Support Vector Machines algorithm.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-07-20",
    "categories": [
      "Machine Learning",
      "Classification"
    ],
    "contents": "\nIntroduction & Brief History\nIn this blog-post we are gonna talk about one of the most powerful and fascinating techniques in Machine Learning: Support Vector Machines.\nIn the field of Statistical Learning the Support Vector Machine technique is a binary classification algorithm which aims to find the hyperplane which is able to separate the data with the largest margin possible. The concept of margin is illustrated in the following images.\nSuppose we have a set of points in \\(\\mathbb{R}^2\\), each point belongs to a class \\(\\in\\{-1,+1\\}\\)\n\n\nWe want to find the best hyperplane (in this case a line) which is able to correctly separate the data.\n\n\nWe identify this hyperplane by maximizing the margin, i.e. the distance from the hyperplane to the closest points of both classes, we call this points support vectors.\n\n\nIn this case we identified two support vectors, they are called like that because they support the dashed lines, which represent the set of points equidistant from the separating hyperplane.\n\n\nThe margins from the support vectors to the hyperplane are drawed in red\n\n\nBefore diving into the theory of the algorithm let’s have a look at the history behind it.\nThe birth of SVMs dates back to \\(1963\\) in Russia, when Vladimir Vapnik and Aleksondr Lerner introduced the Generalized Portrait algorithm.\nAfter almost \\(30\\) years, at the end of \\(1990\\), Vapnik moved to the USA and joined Bernhard Boser and Isabelle Guyen at the Adaptive Systems Research Department at AT&T Bell Labs in New Jersey, where the algorithm was refined.\n\n“The invention of SVMs happened when Bernhard decided to implement Vladimir’s algorithm in the three months we had left before we moved to Berkeley. After some initial success of the linear algorithm, Vladimir suggested introducing products of features. I proposed to rather use the kernel trick of the ‘potential function’ algorithm. Vladimir initially resisted the idea because the inventors of the ‘potential functions’ algorithm (Aizerman, Braverman, and Rozonoer) were from a competing team of his institute back in the 1960’s in Russia! But Bernhard tried it anyways, and the SVMs were born!”\n\nIsabelle Guyen\nPremise on linear classifiers\nFor a binary classification problem, one can visualize the operation of a linear classifier as splitting a high-dimensional input space of dimension \\(d\\) with an hyperplane of dimension \\(d\\) (which, as you’ll see in a minute, corresponds to a \\(d-1\\) dimensional space): all points on one side of the hyperplane are classified as \\(+1\\) (or \\(-1\\)), while the others are classified as \\(-1\\) (or \\(+1\\)). In case you doubt the power of linear classifiers just observe that we’re always able to transform (or enrich) our input space by means of some basis functions, if we “guess” the right transformation maybe we are able to correctly classify our samples with a linear classifier.\nIf, for instance, we have the following unseparable data in the 2D space\n\n\nthere’s nothing stopping us from enriching the input space with some new coordinates which depend on the old features, e.g. by adding a ne w dimension \\(x_3 = \\sqrt{x_1^2+x_2^2}\\).\nThis way, in the new 3D input space, we are able to correctly classify the data by means of a 2D plane.\nDerivation\nFirst of all, we should be familiar with the equation of a generic \\(D\\)-dimensional hyperplane:\n\\[\n\\text{hyperplane: }\\\\\n\\mathbf{w}^T\\mathbf{x}=0\\\\\n\\mathbf{w} = [w_0,w_1,\\dots,w_D]^T\\\\\n\\mathbf{x} = [1,x_1,\\dots,x_D]^T\n\\]\nIf \\(D=2\\) we have that\n\\[\n\\text{hyperplane: }\\\\\nw_0+w_1x_1+w_2x_2=0\\\\\n\\mathbf{w} = [w_0,w_1,w_2]^T\\\\\n\\mathbf{x} = [1,x_1,x_2]^T\n\\]\nLet \\(\\mathbf{x}_N\\) be the nearest data point to the hyperplane \\(\\mathbf{w}^T\\mathbf{x} = 0\\) , before finding the distance we just have to state two observations:\nLet’s say I multiply the vector \\(\\mathbf{w}\\) by \\(1000000\\) , I get the same hyperplane! So any formula that takes \\(\\mathbf{w}\\) and produces the margin will have to have built-in scale-invariance, we do that by normalizing \\(\\mathbf{w}\\) , requiring that for the nearest data point \\(\\mathbf{x}_N\\): \\[\n  |\\mathbf{w}^T\\mathbf{x}_N|=1\n  \\] ( So I just scale \\(\\mathbf{w}\\) up and down in order to fulfill the condition stated above, we just do it because it’s mathematically convenient! By the way remember that \\(1\\) does not represent the Euclidean distance)\nWhen you solve for the margin, the \\(w_1\\) to \\(w_d\\) will play a completely different role from the role of \\(w_0\\) , so it is no longer convenient to have them on the same vector. We pull out \\(w_0\\) from \\(\\mathbf{w}\\) and rename \\(w_0\\) with \\(b\\) (for bias).\n\\[\n\\mathbf{w} = (w_1,\\dots,w_d)\\\\w_0=b\n\\]\nSo now our notation is changed:\nThe hyperplane is represented by\n\\[\n\\mathbf{w}^T\\mathbf{x} +b= 0\n\\]\nand our constraint becomes\n\\[\n|\\mathbf{w}^T\\mathbf{x}_N+b|=1\n\\]\nIt’s trivial to demonstrate that the vector \\(\\mathbf{w}\\) is orthogonal to the hyperplane, just suppose to have two point \\(\\mathbf{x}'\\) and \\(\\mathbf{x''}\\) belonging to the hyperplane , then \\(\\mathbf{w}^T\\mathbf{x}' +b= 0\\) and \\(\\mathbf{w}^T\\mathbf{x}'' +b= 0\\).\nAnd of course \\(\\mathbf{w}^T\\mathbf{x}'' +b - (\\mathbf{w}^T\\mathbf{x}' +b)=\\mathbf{w}^T(\\mathbf{x}''-\\mathbf{x}') = 0\\)\nSince \\(\\mathbf{x}''-\\mathbf{x}'\\) is a vector which lays on the hyperplane , we deduce that \\(\\mathbf{w}\\) is orthogonal to the hyperplane.\n\n\nThen the distance from \\(\\mathbf{x}_N\\) to the hyperplane can be expressed as a dot product between \\(\\mathbf{x}_N-\\mathbf{x}\\) (where \\(\\mathbf{x}\\) is any point belonging to the plane) and the unit vector \\(\\hat{\\mathbf{w}}\\) where \\(\\hat{\\mathbf{w}} = \\frac{\\mathbf{w}}{\\vert\\vert\\mathbf{w}\\vert\\vert}\\)\n(the distance is just the projection of \\(\\mathbf{x}_N-\\mathbf{x}\\) in the direction of \\(\\hat{\\mathbf{w}}\\)!)\n\\[\ndistance = |\\hat{\\mathbf{w}}^T(\\mathbf{x}_N-\\mathbf{x})|\n\\]\nWe take the absolute value since we don’t know if \\(\\mathbf{w}\\) is facing \\(\\mathbf{x}_N\\) or is facing the other direction\n\n\nWe’ll now try to simplify our notion of distance.\n\\[\n\\text{distance} = |\\hat{\\mathbf{w}}^T(\\mathbf{x}_N-\\mathbf{x})\\;| = \\frac{1}{||\\mathbf{w}||}|\\;\\mathbf{w}^T\\mathbf{x}_N-\\mathbf{w}^T\\mathbf{x}|\n\\]\nThis can be simplified if we add and subtract the missing term \\(b\\).\n\\[\ndistance = \\frac{1}{||\\mathbf{w}||}|\\;\\mathbf{w}^T\\mathbf{x}_N+b-\\mathbf{w}^T\\mathbf{x}-b\\;| = \\frac{1}{||\\mathbf{w}||}|\\;\\mathbf{w}^T\\mathbf{x}_N+b-(\\mathbf{w}^T\\mathbf{x}+b)\\;|\n\\]\nWell, \\(\\mathbf{w}^T\\mathbf{x}+b\\) is just the value of the equation of the plane…for a point on the plane. So without any doubt \\(\\mathbf{w}^T\\mathbf{x}+b= 0\\) , our notion of distance becomes\n\\[\ndistance = \\frac{1}{||\\mathbf{w}||}|\\;\\mathbf{w}^T\\mathbf{x}_N+b\\;|\n\\]\nBut wait… what is \\(\\vert\\mathbf{w}^T\\mathbf{x}_N+b\\vert\\) ? It is the constraint that we defined at the beginning of our derivation!\n\\[\n\\vert\\mathbf{w}^T\\mathbf{x}_N+b\\vert=1\n\\]\nSo we end up with the formula for the distance being just\n\\[\ndistance = \\frac{1}{\\vert\\vert\\mathbf{w}\\vert\\vert}\n\\]\nLet’s now formulate the optimization problem, we have:\n\\[\n\\underset{w}{\\operatorname{max}}\\frac{1}{||\\mathbf{w}||}\\\\\\text{subject to}\\;\\underset{n=1,2,\\dots,N}{\\operatorname{min}}|\\mathbf{w}^T\\mathbf{x}_n+b|=1\n\\]\nSince this is not a friendly optimization problem (the constraint is characterized by a minimum and an absolute, which are annoying) we are going to find an equivalent problem which is easier to solve. Our optimization problem can be rewritten as\n\\[\n\\underset{w}{\\operatorname{min}}\\frac{1}{2}\\mathbf{w}^T\\mathbf{w}\n\\\\\n\\text{subject to} \\ \\ \\ \\  y_n \\cdot(\\mathbf{w}^T\\mathbf{x}_n+b)\\ge1 \\;\\;\\;\\;\\text{for $n = 1,2,\\dots,N$}\n\\]\nwhere \\(y_n\\) is a variable that we introduce that will be equal to either \\(+1\\) or \\(-1\\) accordingly to its real target value (remember that this is a supervised learning technique and we know the real target value of each sample). One could argue that the new constraint is actually different from the former one, since maybe the \\(\\mathbf{w}\\) that we’ll find will allow the constraint to be strictly greater than \\(1\\) for every possible point in our dataset [ \\(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)> 1 \\;\\;\\forall{n}\\) ] while we’d like it to be exactly equal to \\(1\\) for at least one value of \\(n\\). But that’s actually not true! Since we’re trying to minimize \\(\\frac{1}{2}\\mathbf{w}^T\\mathbf{w}\\) our algorithm will try to scale down \\(\\mathbf{w}\\) until \\(\\mathbf{w}^T\\mathbf{x}_n+b\\) will touch \\(1\\) for some specific point \\(n\\) of the dataset.\nSo how can we solve this? This is a constraint optimization problem with inequality constraints, we have to derive the Lagrangian and apply the KKT (Karush–Kuhn–Tucker) conditions.\nObjective Function:\nWe have to minimize\n\\[\n\\mathcal{L}(\\mathbf{w},b,\\mathbf{\\alpha}) = \\frac{1}{2}\\mathbf{w}^T\\mathbf{w}-\\sum_{n=1}^{N}\\alpha_n(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1)\\\\\n\\]\nw.r.t. to \\(\\mathbf{w}\\) and \\(b\\) and maximize it w.r.t. the Lagrange Multipliers \\(\\alpha_n\\)\nWe can easily get the two conditions for the unconstrained part:\n\\[\n\\nabla_{\\mathbf{w}}\\mathcal{L}=\\mathbf{w}-\\sum_{n=1}^{N}\\alpha_n y_n\\mathbf{x}_n = 0 \\;\\;\\;\\;\\;\\;\\;\\; \\mathbf{w}=\\sum_{n=1}^{N}\\alpha_n y_n\\mathbf{x}_n\\\\\n\\frac{\\partial\\mathcal{L}}{\\partial b} = -\\sum_{n=1}^{N}\\alpha_n y_n = 0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sum_{n=1}^{N}\\alpha_n y_n=0\n\\]\nAnd list the other KKT conditions:\n\\[\ny_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1\\ge0\\;\\;\\;\\;\\;\\;\\forall{n}\\\\\n\\alpha_n\\ge0\\;\\;\\;\\;\\;\\;\\;\\forall{n}\\\\\n\\alpha_n(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1)=0\\;\\;\\;\\;\\;\\;\\forall{n}\n\\]\nAlert : the last condition is called the KKT dual complementary condition and will be key for showing that the SVM has only a small number of “support vectors”, and will also give us our convergence test when we’ll talk about the SMO algorithm.\nNow we can reformulate the Lagrangian by applying some substitutions\n\\[\n\\mathcal{L}(\\mathbf{w},b,\\mathbf{\\alpha}) = \\frac{1}{2}\\mathbf{w}^T\\mathbf{w}-\\sum_{n=1}^{N}\\alpha_n(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1)\\\\\n\\mathcal{L}(\\mathbf{\\alpha}) =\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n y_m\\alpha_n\\alpha_m\\mathbf{x}_n^T\\mathbf{x}_m\n\\]\n(if you have doubts just go to minute 36.50 of this excellent lecture by professor Yaser Abu-Mostafa at Caltech )\nWe end up with the dual formulation of the problem\n\\[\n\\underset{\\alpha}{\\operatorname{argmax}}\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n y_m\\alpha_n\\alpha_m\\mathbf{x}_n^T\\mathbf{x}_m\\\\\n\\;\\\\\ns.t. \\;\\;\\;\\;\\;\\;\\;\\;\\alpha_n\\ge0\\;\\;\\;\\forall{n}\\\\\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sum_{n=1}^{N}\\alpha_n y_n=0\n\\]\nWe can notice that the old constraint \\(\\mathbf{w}=\\sum_{n=1}^{N}\\alpha_n y_n\\mathbf{x}_n\\) doesn’t appear in the new formulation since it is not a constraint on \\(\\alpha\\) , it was a constraint on \\(\\mathbf{w}\\) which is not part of our formulation anymore.\nHow do we find the solution? we throw this objective (which btw happens to be a convex function) to a quadratic programming package.\nOnce the quadratic programming package gives us back the solution we find out that a whole bunch of \\(\\alpha\\) are just \\(0\\) ! All the \\(\\alpha\\) which are not \\(0\\) are the ones associated with the so-called support vectors ! ( which are just samples from our dataset )\nThey are called support vectors because they are the vectors that determine the width of the margin , this can be noted by observing the last KKT condition\n\\[\n\\big\\{\\alpha_n(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1)=0\\;\\;\\;\\forall{n}\\big\\}\n\\]\nin fact either a constraint is active, and hence the point is a support vector, or its multiplier is zero.\nNow that we solved the problem we can get both \\(\\mathbf{w}\\) and \\(b\\).\n\\[\n\\mathbf{w} = \\sum_{\\mathbf{x}_n \\in \\text{ SV}}\\alpha_ny_n\\mathbf{x}_n\\\\\ny_n(\\mathbf{w}^T\\mathbf{x}_{n\\in\\text{SV}}+b)=1\n\\]\nwhere \\(\\mathbf{x}_{n\\in\\text{SV}}\\) is any support vector. (you’d find the same \\(b\\) for every support vector)\nBut the coolest thing about SVMs is that we can rewrite our objective functions from\n\\[\n\\mathcal{L}(\\mathbf{\\alpha}) =\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n y_m\\alpha_n\\alpha_m\\mathbf{x}_n^T\\mathbf{x}_m\n\\]\nto\n\\[\n\\mathcal{L}(\\mathbf{\\alpha}) =\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n y_m\\alpha_n\\alpha_mk(\\mathbf{x}_n,\\mathbf{x}_m)\n\\]\nWe can use kernels ! (if you don’t know what I’m talking about just check this one)\nFinally we end up with the following equation for classifying new points:\n\\[\n\\hat{y}(\\mathbf{x}) = sign\\left(\\sum_{n=1}^{N}\\alpha_n y_n k(\\mathbf{x},\\mathbf{x}_n)+b\\right)\n\\]\nSoft-margin Formulation\nThe method described so far is called hard-margin SVM since the margin has to be satisfied strictly, it can happen that the points are not linearly separable in any way, or we just want to handle noisy data to avoid overfitting, so now we’re going to briefly define another version of it, which is called soft-margin SVM that allows for few errors and penalizes for them.\nWe introduce slack variables \\(\\xi_n\\) , this way we allow to violate the margin constraint but we add a penalty expressed by the distance of the misclassified samples from the hyperplane ( samples correctly classified have \\(\\xi_n=0\\)).\nWe now have to\n\\[\n\\text{Minimize}\\ \\ ||\\mathbf{w}||_2^2+C\\sum_n \\xi_n \\\\\n\\text{s.t.}\\\\ \n\\ y_n(\\mathbf{w}^Tx_n+b)\\ge1-\\xi_n\\ ,\\ \\ \\ \\forall{n}\\\\\n\\xi_n\\ge0\\ ,\\ \\ \\ \\forall{n}\n\\]\n\\(C\\) is a coefficient that allows to trade-off bias-variance and is chosen by cross-validation.\nAnd obtain the Dual Representation\n\\[\n\\text{Maximize}\\ \\ \\ \\mathcal{L}(\\mathbf{\\alpha}) =\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n y_m\\alpha_n\\alpha_mk(\\mathbf{x}_n\\mathbf{x}_m)\\\\\n  \\text{s.t.}\\\\\n  0\\le\\alpha_n\\le C\\ \\ \\ \\ \\ \\forall{n}\\\\\n  \\sum_{n=1}^N\\alpha_n y_n = 0\n\\]\nif \\(\\alpha_n\\le0\\) the point \\(x_n\\) is just correctly classified.\nif \\(0<\\alpha_n<C\\) the points lies on the margin. They are indeed Support Vectors.\nif \\(\\alpha_n = C\\) the point lies inside the margin, and it can be either correctly classified (\\(\\xi_n \\le 1\\)) or misclassified (\\(\\xi_n>1\\))\nFun fact: When \\(C\\) is large, larger slacks penalize the objective function of SVM’s more than when \\(C\\) is small. As \\(C\\) approaches infinity, this means that having any slack variable set to non-zero would have infinite penalty. Consequently, as \\(C\\) approaches infinity, all slack variables are set to \\(0\\) and we end up with a hard-margin SVM classifier.\nError bounds\nAnd what about generalization? Can we compute an Error bound in order to see if our model is overfitting?\nAs Vapnik said :\n\n“In the support-vectors learning algorithm the complexity of the construction does not depend on the dimensionality of the feature space, but on the number of support vectors.”\n\nIt’s reasonable to define an upper bound of the error as:\n\\[\nL_h\\le\\frac{\\mathbb{E}[\\text{number of support vectors}]}{N}\n\\]\nWhere \\(N\\) is the total number of samples in the dataset. The good thing is that this bound can be easily computed and we don’t need to run SVM multiple times.\n\n\n\n",
    "preview": "posts/2021-07-20-support-vector-machines/media/outer_points2D-white.png",
    "last_modified": "2021-07-20T22:09:51+02:00",
    "input_file": {},
    "preview_width": 1675,
    "preview_height": 1662
  },
  {
    "path": "posts/2021-07-20-the-hazard-function/",
    "title": "The Hazard Function",
    "description": "What is the Hazard Function in the context of Survival Analysis? How can we apply it in Computational Neuroscience?",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-07-20",
    "categories": [
      "Computational Neuroscience"
    ],
    "contents": "\nIn Survival Analysis we are interested in understanding the risk of an event happening at a particular point in time, where time is a continuous variable.\nFor example, let’s consider the event firing of a neuron: we define the time of firing as \\(X\\), and time in general as \\(t\\).\nThe hazard function, which is a function of time, is defined as:\n\\[\nh(t) = \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t|X>t)}{\\Delta t}\n\\]\nWe are conditioning on \\(X>t\\) because we want to condition our probability on the fact that the event hasn’t occurred yet.\nIs there a way to rewrite \\(h(t)\\) in a different way?\n\\[\nh(t) = \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t|X>t)}{\\Delta t}\\\\\nh(t) = \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t,X>t)}{P(X>t)\\Delta t}\\\\\n\\]\nIt is easy to see that \\((t<X<t+\\Delta t)\\) is just a subset of \\(X>t\\)\n    O---------------------- {     X > t    }\n    |       o-------------- { t < X < t+Δt }\n    |       |       \n----.-------.----.---------\n    t       X   t+Δt\n\\[\n  h(t) = \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t)}{P(X>t)\\Delta t}\n\\]\n\\(P(X>t)\\) is called the survival function and is just \\(1\\) minus the cumulative distribution function (CDF):\n\\[\n  P(X>t) = 1-F(t)=1-\\int_{t_0}^tp(t)dt\n\\]\nThe remaining part is the definition of the derivative of the CDF, which is just the probability density function (PDF) at time \\(t\\)\n\\[\n  \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t)}{\\Delta t}= \\lim_{\\Delta t\\to0}\\frac{P(X<t+\\Delta t)-P(X <t)}{\\Delta t}=\\\\\n  \\lim_{\\Delta t\\to0}\\frac{F(t+\\Delta t)-F(t)}{\\Delta t}=p(t)\n\\]\nSo, finally we can rewrite the hazard function as:\n\\[\n  h(t) = \\frac{p(t)}{1-\\int_{t_0}^tp(t)dt}\n\\]\n\n\n\n",
    "preview": "posts/2021-07-20-the-hazard-function/neuron.jpg",
    "last_modified": "2021-07-20T22:11:12+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-21-how-to-call-a-c-function-from-python/",
    "title": "How to call a C function from Python.",
    "description": "Need to speed up things by calling a C function from your Python script? Check this out.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-03-03",
    "categories": [
      "Coding",
      "Performance Optimization",
      "C",
      "Python"
    ],
    "contents": "\nThis will be a really short blogpost which mainly serves as a personal reminder on how to call a C function from Python… the title is pretty self-explanatory.\nI will show how to call some really simple c functions wich return a double or an array (which will be a np.array in Python).\nHere we have our simple c file cfunctions.c:\n\n#include<stdlib.h>\n\nlong factorial(int n){\n    // \n    long res = 1;\n    if (n <= 0) {\n        return -1;\n    }\n    else {\n        for (long i = 1; i <= n; i++) {\n           res *= i;\n        }\n    }\n    return res;\n}\n\ndouble dotproduct(int dim, double a[dim], double b[dim]){\n    // Compute the dot product between two vectors...\n    // e.g. a = [1,2,3,4] , b = [4,3,2,1]\n    // res = 1*4 + 2*3 + 3*2 + 4*1 = 4 + 6 + 6 + 4 = 20\n    double res = 0;   \n    for(int i = 0; i < dim; i++){\n        res = res + a[i]*b[i];\n    }\n    return res;\n}\n\ndouble * elementwiseproduct(int dim, double a[dim], double b[dim]){\n    // Compute the elementiwise product between two vectors...\n    // e.g. a = [1,2,3,4] , b = [4,3,2,1]\n    // res will point to an array such as [1*4, 2*3, 3*2, 4*1] = [4,6,6,4]\n    double * res = (double *) malloc(sizeof(double) * dim);\n    for(int i = 0; i < dim; i++){\n        res[i] = a[i]*b[i];\n    }    \n    return res;\n}\n\nWe then create a shared object witht the following command:\n\ncc -fPIC -shared -o cfunctions.so cfunctions.c\n\nThen we can write a Python script py_cfunctions.py such as:\n\nimport ctypes\nimport numpy as np\n\ndef factorial(num: int):\n    py_cfunctions.factorial.restype = ctypes.c_long\n    return py_cfunctions.factorial(num)\n\ndef dotproduct(dim: int, a: np.array, b: np.array):\n    # Convert np.array to ctype doubles\n    a_data = a.astype(np.double)\n    a_data_p = a_data.ctypes.data_as(c_double_p)\n    b_data = b.astype(np.double)\n    b_data_p = b_data.ctypes.data_as(c_double_p)\n    py_cfunctions.dotproduct.restype = ctypes.c_double\n    # Compute result...\n    return py_cfunctions.dotproduct(dim,a_data_p,b_data_p)\n    \ndef elementwiseproduct(dim: int, a: np.array, b: np.array):\n    # Convert np.array to ctype doubles\n    a_data = a.astype(np.double)\n    a_data_p = a_data.ctypes.data_as(c_double_p)\n    b_data = b.astype(np.double)\n    b_data_p = b_data.ctypes.data_as(c_double_p)\n\n    py_cfunctions.elementwiseproduct.restype = np.ctypeslib.ndpointer(dtype=ctypes.c_double,shape=(dim,))\n    # Compute result...\n    return py_cfunctions.elementwiseproduct(dim,a_data_p,b_data_p)\n\n# so_file genreated with:\n# cc -fPIC -shared -o cfunctions.so cfunctions.c\n\nso_file = 'MY_PATH/cfunctions.so'\npy_cfunctions = ctypes.CDLL(so_file)\nc_double_p = ctypes.POINTER(ctypes.c_double)\npy_cfunctions.factorial.argtypes = [ctypes.c_int] \npy_cfunctions.elementwiseproduct.argtypes = [ctypes.c_int, c_double_p, c_double_p]\npy_cfunctions.dotproduct.argtypes= [ctypes.c_int, c_double_p, c_double_p]\n\nAnd that’s it! You now can import py_cfunctions and call factorial(), dotproduct() and elementwiseproduct().\n\n\n\n",
    "preview": "posts/2021-07-21-how-to-call-a-c-function-from-python/c-python.png",
    "last_modified": "2021-07-22T00:49:42+02:00",
    "input_file": "how-to-call-a-c-function-from-python.knit.md",
    "preview_width": 1600,
    "preview_height": 1200
  },
  {
    "path": "posts/2021-07-20-a-boring-post-on-linear-regression/",
    "title": "A boring Post on Linear Regression",
    "description": "A visual and mathematical overview of Linear Regression.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2020-09-12",
    "categories": [
      "Machine Learning",
      "Regression"
    ],
    "contents": "\nRegression is one of the main problems tackled by machine learning.\nWe define regression analysis as a set of statistical processes for estimating the relationship between a dependent variable (often called the target variable) and one or more independent variables (often called features). In Linear Regression we aim to find the best hyperplane that is able to predict the outcome variable starting from a set of features.\nOne trivial example could be estimating the price (target variable) of a house starting from the house’s dimension expressed in squared meters (\\(m^2\\)). In this case, since we are just considering one feature (the dimension of the house expressed in \\(m^2\\)) our hyperplane will consist of a simple line.\n\n\nEach dot in the plot corresponds to a real data sample, namely a real correspondence among area and price. This is how our dataset would look like\n\n\nOur goal consists in finding the line that better explains the data we have been provided with.\n\n\nThis traduces in finding the best weights \\(\\textbf{w} = \\begin{bmatrix}w_0 \\\\ w_1\\end{bmatrix}\\) such that \\(w_0+w_1\\cdot\\left(\\text{dimension in }m^2\\right) \\simeq\\text{Price}\\)\nNow it’s time to introduce some formalism.\nFor the \\(i_{th}\\) data sample we call our target variable \\(t_i\\) , our features \\(\\mathbf{x}_i\\) and the weights that we apply to our features \\(\\mathbf{w}\\). Note that \\(\\mathbf{x}_i\\) and \\(\\mathbf{w}\\) are column vectors , while \\(t_i\\) is just a scalar.\nSuppose we have \\(n\\) data samples and we consider \\(m\\) features, then we define:\nThe target vector\n\\[\n\\mathbf{t} = \\begin{bmatrix}t_1\\\\\nt_2\\\\\n\\vdots\n\\\\\nt_n\n\\end{bmatrix}\n\\]\nThe dataset\n\\[\n\\mathbf{X}=\\begin{bmatrix}\n\\mathbf{x_1}^T\n\\\\ \n\\mathbf{x_2}^T\n\\\\ \n\\vdots\n\\\\ \n\\mathbf{x}_n^T\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 && x_{11} && \\cdots && x_{1m}\n\\\\ \n1 && x_{21} && \\cdots && x_{2m}\n\\\\ \n\\vdots && \\vdots && \\ddots &&\\vdots\n\\\\ 1 &&\\cdots && \\cdots &&x_{nm}\n\\end{bmatrix}\n\\]\nNote that the \\(1\\) at the beginning of each row are just to take in account the bias \\(w_0\\) (that would be the intercept in our trivial example)\nAnd the weights\n\\[\n\\mathbf{w}=\n\\begin{bmatrix}\nw_0\n\\\\\nw_1\n\\\\\nw_2\n\\\\\n\\vdots\n\\\\\nw_m\n\\end{bmatrix}\n\\]\nThen our prediction (that from now on we’ll call \\(y\\) ) for the \\(i_{th}\\) sample will be\n\\[\ny_i = \\mathbf{w}^T\\mathbf{x}_i = \\begin{bmatrix} \nw_0\n&&\nw_1\n&&\nw_2\n&&\n\\cdots\n&&\nw_m\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1\n\\\\\nx_{i1}\n\\\\\nx_{i2}\n\\\\\n\\vdots\n\\\\\nx_{im}\n\\end{bmatrix}\n\\]\nAnd our prediction vector (which will have dimension \\(n\\times 1\\)) will be computed as\n\\[\n\\mathbf{y}=\\begin{bmatrix}\n1 && x_{11} && \\cdots && x_{1m}\n\\\\\n1 && x_{21} && \\cdots && x_{2m}\n\\\\\n\\vdots && \\vdots && \\ddots &&\\vdots\n\\\\\n1 &&\\cdots && \\cdots &&x_{nm}\n\\end{bmatrix}\\cdot\\begin{bmatrix}\nw_0\n\\\\\nw_1\n\\\\\n\\vdots\n\\\\\nw_m\n\\end{bmatrix}=\\mathbf{X}\\mathbf{w}\n\\]\nBut how can we find the optimal weights? We do that by minimizing the so-called Mean Squared Error \\(J(\\mathbf{w})\\).\n\\[\nJ(\\mathbf{w}) =\\\\\n\\frac{1}{N}\\left(\\mathbf{t}-\\mathbf{y}\\right)^T\\left(\\mathbf{t}-\\mathbf{y}\\right)=\\\\\\frac{1}{N}\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)^T\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)\n\\]\nWhere \\(\\epsilon_i\\) is just the difference between the target values \\(t_i\\) and our predictions \\(y_i\\),\n\\[\n\\begin{bmatrix}\nt_1-y_1\n\\\\\nt_2-y_2\n\\\\\nt_3-y_3\n\\\\\n\\vdots\n\\\\\nt_n-y_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\epsilon_1\n\\\\\n\\epsilon_2\n\\\\\n\\epsilon_3\n\\\\\n\\vdots\n\\\\\n\\epsilon_n\n\\end{bmatrix}\n\\]\nTo have a visual understanding of what we’re talking about, the various \\(\\epsilon_i\\) corresponds to the green segments in the image below.\n\nOur cost function is just the Mean Squared Error\n\\[\nJ(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^n\\epsilon_i^2\n\\]\nSince we would like to minimize this quantity, we derive with respect to \\(\\mathbf{w}\\) and set the derivative equal to \\(0\\) .\n\\[\nJ\\left(\\mathbf{w}\\right) =\n\\frac{1}{N}\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)^T\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)\\\\\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} =\n-\\frac{2}{N}\\mathbf{X}^T\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)\n=0\n\\]\nWhich is equivalent to\n\\[\n\\mathbf{X}^T\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)=0\n\\]\nWe then isolate the weights\n\\[\n\\mathbf{X}^T\\mathbf{t}=\\mathbf{X}^T\\mathbf{X}\\mathbf{w}\\\\\n\\mathbf{w}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n\\]\nAnd that’s all!\nNow, before tackling the problems relative with such closed form solution, it is useful to introduce the parameters-space, i.e. the space representing all the possible solutions \\((w)\\) of our problem: in our trivial example this space corresponds to all the possible points \\(w:(w_0,w_1) \\in \\mathbb{R}^2\\), each of this points traduces in a different predictor (line) in the features-space as you can see in the animation below.\nLet’s talk now about some problems that can arise from the closed form solution\nThe inverse of \\(\\left(\\mathbf{X}^T\\mathbf{X}\\right)\\) is computationally expensive when the number of features is high, being the temporal complexity of such inversion \\(\\mathcal{O}(m^3)\\) (Note that \\(\\left(\\mathbf{X}^T\\mathbf{X}\\right)\\) has dimensionality equal to \\((m+1)\\times (m+1)\\) )\nThe matrix \\(\\left(\\mathbf{X}^T\\mathbf{X}\\right)\\) isn’t always invertible since \\(\\mathbf{X}^T\\mathbf{X}\\) could contain linearly dependent rows, which implies a null determinant. In such case we’d have infinite solutions for \\(\\mathbf{w}\\).\nIn order to show this last drawback we’ll use a toy example:\nWe have to find a valid predictor for a dataset which contains just three samples.\n\n\nWe solve by means of the closed form solution:\n\\[\n\\mathbf{X}\\mathbf{w}=\\mathbf{t}\n\\]\n\\[\n\\begin{bmatrix}1 && 1\\\\1 && 0\\\\1 && -1\\\\\\end{bmatrix}\\cdot\\begin{bmatrix}\\mathbf{w}_0\\\\\\mathbf{w}_1\\\\\\end{bmatrix}=\\begin{bmatrix}4.1\\\\1.9\\\\0\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n\\]\n\\[\n\\mathbf{w} = \n\\begin{bmatrix}\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 && 1\\\\\n1 && 0\\\\\n1 && -1\\\\\n\\end{bmatrix}\n\\end{bmatrix}\n^{-1}\n\\cdot\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \n\\begin{bmatrix}\n3 && 0\\\\\n0 && 2\n\\end{bmatrix}\n^{-1}\n\\cdot\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \n\\frac{1}{6}\n\\begin{bmatrix}\n2 && 0\\\\\n0 && 3\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \n\\begin{bmatrix}\n\\frac{1}{3} && 0\\\\\n0 && \\frac{1}{2} \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \n\\begin{bmatrix}\n \\frac{1}{3} &&  \\frac{1}{3}  && \\frac{1}{3}\\\\\n \\frac{1}{2} &&  0  && -\\frac{1}{2}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \n\\begin{bmatrix}\n2.00\\\\\n2.05\n\\end{bmatrix}\n\\]\nIn this case \\(\\mathbf{X}^T\\mathbf{X}\\) is invertible and, if we plot the cost function \\(J(\\mathbf{w})\\) in the parameter space, we can see that \\(J(\\mathbf{w})\\) is a convex function with one single minimum and a well defined bowl shape. This minimum corresponds to the point \\(\\mathbf{w} = \\begin{bmatrix} 2.00\\\\ 2.05 \\end{bmatrix}\\), i.e. the blue dot which appears at the base of the bowl.\nWhich corresponds to the following predictor line:\n\n\nThe above example is the ideal scenario, but it is not always the one you’ll be dealing with. In fact there are some observations to be done regarding the dataset to be used:\nNow let’s compute the closed form solution for another example:\n\\[\n\\begin{bmatrix}m^2 && price\\\\2 && 2\\\\2 && 4\\\\2 && 6\\\\\\end{bmatrix}\n\\]\n\n\nHere our data samples are pretty bad since there doesn’t seem to be any correlation between the independent variable \\((m^2)\\) and the dependent variable \\((price)\\). We will observe that the inversion of the matrix \\(\\mathbf{X}^T\\mathbf{X}\\) becomes problematic.\n\\[\n\\mathbf{X}\\mathbf{w}=\\mathbf{t}\n\\]\n\\[\n\\begin{bmatrix}1 && 2\\\\1 && 2\\\\1 && 2\\\\\\end{bmatrix}\\cdot\\begin{bmatrix}\\mathbf{w}_0\\\\\\mathbf{w}_1\\\\\\end{bmatrix}=\\begin{bmatrix}2\\\\4\\\\6\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n\\]\n\\[\n\\mathbf{w} =\\begin{bmatrix}\\begin{bmatrix}1 && 1 && 1\\\\2 && 2 && 2\\\\\\end{bmatrix}\\cdot\\begin{bmatrix}1 && 2\\\\1 && 2\\\\1 && 2\\\\\\end{bmatrix}\\end{bmatrix}^{-1}\\cdot\\begin{bmatrix}1 && 1 && 1\\\\2 && 2 && 2\\\\\\end{bmatrix}\\begin{bmatrix}2\\\\4\\\\6\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \\begin{bmatrix}  3&& 6\\\\6 && 12\\end{bmatrix}^{-1}\\cdot\\begin{bmatrix}1 && 1 && 1\\\\2 && 2 && 2\\\\\\end{bmatrix}\\cdot\\begin{bmatrix}2\\\\4\\\\6\\end{bmatrix}\n\\]\nAs you can see here we can’t invert \\[\n\\begin{bmatrix}\n3\n&&\n6\n\\\\\n6\n&&\n12\n\\end{bmatrix}\n\\] since its determinant would be \\(0\\) !\nBy plotting the cost function \\(J(\\mathbf{w})\\) we would obtain a sort of parabolic cylinder\nwhose minimum has infinite solutions (that we can find by gradient-based techniques).\nLastly it is opportune to remember that our dataset needs to have a number of samples greater than the number of parameters to be learnt. Being \\(m\\) the number of features of each sample and \\(n\\) the number of samples in our dataset, we need to satisfy the constraint \\(n>m+1\\). Let’s reason why through a simple example. Suppose that you have been asked to find the model that best explains a dataset made of \\(2\\) samples \\((n=2)\\). Suppose that each sample is represented by \\(2\\) features \\((m=2)\\) and that we need to predict a value \\(t\\). In this scenario we are not satisfying the constraint since \\(2\\not>2+1\\). What are the consequences? Of course we will get a perfect fit! There are infinite planes \\((w_0+w_1x_1+w_2x_2=0)\\) which pass through two points!\nWe’ll find at least a model that passes through all the data, more specifically we’d find one solution in the \\(n=m+1\\) case, and infinite solutions in the \\(n<m+1\\) case. We have found a very bad family of models because they are an exact representation of the training data, which translates into a very obvious overfitting problem. Generally we want \\(n >> m\\).\n\n\n\n",
    "preview": "posts/2021-07-20-a-boring-post-on-linear-regression/preview.png",
    "last_modified": "2021-07-20T22:40:44+02:00",
    "input_file": {},
    "preview_width": 2478,
    "preview_height": 1738
  }
]

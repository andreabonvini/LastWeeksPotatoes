[
  {
    "path": "posts/2022-06-20-getting-lazy-with-cpp/",
    "title": "Getting Lazy with Cpp",
    "description": "How to easily create a lazy generator in cpp.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2022-06-20",
    "categories": [
      "Software Engineering",
      "cpp"
    ],
    "contents": "\nWhen coding in Python, it may happen that we make use of the\nso-called generators. A generator is a function that\ncontains at least one yield statement (it may contain other\nyield or return statements). Both yield and return will return some\nvalue from this function.\nThe difference is that while a return statement terminates a\nfunction entirely, yield statement pauses the function saving\nall its states and later continues from there on successive calls. This\ncomes in handy when we want to generate elements in lazy-fashion without\nthe need to create a real iterator.\nUnfortunately in cpp we don’t have generators and we have to\nrely exclusively on iterators, the good news is that as long as\nwe understand some basic concepts we are good to go.\nTo illustrate this concept, we’ll create a generator of Fibonacci\nnumbers as an example.\nHere it is a Fibonacci generator in Python:\n\ndef fibonacci(n: int):\n    current = 1\n    old = 1\n    for _ in range(n):\n        yield current\n        tmp = current \n        current = current + old\n        old = tmp\n    \nfor val in fibonacci(10):\n    print(val)\n1 2 3 5 8 13 21 34 55 89\n\nLet’s start by checking what will be the usage of our generator in a\ngeneric cpp executable:\n\n\nint main() {\n    // Create a generator instance for retrieving the first 10 Fibonacci numbers.\n    auto fibonacciGenerator = FibonacciGenerator(10); // 1, 2, 3, 5, 8, 13, 21, 34, 55, 89\n    // Ideally, we want to use range-based for loop\n    for(auto el: fibonacciGenerator) {\n      std::cout << el << std.:endl;\n    }\n}\n\nThis is just syntactic sugar for the following code:\n\nfor(auto it = fibonacciIterator.begin(); it != fibonacciIterator.end(); it++) {\n        // Case1: for(auto el: fibonacciGenerator){ // do stuff }\n        // Observe that we are creating a copy by value \n        // (but since we're dealing with unsigned long variables, this is costless)\n        auto elem = *it;\n  \n        // Case2: for(auto& el: fibonacciGenerator){ // do stuff }\n        // Now you're directly modifying the elements\n        // because elem1 is an lvalue reference\n        // auto& elem1 = *it;\n        \n        // Case3: for(const auto& el: fibonacciGenerator){ // do stuff }\n        // You just want to read stuff, no modification allowed\n        // const auto& elem3 = *it;\n        \n        std::cout << elem << std::endl;\n        \n        }\n\nSo we somehow have to define two methods\nFibonacciGenerator::begin() and\nFibonacciGenerator::end(), these two methods have to return\nan iterator object (it) that will contain useful\ninformation about the starting and final configuration of our\nFibonacciGenerator instance. So we may define an\nIterator struct inside our FibonacciGenerator\nclass as\n\n\nclass FibonacciGenerator {\npublic:\n    // ============================ Start Iterator definition ================================\n    struct Iterator\n    {\n      // ...\n    }\n    // ============================ End Iterator definition ==================================\n\n    explicit FibonacciGenerator(unsigned int n){\n        this->n = n;\n    }\n\n    Iterator begin(){\n        // We will use an index to denote the current state of the fibonacci sequence\n        // When we instantiate a FibonacciGenerator object this index will be 0.\n        return FibonacciGenerator::Iterator(this, 0);\n    }\n    Iterator end(){\n        // At the end of the generation process, this index will be exactly n.\n        return FibonacciGenerator::Iterator(this, n);\n    }\n\nBefore looking at the actual implementation of the\nIterator struct, we observe that all we need to do is to\noverride the following operators:\noperator()*\noperator()++\noperator==(Iterator other)\noperator!=(Iterator other)\noperator()* will have the task of dereferencing the\nvalue that we want to retrieve at each iteration,\noperator()++ will instead actually update our\nvalue (and for that we will use an helper function\nFibonacciGenerator::Iterator::update(), what will carry out\nthe actual new-value generation process), while the last two operators\nwill be necessary for the ending condition of our range-based for\nloop.\nHere it is a possible implementation:\n\n\n#include <iostream>\n\n\nclass FibonacciGenerator {\npublic:\n    // ============================ Start Iterator definition ================================\n    struct Iterator\n    {\n    public:\n        using obj_type = FibonacciGenerator;\n        // Constructor definition (used in FibonacciSequence::begin() and\n        //  FibonacciSequence::end())\n        explicit Iterator(obj_type* obj_ptr, unsigned long index): obj_ptr_ {obj_ptr}, index_ {index} {}\n        // In case of a Fibonacci sequence we want our value to be \n        //  just a number (unsigned long to be precise)\n        using value_type = unsigned long;\n        using reference = const value_type&;\n\n        // Pre-increment operator\n        value_type operator++() { increment(); return obj_ptr_->current_; }\n        // Post-increment operator\n        value_type operator++(int) { value_type x = obj_ptr_->current_; increment(); return x; }\n        \n        // Dereference operator, will return the current value saved in FibonacciGenerator.\n        reference operator*() const { return obj_ptr_->current_; }\n\n        bool operator==(Iterator other) const { return this->index_ == other.index_; }\n        bool operator!=(Iterator other) const { return this->index_ != other.index_; }\n        \n    private:\n\n        obj_type* obj_ptr_;\n        unsigned long index_;\n\n        void increment()\n        {\n            unsigned long tmp;\n            tmp = obj_ptr_->current_;\n            // Fibonacci update.\n            obj_ptr_->current_ = obj_ptr_->current_ + obj_ptr_->old_;\n            obj_ptr_->old_ = tmp;\n            // Of course we need to incremen the Iterator index.\n            index_++;\n        }\n    };\n\n    // ============================ End Iterator definition ==================================\n\n    explicit FibonacciGenerator(unsigned int n){\n        // n represents the maximum length of the Fibonacci sequence created by an instance of FibonacciGenerator\n        this->n = n;\n    }\n\n    Iterator begin(){\n        // We will use an index to denote the current state of the fibonacci sequence\n        // When we instantiate a FibonacciGenerator object this index will be 0.\n        return FibonacciGenerator::Iterator(this, 0);\n    }\n    Iterator end(){\n        // At the end of the generation process, this index will be exactly n.\n        return FibonacciGenerator::Iterator(this, n);\n    }\n\n\n\nprivate:\n    // Here we can define all the variables we need to carry out our generation process\n    unsigned long n;\n    unsigned long current_ = 1;\n    unsigned long old_ = 1;\n\n};\n\n\n\nint main() {\n    auto fibonacciGenerator = FibonacciGenerator(10); // 1, 2, 3, 5, 8, 13, 21, 34, 55, 89\n    for(auto el: fibonacciGenerator) {\n      std::cout << el << std::endl;\n    }\n}\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-02-25T22:53:35+01:00",
    "input_file": "getting-lazy-with-cpp.knit.md"
  },
  {
    "path": "posts/2021-07-20-support-vector-machines/",
    "title": "Support Vector Machines",
    "description": "The math behind the Support Vector Machines algorithm.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-07-20",
    "categories": [
      "Machine Learning",
      "Classification"
    ],
    "contents": "\nIntroduction & Brief\nHistory\nIn this blog-post we are gonna talk about one of the most powerful\nand fascinating techniques in Machine Learning: Support Vector\nMachines.\nIn the field of Statistical Learning the Support Vector\nMachine technique is a binary classification algorithm\nwhich aims to find the hyperplane which is able to separate the data\nwith the largest margin possible. The concept of margin is\nillustrated in the following images.\nSuppose we have a set of points in \\(\\mathbb{R}^2\\), each point belongs to a\nclass \\(\\in\\{-1,+1\\}\\)\n\n\nWe want to find the best hyperplane (in this case a line)\nwhich is able to correctly separate the data.\n\n\nWe identify this hyperplane by maximizing the margin,\ni.e. the distance from the hyperplane to the closest points of\nboth classes, we call this points support vectors.\n\n\nIn this case we identified two support vectors, they are\ncalled like that because they support the dashed lines, which represent\nthe set of points equidistant from the separating hyperplane.\n\n\nThe margins from the support vectors to the hyperplane are\ndrawed in red\n\n\nBefore diving into the theory of the algorithm let’s have a look at\nthe history behind it.\nThe birth of SVMs dates back to \\(1963\\) in Russia, when Vladimir\nVapnik and Aleksondr Lerner introduced the Generalized\nPortrait algorithm.\nAfter almost \\(30\\) years, at the\nend of \\(1990\\), Vapnik moved\nto the USA and joined Bernhard Boser and Isabelle\nGuyen at the Adaptive Systems Research Department at AT&T Bell\nLabs in New Jersey, where the algorithm was refined.\n\n“The invention of SVMs happened when Bernhard decided to\nimplement Vladimir’s algorithm in the three months we had left before we\nmoved to Berkeley. After some initial success of the linear algorithm,\nVladimir suggested introducing products of features. I proposed to\nrather use the kernel trick of the ‘potential function’ algorithm.\nVladimir initially resisted the idea because the inventors of the\n‘potential functions’ algorithm (Aizerman, Braverman, and Rozonoer) were\nfrom a competing team of his institute back in the 1960’s in Russia! But\nBernhard tried it anyways, and the SVMs were born!”\n\nIsabelle Guyen\nPremise on linear\nclassifiers\nFor a binary classification problem, one can visualize the\noperation of a linear classifier as splitting a high-dimensional input\nspace of dimension \\(d\\) with an\nhyperplane of dimension \\(d\\)\n(which, as you’ll see in a minute, corresponds to a \\(d-1\\) dimensional space): all points on one\nside of the hyperplane are classified as \\(+1\\) (or \\(-1\\)), while the others are classified as\n\\(-1\\) (or \\(+1\\)). In case you doubt the power of\nlinear classifiers just observe that we’re always able to transform (or\nenrich) our input space by means of some basis functions, if we\n“guess” the right transformation maybe we are able to correctly classify\nour samples with a linear classifier.\nIf, for instance, we have the following unseparable data in the 2D\nspace\n\n\nthere’s nothing stopping us from enriching the input space with some\nnew coordinates which depend on the old features, e.g. by adding a ne w\ndimension \\(x_3 =\n\\sqrt{x_1^2+x_2^2}\\).\nThis way, in the new 3D input space, we are able to correctly\nclassify the data by means of a 2D plane.\nDerivation\nFirst of all, we should be familiar with the equation of a generic\n\\(D\\)-dimensional hyperplane:\n\\[\n\\text{hyperplane: }\\\\\n\\mathbf{w}^T\\mathbf{x}=0\\\\\n\\mathbf{w} = [w_0,w_1,\\dots,w_D]^T\\\\\n\\mathbf{x} = [1,x_1,\\dots,x_D]^T\n\\]\nIf \\(D=2\\) we have that\n\\[\n\\text{hyperplane: }\\\\\nw_0+w_1x_1+w_2x_2=0\\\\\n\\mathbf{w} = [w_0,w_1,w_2]^T\\\\\n\\mathbf{x} = [1,x_1,x_2]^T\n\\]\nLet \\(\\mathbf{x}_N\\) be the nearest\ndata point to the hyperplane \\(\\mathbf{w}^T\\mathbf{x} = 0\\) , before\nfinding the distance we just have to state two observations:\nLet’s say I multiply the vector \\(\\mathbf{w}\\) by \\(1000000\\) , I get the same\nhyperplane! So any formula that takes \\(\\mathbf{w}\\) and produces the margin will\nhave to have built-in scale-invariance, we do that by\nnormalizing \\(\\mathbf{w}\\) , requiring\nthat for the nearest data point \\(\\mathbf{x}_N\\): \\[\n  |\\mathbf{w}^T\\mathbf{x}_N|=1\n  \\] ( So I just scale \\(\\mathbf{w}\\) up and down in order to\nfulfill the condition stated above, we just do it because it’s\nmathematically convenient! By the way remember that \\(1\\) does not represent the\nEuclidean distance)\nWhen you solve for the margin, the \\(w_1\\) to \\(w_d\\) will play a completely different role\nfrom the role of \\(w_0\\) , so it is no\nlonger convenient to have them on the same vector. We pull out \\(w_0\\) from \\(\\mathbf{w}\\) and rename \\(w_0\\) with \\(b\\) (for bias).\n\\[\n\\mathbf{w} = (w_1,\\dots,w_d)\\\\w_0=b\n\\]\nSo now our notation is changed:\nThe hyperplane is represented by\n\\[\n\\mathbf{w}^T\\mathbf{x} +b= 0\n\\]\nand our constraint becomes\n\\[\n|\\mathbf{w}^T\\mathbf{x}_N+b|=1\n\\]\nIt’s trivial to demonstrate that the vector \\(\\mathbf{w}\\) is orthogonal to the\nhyperplane, just suppose to have two point \\(\\mathbf{x}'\\) and \\(\\mathbf{x''}\\) belonging to the\nhyperplane , then \\(\\mathbf{w}^T\\mathbf{x}' +b= 0\\) and\n\\(\\mathbf{w}^T\\mathbf{x}'' +b=\n0\\).\nAnd of course \\(\\mathbf{w}^T\\mathbf{x}'' +b -\n(\\mathbf{w}^T\\mathbf{x}'\n+b)=\\mathbf{w}^T(\\mathbf{x}''-\\mathbf{x}') = 0\\)\nSince \\(\\mathbf{x}''-\\mathbf{x}'\\) is a\nvector which lays on the hyperplane , we deduce that \\(\\mathbf{w}\\) is orthogonal to the\nhyperplane.\n\n\nThen the distance from \\(\\mathbf{x}_N\\) to the hyperplane\ncan be expressed as a dot product between \\(\\mathbf{x}_N-\\mathbf{x}\\) (where \\(\\mathbf{x}\\) is any point belonging to the\nplane) and the unit vector \\(\\hat{\\mathbf{w}}\\) where \\(\\hat{\\mathbf{w}} =\n\\frac{\\mathbf{w}}{\\vert\\vert\\mathbf{w}\\vert\\vert}\\)\n(the distance is just the projection of \\(\\mathbf{x}_N-\\mathbf{x}\\) in the direction\nof \\(\\hat{\\mathbf{w}}\\)!)\n\\[\ndistance = |\\hat{\\mathbf{w}}^T(\\mathbf{x}_N-\\mathbf{x})|\n\\]\nWe take the absolute value since we don’t know if \\(\\mathbf{w}\\) is facing \\(\\mathbf{x}_N\\) or is facing the other\ndirection\n\n\nWe’ll now try to simplify our notion of distance.\n\\[\n\\text{distance} = |\\hat{\\mathbf{w}}^T(\\mathbf{x}_N-\\mathbf{x})\\;| =\n\\frac{1}{||\\mathbf{w}||}|\\;\\mathbf{w}^T\\mathbf{x}_N-\\mathbf{w}^T\\mathbf{x}|\n\\]\nThis can be simplified if we add and subtract the missing term \\(b\\).\n\\[\ndistance =\n\\frac{1}{||\\mathbf{w}||}|\\;\\mathbf{w}^T\\mathbf{x}_N+b-\\mathbf{w}^T\\mathbf{x}-b\\;|\n=\n\\frac{1}{||\\mathbf{w}||}|\\;\\mathbf{w}^T\\mathbf{x}_N+b-(\\mathbf{w}^T\\mathbf{x}+b)\\;|\n\\]\nWell, \\(\\mathbf{w}^T\\mathbf{x}+b\\)\nis just the value of the equation of the plane…for a point on\nthe plane. So without any doubt \\(\\mathbf{w}^T\\mathbf{x}+b= 0\\) , our notion\nof distance becomes\n\\[\ndistance = \\frac{1}{||\\mathbf{w}||}|\\;\\mathbf{w}^T\\mathbf{x}_N+b\\;|\n\\]\nBut wait… what is \\(\\vert\\mathbf{w}^T\\mathbf{x}_N+b\\vert\\) ? It\nis the constraint that we defined at the beginning of our\nderivation!\n\\[\n\\vert\\mathbf{w}^T\\mathbf{x}_N+b\\vert=1\n\\]\nSo we end up with the formula for the distance being just\n\\[\ndistance = \\frac{1}{\\vert\\vert\\mathbf{w}\\vert\\vert}\n\\]\nLet’s now formulate the optimization problem, we have:\n\\[\n\\underset{w}{\\operatorname{max}}\\frac{1}{||\\mathbf{w}||}\\\\\\text{subject\nto}\\;\\underset{n=1,2,\\dots,N}{\\operatorname{min}}|\\mathbf{w}^T\\mathbf{x}_n+b|=1\n\\]\nSince this is not a friendly optimization problem (the\nconstraint is characterized by a minimum and an absolute, which are\nannoying) we are going to find an equivalent problem which is easier to\nsolve. Our optimization problem can be rewritten as\n\\[\n\\underset{w}{\\operatorname{min}}\\frac{1}{2}\\mathbf{w}^T\\mathbf{w}\n\\\\\n\\text{subject to} \\ \\ \\ \\  y_n \\cdot(\\mathbf{w}^T\\mathbf{x}_n+b)\\ge1\n\\;\\;\\;\\;\\text{for $n = 1,2,\\dots,N$}\n\\]\nwhere \\(y_n\\) is a variable that we\nintroduce that will be equal to either \\(+1\\) or \\(-1\\) accordingly to its real target value\n(remember that this is a supervised learning technique and we\nknow the real target value of each sample). One could argue that the new\nconstraint is actually different from the former one, since maybe the\n\\(\\mathbf{w}\\) that we’ll find will\nallow the constraint to be strictly greater than \\(1\\) for every possible point in our dataset\n[ \\(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)> 1\n\\;\\;\\forall{n}\\) ] while we’d like it to be exactly\nequal to \\(1\\) for at least\none value of \\(n\\). But that’s actually\nnot true! Since we’re trying to minimize \\(\\frac{1}{2}\\mathbf{w}^T\\mathbf{w}\\) our\nalgorithm will try to scale down \\(\\mathbf{w}\\) until \\(\\mathbf{w}^T\\mathbf{x}_n+b\\) will touch\n\\(1\\) for some specific point \\(n\\) of the dataset.\nSo how can we solve this? This is a constraint optimization problem\nwith inequality constraints, we have to derive the Lagrangian\nand apply the KKT (Karush–Kuhn–Tucker) conditions.\nObjective Function:\nWe have to minimize\n\\[\n\\mathcal{L}(\\mathbf{w},b,\\mathbf{\\alpha}) =\n\\frac{1}{2}\\mathbf{w}^T\\mathbf{w}-\\sum_{n=1}^{N}\\alpha_n(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1)\\\\\n\\]\nw.r.t. to \\(\\mathbf{w}\\)\nand \\(b\\) and maximize it\nw.r.t. the Lagrange Multipliers \\(\\alpha_n\\)\nWe can easily get the two conditions for the unconstrained part:\n\\[\n\\nabla_{\\mathbf{w}}\\mathcal{L}=\\mathbf{w}-\\sum_{n=1}^{N}\\alpha_n\ny_n\\mathbf{x}_n = 0 \\;\\;\\;\\;\\;\\;\\;\\; \\mathbf{w}=\\sum_{n=1}^{N}\\alpha_n\ny_n\\mathbf{x}_n\\\\\n\\frac{\\partial\\mathcal{L}}{\\partial b} = -\\sum_{n=1}^{N}\\alpha_n y_n =\n0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sum_{n=1}^{N}\\alpha_n y_n=0\n\\]\nAnd list the other KKT conditions:\n\\[\ny_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1\\ge0\\;\\;\\;\\;\\;\\;\\forall{n}\\\\\n\\alpha_n\\ge0\\;\\;\\;\\;\\;\\;\\;\\forall{n}\\\\\n\\alpha_n(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1)=0\\;\\;\\;\\;\\;\\;\\forall{n}\n\\]\nAlert : the last condition is called the KKT dual\ncomplementary condition and will be key for showing that the SVM\nhas only a small number of “support vectors”, and will also give us our\nconvergence test when we’ll talk about the SMO algorithm.\nNow we can reformulate the Lagrangian by applying some\nsubstitutions\n\\[\n\\mathcal{L}(\\mathbf{w},b,\\mathbf{\\alpha}) =\n\\frac{1}{2}\\mathbf{w}^T\\mathbf{w}-\\sum_{n=1}^{N}\\alpha_n(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1)\\\\\n\\mathcal{L}(\\mathbf{\\alpha})\n=\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n\ny_m\\alpha_n\\alpha_m\\mathbf{x}_n^T\\mathbf{x}_m\n\\]\n(if you have doubts just go to minute 36.50 of this excellent\nlecture by professor Yaser Abu-Mostafa at Caltech )\nWe end up with the dual formulation of the problem\n\\[\n\\underset{\\alpha}{\\operatorname{argmax}}\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n\ny_m\\alpha_n\\alpha_m\\mathbf{x}_n^T\\mathbf{x}_m\\\\\n\\;\\\\\ns.t. \\;\\;\\;\\;\\;\\;\\;\\;\\alpha_n\\ge0\\;\\;\\;\\forall{n}\\\\\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sum_{n=1}^{N}\\alpha_n y_n=0\n\\]\nWe can notice that the old constraint \\(\\mathbf{w}=\\sum_{n=1}^{N}\\alpha_n\ny_n\\mathbf{x}_n\\) doesn’t appear in the new formulation since it\nis not a constraint on \\(\\alpha\\) , it was a constraint on \\(\\mathbf{w}\\) which is not part of our\nformulation anymore.\nHow do we find the solution? we throw this objective (which btw\nhappens to be a convex function) to a quadratic\nprogramming package.\nOnce the quadratic programming package gives us back the\nsolution we find out that a whole bunch of \\(\\alpha\\) are just \\(0\\) ! All the \\(\\alpha\\) which are not \\(0\\) are the ones associated with the\nso-called support vectors ! ( which are just samples from our\ndataset )\nThey are called support vectors because they are the vectors\nthat determine the width of the margin , this can be noted by\nobserving the last KKT condition\n\\[\n\\big\\{\\alpha_n(y_n(\\mathbf{w}^T\\mathbf{x}_n+b)-1)=0\\;\\;\\;\\forall{n}\\big\\}\n\\]\nin fact either a constraint is active, and hence the point is a\nsupport vector, or its multiplier is zero.\nNow that we solved the problem we can get both \\(\\mathbf{w}\\) and \\(b\\).\n\\[\n\\mathbf{w} = \\sum_{\\mathbf{x}_n \\in \\text{ SV}}\\alpha_ny_n\\mathbf{x}_n\\\\\ny_n(\\mathbf{w}^T\\mathbf{x}_{n\\in\\text{SV}}+b)=1\n\\]\nwhere \\(\\mathbf{x}_{n\\in\\text{SV}}\\)\nis any support vector. (you’d find the same \\(b\\) for every support vector)\nBut the coolest thing about SVMs is that we can rewrite our\nobjective functions from\n\\[\n\\mathcal{L}(\\mathbf{\\alpha})\n=\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n\ny_m\\alpha_n\\alpha_m\\mathbf{x}_n^T\\mathbf{x}_m\n\\]\nto\n\\[\n\\mathcal{L}(\\mathbf{\\alpha})\n=\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n\ny_m\\alpha_n\\alpha_mk(\\mathbf{x}_n,\\mathbf{x}_m)\n\\]\nWe can use kernels ! (if you don’t know what I’m talking\nabout just check this\none)\nFinally we end up with the following equation for classifying new\npoints:\n\\[\n\\hat{y}(\\mathbf{x}) = sign\\left(\\sum_{n=1}^{N}\\alpha_n y_n\nk(\\mathbf{x},\\mathbf{x}_n)+b\\right)\n\\]\nSoft-margin Formulation\nThe method described so far is called hard-margin SVM since\nthe margin has to be satisfied strictly, it can happen that the points\nare not linearly separable in any way, or we just want\nto handle noisy data to avoid overfitting, so now we’re going\nto briefly define another version of it, which is called soft-margin\nSVM that allows for few errors and penalizes for them.\nWe introduce slack variables \\(\\xi_n\\) , this way we allow to\nviolate the margin constraint but we add a penalty\nexpressed by the distance of the misclassified samples from the\nhyperplane ( samples correctly classified have \\(\\xi_n=0\\)).\nWe now have to\n\\[\n\\text{Minimize}\\ \\ ||\\mathbf{w}||_2^2+C\\sum_n \\xi_n \\\\\n\\text{s.t.}\\\\\n\\ y_n(\\mathbf{w}^Tx_n+b)\\ge1-\\xi_n\\ ,\\ \\ \\ \\forall{n}\\\\\n\\xi_n\\ge0\\ ,\\ \\ \\ \\forall{n}\n\\]\n\\(C\\) is a coefficient that allows\nto trade-off bias-variance and is chosen by\ncross-validation.\nAnd obtain the Dual Representation\n\\[\n\\text{Maximize}\\ \\ \\ \\mathcal{L}(\\mathbf{\\alpha})\n=\\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}y_n\ny_m\\alpha_n\\alpha_mk(\\mathbf{x}_n\\mathbf{x}_m)\\\\\n  \\text{s.t.}\\\\\n  0\\le\\alpha_n\\le C\\ \\ \\ \\ \\ \\forall{n}\\\\\n  \\sum_{n=1}^N\\alpha_n y_n = 0\n\\]\nif \\(\\alpha_n\\le0\\) the point \\(x_n\\) is just correctly classified.\nif \\(0<\\alpha_n<C\\) the points\nlies on the margin. They are indeed Support Vectors.\nif \\(\\alpha_n = C\\) the point lies\ninside the margin, and it can be either correctly\nclassified (\\(\\xi_n \\le 1\\)) or\nmisclassified (\\(\\xi_n>1\\))\nFun fact: When \\(C\\) is large,\nlarger slacks penalize the objective function of SVM’s more than when\n\\(C\\) is small. As \\(C\\) approaches infinity, this means that\nhaving any slack variable set to non-zero would have infinite penalty.\nConsequently, as \\(C\\) approaches\ninfinity, all slack variables are set to \\(0\\) and we end up with a hard-margin SVM\nclassifier.\nError bounds\nAnd what about generalization? Can we compute an Error bound\nin order to see if our model is overfitting?\nAs Vapnik said :\n\n“In the support-vectors learning algorithm the complexity of the\nconstruction does not depend on the dimensionality of the feature space,\nbut on the number of support vectors.”\n\nIt’s reasonable to define an upper bound of the error as:\n\\[\nL_h\\le\\frac{\\mathbb{E}[\\text{number of support vectors}]}{N}\n\\]\nWhere \\(N\\) is the total number of\nsamples in the dataset. The good thing is that this bound can be easily\ncomputed and we don’t need to run SVM multiple times.\n\n\n\n",
    "preview": {},
    "last_modified": "2025-02-25T22:36:18+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-20-the-hazard-function/",
    "title": "The Hazard Function",
    "description": "What is the Hazard Function in the context of Survival Analysis? How can we apply it in Computational Neuroscience?",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-07-10",
    "categories": [
      "Computational Neuroscience"
    ],
    "contents": "\nIn Survival Analysis we are interested in understanding the\nrisk of an event happening at a particular point in time, where\ntime is a continuous variable.\nFor example, let’s consider the event firing of a neuron: we\ndefine the time of firing as \\(X\\), and\ntime in general as \\(t\\).\nThe hazard function, which is a function of time, is defined\nas:\n\\[\nh(t) = \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t|X>t)}{\\Delta\nt}\n\\]\nWe are conditioning on \\(X>t\\)\nbecause we want to condition our probability on the fact that the event\nhasn’t occurred yet.\nIs there a way to rewrite \\(h(t)\\)\nin a different way?\n\\[\nh(t) = \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t|X>t)}{\\Delta\nt}\\\\\nh(t) = \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta\nt,X>t)}{P(X>t)\\Delta t}\\\\\n\\]\nIt is easy to see that \\((t<X<t+\\Delta t)\\) is just a subset\nof \\(X>t\\)\n    O---------------------- {     X > t    }\n    |       o-------------- { t < X < t+Δt }\n    |       |       \n----.-------.----.---------\n    t       X   t+Δt\n\\[\n  h(t) = \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta\nt)}{P(X>t)\\Delta t}\n\\]\n\\(P(X>t)\\) is called the\nsurvival function and is just \\(1\\) minus the cumulative distribution\nfunction (CDF):\n\\[\n  P(X>t) = 1-F(t)=1-\\int_{t_0}^tp(t)dt\n\\]\nThe remaining part is the definition of the derivative of the\nCDF, which is just the probability density function\n(PDF) at time \\(t\\)\n\\[\n  \\lim_{\\Delta t\\to0}\\frac{P(t<X<t+\\Delta t)}{\\Delta t}=\n\\lim_{\\Delta t\\to0}\\frac{P(X<t+\\Delta t)-P(X <t)}{\\Delta t}=\\\\\n  \\lim_{\\Delta t\\to0}\\frac{F(t+\\Delta t)-F(t)}{\\Delta t}=p(t)\n\\]\nSo, finally we can rewrite the hazard function as:\n\\[\n  h(t) = \\frac{p(t)}{1-\\int_{t_0}^tp(t)dt}\n\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2025-02-25T22:52:15+01:00",
    "input_file": "the-hazard-function.knit.md"
  },
  {
    "path": "posts/2021-07-22-a-quick-overview-of-logistic-regression/",
    "title": "A quick overview of Logistic Regression.",
    "description": "A pretty basic technique for binary classification.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-05-21",
    "categories": [
      "Machine Learning",
      "Classification"
    ],
    "contents": "\nAlthough the name might confuse, please note that this is a\nclassiﬁcation algorithm.\nIn Logistic Regression, we define a set weights \\(\\mathbf{w}\\) that should be combined\n(through a trivial dot product) with some features \\(\\phi\\). Considering a problem of two-class\nclassiﬁcation, the posterior probability of class \\(C_1\\) can be written as a logistic sigmoid\nfunction:\n\\[\np(C_1\\vert\\phi) =\n\\frac{1}{1+e^{-\\mathbf{w}^T\\phi}}=\\sigma(\\mathbf{w}^T\\phi)\n\\]\n\nand \\(p(C_2\\vert\\phi) = 1 -\np(C_1\\vert\\phi)\\)\nApplying the Maximum Likelihood approach…\nGiven a dataset \\(\\mathcal{D} =\n\\{(\\mathbf{\\phi}_n,t_n)\\ \\forall n\\in[1,N]\\}\\), \\(t_n \\in \\{0,1\\}\\), we have to maximize the\nprobability of getting the right label:\n\\[\nP(\\mathbf{t}\\vert\\mathbf{\\Phi},\\mathbf{w}) =\n\\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n},\\ \\ y_n =\n\\sigma(\\mathbf{w}^T\\phi_n)\n\\]\nTaking the negative log of the likelihood, the cross-entropy\nerror function can be deﬁned and it has to be minimized:\n\\[\nL(\\mathbf{w}) = -\\ln P(\\mathbf{t}\\vert\\mathbf{\\Phi},\\mathbf{w}) =\n-\\sum_{n=1}^{N}(t_n\\ln y_n+(1-t_n)\\ln(1-y_n))=\\sum_{n}^NL_n\n\\]\nDifferentiating and using the chain rule:\n\\[\n\\frac{\\partial L_n}{\\partial y_n}= \\frac{y_n-t_n}{y_n(1-y_n)},\\ \\ \\ \\\n\\frac{\\partial y_n}{\\partial\\mathbf{w}}=y_n(1-y_n)\\phi_n\\\\\n\\frac{\\partial L_n}{\\partial \\mathbf{w}}= \\frac{\\partial L_n}{\\partial\ny_n}\\frac{\\partial y_n}{\\partial\\mathbf{w}}=(y_n-t_n)\\phi\n\\]\nThe gradient of the loss function is\n\\[\n\\nabla L(\\mathbf{w}) = \\sum_{n=1}^{N}(y_n-t_n)\\phi_n\n\\]\nIt has the same form as the gradient of the sum-of-squares error\nfunction for linear regression. But in this case \\(y\\) is not a linear function of \\(\\mathbf{w}\\) and so, there is no closed\nform solution. The error function is convex (only one optimum)\nand can be optimized by standard gradient-based optimization\ntechniques. It is, hence, easy to adapt to the online learning\nsetting.\nTalking about Multiclass Logistic Regression…\nFor the multiclass case, the posterior probabilities can be\nrepresented by a softmax transformation of linear functions of\nfeature variables:\n\\[\np(C_k\\vert\\phi)=y_k(\\phi)=\\frac{e^{\\mathbf{w}_k^T\\phi}}{\\sum_j\ne^{\\mathbf{w}_j^T\\phi}}\n\\]\n\\(\\phi(\\mathbf{x})\\) has been\nabbreviated with \\(\\phi\\) for\nsimplicity.\nMaximum Likelihood is used to directly determine the\nparameters\n\\[\np(\\mathbf{T}\\vert\\Phi,\\mathbf{w}_1,\\dots,\\mathbf{w}_K)=\\prod_{n=1}^{N}{\\underset{\\text{Term\nfor correct\nclass$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\,\\,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$}}{\\underbrace{\\left(\\prod_{k=1}^{K}p(C_k\\vert\\phi_n)^{t_{nk}}\\right)}=\\prod_{n=1}^{N}\\left(\\prod_{k=1}^{K}y_{nk}^{t_{nk}}\\right)}}\\\\\n\\]\nwhere \\(y_{nk}=p(C_k\\vert\\phi_n)=\\frac{e^{\\mathbf{w}_k^T\\phi_n}}{\\sum_j\ne^{\\mathbf{w}_j^T\\phi_n}}\\)\nThe cross-entropy function is:\n\\[\nL(\\mathbf{w}_1,\\dots,\\mathbf{w}_K)=-\\ln\np(\\mathbf{T}\\vert\\Phi,\\mathbf{w}_1,\\dots,\\mathbf{w}_K)=-\\sum_{n=1}^{N}\\left(\\sum_{k=1}^{K}t_{nk}\\ln\ny_{nk}\\right)\n\\]\nTaking the gradient\n\\[\n\\nabla L_{\\mathbf{w}_j}(\\mathbf{w}_1,\\dots,\\mathbf{w}_K)\n=\\sum_{n=1}^{N}(y_{nj}-t_{nj})\\phi_n\n\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2025-02-25T22:52:32+01:00",
    "input_file": "a-quick-overview-of-logistic-regression.knit.md"
  },
  {
    "path": "posts/2021-07-22-the-kernel-trick/",
    "title": "The Kernel Trick.",
    "description": "What is the kernel trick? What's the main advantage of this technique?",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-05-18",
    "categories": [
      "Machine Learning"
    ],
    "contents": "\nTraditionally, theory and algorithms of machine learning and statistics have been very well developed for the linear case. Real world data analysis problems, on the other hand, often require nonlinear methods to detect the kind of dependencies that allow successful prediction of properties of interest. By using a positive definite kernel, one can sometimes have the best of both worlds. The kernel corresponds to a dot product in a usually high-dimensional (possibly infinite) feature space. In this space, our estimation methods are linear, but as long as we can formulate everything in terms of kernel evaluations, we never explicitly have to compute in the high dimensional feature space! (this is called the Kernel Trick)\nSuppose we have a mapping \\(\\varphi : \\mathbb{R}^d \\to \\mathbb{R}^m\\) that brings our vectors in to some feature space \\(\\mathbb{R}^m\\). Then the dot product of \\(\\textbf{x}\\) and \\(\\textbf{y}\\) in this space is \\(\\varphi (\\textbf{x})^T\\varphi (\\textbf{y})\\).\nA kernel is a function \\(k\\) that corresponds to this dot product, i.e. $k(,)=()^T() $ .\nWhy is this useful? Kernels give a way to compute dot products in some feature space without even knowing what this space is and what is \\(\\varphi\\) .\nFor example, consider a simple polynomial kernel \\(k(\\textbf{x},\\textbf{y})=(1+\\textbf{x}^T\\textbf{y})^2\\) with \\(\\textbf{x},\\textbf{y} \\in \\mathbb{R}^2\\).\nThis doesn’t seem to correspond to any mapping function \\(\\varphi\\) , it’s just a function that returns a real number. Assuming that \\(\\textbf{x} = (x_1,x_2)\\) and \\(\\textbf{y} = (y_1,y_2)\\), let’s expand this expression:\n\\[\nk(\\textbf{x},\\textbf{y})=(1+\\textbf{x}^T\\textbf{y})^2 = (1+x_1y_1 + x_2y_2)^2=\\\\1+x_1^2y_1^2+x_2^2y_2^2+2x_1y_1+2x_2y_2+2x_1x_2y_1y_2\n\\]\nNote that this is nothing else but a dot product between two vectors:\n\\[\\varphi(\\mathbf x) = \\varphi(x_1, x_2) = (1, x_1^2, x_2^2, \\sqrt{2} x_1, \\sqrt{2} x_2, \\sqrt{2} x_1 x_2)\\]\nand\n\\[\\varphi(\\mathbf y) = \\varphi(y_1, y_2) = (1, y_1^2, y_2^2, \\sqrt{2} y_1, \\sqrt{2} y_2, \\sqrt{2} y_1 y_2)\\]\nSo the kernel \\(k(\\mathbf x, \\mathbf y) = (1 + \\mathbf x^T \\mathbf y)^2 = \\varphi(\\mathbf x)^T \\varphi(\\mathbf y)\\) computes a dot product in a 6-dimensional space without explicitly visiting this space.\nAnother example is the Gaussian kernel \\(k(\\mathbf x, \\mathbf y) = \\exp\\big(- \\gamma \\, \\|\\mathbf x - \\mathbf y\\|^2 \\big)\\). If we Taylor-expand this function, we’ll see that it corresponds to an infinite-dimensional codomain of \\(\\varphi\\).\nInstead, the simplest kernel is the linear kernel which corresponds to an identity mapping in the feature space: \\(k(\\mathbf{x},\\mathbf{x'}) = \\varphi(\\mathbf{x})^T\\varphi(\\mathbf{x'}) = \\mathbf{x}^T\\mathbf{x}\\)\nMoreover, the kernel is a symmetric function of its arguments: \\(k(\\mathbf{x},\\mathbf{x'}) = k(\\mathbf{x'},\\mathbf{x})\\)\nMany linear models for regression and classiﬁcation can be reformulated in terms of dual representation in which the kernel function arises naturally ! For example if we consider a linear ridge regression model we know that we obtain the best parameters by minimizing the regularized sum of squares error function (ridge):\n\\[\nL_{\\mathbf{w}} = \\frac{1}{2}\\sum_{n=1}^{N}(\\mathbf{w}^T\\varphi(\\mathbf{x_n})-t_n)^2+\\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}=\\\\\n\\frac{1}{2}\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)^T\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)+\\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}\n\\]\nWhere \\(\\Phi\\) is the design matrix whose \\(n^{th}\\) row is \\(\\varphi(\\mathbf{x}_n)^T\\) (remember that in \\(L_{\\mathbf{w}}\\) all the vectors are column vectors) and \\(\\mathbf{t} = (t_1,...,t_N)^T\\) is the target vector.\nSetting the gradient of \\(L_{\\mathbf{w}}\\) w.r.t. \\(\\mathbf{w}\\) equal to \\(0\\) we obtain the following:\n\\[\n\\frac{\\partial L_\\mathbf{w}}{\\partial \\mathbf{w}}=0\\\\\n\\frac{\\partial \\left(\\frac{1}{2}\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)^T\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)+\\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}\\right)}{\\partial \\mathbf{w}}=0\\\\\n\\mathbf{\\Phi}^T\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)+\\lambda\\mathbf{w} = 0\\\\\n\\mathbf{w} = -\\frac{1}{\\lambda}\\Phi^T\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)\\\\\n\\mathbf{w} = \\Phi^T\\mathbf{a}\n\\]\nWhere \\(\\mathbf{a}=-\\frac{1}{\\lambda}\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)\\) is a \\(N\\times 1\\) vector.\nWe observe that the coefficients \\(a_n\\) are functions of \\(\\mathbf{w}\\). So our definition of \\(\\mathbf{w}\\) is function of \\(\\mathbf{w}\\) itself…which is surely weird, just wait for it…\nWe now define the Gram Matrix \\(\\mathbf{K} = \\Phi \\times \\Phi^T\\), an \\(N \\times N\\) matrix, with elements:\n\\[\nK_{nm} = \\varphi(\\mathbf{x_n})^T\\varphi(\\mathbf{x_m})=k(\\mathbf{x}_n,\\mathbf{x}_m)\n\\]\nSo, given \\(N\\) samples, the Gram Matrix is the matrix of all inner products\n\\[\nK =\n\\begin{bmatrix}\nk(\\mathbf{x}_1,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_1,\\mathbf{x}_N) \\\\\n\\vdots &\\ddots & \\vdots\\\\\nk(\\mathbf{x}_N,\\mathbf{x}_1)  & \\dots&  k(\\mathbf{x}_N,\\mathbf{x}_N) \n\\end{bmatrix}\n\\]\nThis will come in handy in a few seconds…\nIf we substitute \\(\\mathbf{w} = \\Phi^T\\mathbf{a}\\) into \\(L_{\\mathbf{w}}\\) we get\n\\[\nL_{\\mathbf{w}} =\n\\frac{1}{2}\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)^T\\left(\\mathbf{\\Phi\\mathbf{w}}-\\mathbf{t}\\right)+\\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}\n\\]\n\\[\nL_{\\mathbf{w}} =\n\\frac{1}{2}\\left(\\mathbf{\\Phi\\Phi^T\\mathbf{a}}-\\mathbf{t}\\right)^T\\left(\\mathbf{\\Phi\\Phi^T\\mathbf{a}}-\\mathbf{t}\\right)+\\frac{\\lambda}{2}\\left(\\Phi^T\\mathbf{a}\\right)^T\\left(\\Phi^T\\mathbf{a}\\right)\n\\]\n\\[\nL_{\\mathbf{a}} = \\frac{1}{2}\\mathbf{a}^T\\Phi\\Phi^T\\Phi\\Phi^T\\mathbf{a}-\\mathbf{a}^T\\Phi\\Phi^T\\mathbf{t}+\\frac{1}{2}\\mathbf{t}^T\\mathbf{t}+\\frac{\\lambda}{2}\\mathbf{a}^T\\Phi\\Phi^T\\mathbf{a}\n\\]\nGuess what? we can rewrite the Loss function in terms of the Gram Matrix !\n\\[\nL_{\\mathbf{a}} = \\frac{1}{2}\\mathbf{a}^TKK\\mathbf{a}-\\mathbf{a}^TK\\mathbf{t}+\\frac{1}{2}\\mathbf{t}^T\\mathbf{t}+\\frac{\\lambda}{2}\\mathbf{a}^TK\\mathbf{a}\n\\]\nBy combining \\(\\mathbf{w} = \\Phi^T\\mathbf{a}\\) and \\(a_n = -\\frac{1}{\\lambda}(\\mathbf{w}^T\\varphi(\\mathbf{x}_n)-t_n)\\), setting the gradient w.r.t \\(\\mathbf{a}\\) equal to \\(0\\) and isolating \\(\\mathbf{a}\\) we obtain:\n\\[\n\\mathbf{a}=(K+\\lambda\\mathbf{I}_N)^{-1}\\mathbf{t}\n\\]\nWhere \\(I_N\\) is the identity matrix of dimension \\(N\\). Consider that \\(K = N\\times N\\) and \\(\\mathbf{t} = N\\times 1\\), so \\(\\mathbf{a} = N \\times 1\\).\nSo we can make our prediction for a new input \\(\\mathbf{x}\\) by substituting back into our linear regression model:\n\\[\ny(\\mathbf{x}) = \\mathbf{w}^T\\varphi(\\mathbf{x}) = (\\Phi^T\\mathbf{a})^T\\varphi(\\mathbf{x}) = \\mathbf{a}^T\\Phi\\varphi(\\mathbf{x})= \\mathbf{k}(\\mathbf{x})^T(K+\\lambda\\mathbf{I}_N)^{-1}\\mathbf{t}\n\\]\nwhere \\(\\mathbf{k}(\\mathbf{x})\\) is an \\(N\\)-dimensional column vector with elements \\(k_n(\\mathbf{x}) = k(\\mathbf{x}_n,\\mathbf{x})\\).\nThe good thing is that instead of inverting an \\(M\\times M\\) matrix, we are inverting an \\(N\\times N\\) matrix! This allows us to work with very high or infinite dimensionality of \\(\\mathbf{x}\\).\nBut how can we build a valid kernel?\nWe have mainly two ways to do it:\nBy construction: we choose a feature space mapping \\(\\varphi(\\mathbf{x})\\) and use it to ﬁnd the corresponding kernel.\nIt is possible to test whether a function is a valid kernel without having to construct the basis function explicitly. The necessary and suﬃcient condition for a function \\(k(\\mathbf{x},\\mathbf{x}')\\) to be a kernel is that the Gram matrix \\(K\\) is positive semi-deﬁnite for all possible choices of the set \\(\\{x_n\\}\\). It means that \\(\\mathbf{x}^TK\\mathbf{x}\\ge 0\\) for non-zero vectors \\(\\mathbf{x}\\) with real entries, i.e.\\(\\sum_n\\sum_m K_{n,m}x_nx_m \\ge 0\\) for any real number \\(x_n,x_m\\).\n\\(\\implies\\)Mercer’s Theorem : Any continuous, symmetric, positive semi-deﬁnite kernel function \\(k(\\mathbf{x},\\mathbf{y})\\) can be expressed as a dot product in a high-dimensional space.\nNew kernels can be constructed from simpler kernels as building blocks; given valid kernels \\(k_1(\\mathbf{x},\\mathbf{x})\\) and \\(k_2(\\mathbf{x},\\mathbf{x})\\) the following new kernels will be valid:\n\\(k(\\mathbf{x},\\mathbf{x}^{'})=c \\cdot k_1(\\mathbf{x},\\mathbf{x}^{'})\\)\n\\(k(\\mathbf{x},\\mathbf{x}^{'})=f(x)\\cdot k_1(\\mathbf{x},\\mathbf{x}^{'})\\cdot f(x)\\)\n\\(k(\\mathbf{x},\\mathbf{x}^{'})=q\\left(k_1(\\mathbf{x},\\mathbf{x}^{'})\\right)\\) where \\(q()\\) is a polynomial with non-negative coefficients.\n\\(k(\\mathbf{x},\\mathbf{x}^{'})=e^{k_1(\\mathbf{x},\\mathbf{x}^{'})}\\)\n\\(k(\\mathbf{x},\\mathbf{x}^{'})=k_1(\\mathbf{x},\\mathbf{x}^{'})+k_2(\\mathbf{x},\\mathbf{x}^{'})\\)\n\\(k(\\mathbf{x},\\mathbf{x}^{'})=k_1(\\mathbf{x},\\mathbf{x}^{'})k_2(\\mathbf{x},\\mathbf{x}^{'})\\)\n\n\n\n",
    "preview": "posts/2021-07-22-the-kernel-trick/images/preview.png",
    "last_modified": "2025-02-25T22:36:19+01:00",
    "input_file": {},
    "preview_width": 700,
    "preview_height": 614
  },
  {
    "path": "posts/2021-07-21-how-to-call-a-c-function-from-python/",
    "title": "How to call a C function from Python.",
    "description": "Need to speed up things by calling a C function from your Python script? Check this out.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-03-03",
    "categories": [
      "Software Engineering",
      "c",
      "python"
    ],
    "contents": "\nThis will be a really short blogpost which mainly serves as a\npersonal reminder on how to call a C function from Python… the title is\npretty self-explanatory.\nI will show how to call some really simple c functions\nwich return a double or an array (which will be a np.array\nin Python).\nHere we have our simple c file\ncfunctions.c:\n\n#include<stdlib.h>\n\nlong factorial(int n){\n    // \n    long res = 1;\n    if (n <= 0) {\n        return -1;\n    }\n    else {\n        for (long i = 1; i <= n; i++) {\n           res *= i;\n        }\n    }\n    return res;\n}\n\ndouble dotproduct(int dim, double a[dim], double b[dim]){\n    // Compute the dot product between two vectors...\n    // e.g. a = [1,2,3,4] , b = [4,3,2,1]\n    // res = 1*4 + 2*3 + 3*2 + 4*1 = 4 + 6 + 6 + 4 = 20\n    double res = 0;   \n    for(int i = 0; i < dim; i++){\n        res = res + a[i]*b[i];\n    }\n    return res;\n}\n\ndouble * elementwiseproduct(int dim, double a[dim], double b[dim]){\n    // Compute the elementiwise product between two vectors...\n    // e.g. a = [1,2,3,4] , b = [4,3,2,1]\n    // res will point to an array such as [1*4, 2*3, 3*2, 4*1] = [4,6,6,4]\n    double * res = (double *) malloc(sizeof(double) * dim);\n    for(int i = 0; i < dim; i++){\n        res[i] = a[i]*b[i];\n    }    \n    return res;\n}\n\nWe then create a shared object witht the following command:\n\ncc -fPIC -shared -o cfunctions.so cfunctions.c\n\nThen we can write a Python script py_cfunctions.py such\nas:\n\nimport ctypes\nimport numpy as np\n\ndef factorial(num: int):\n    py_cfunctions.factorial.restype = ctypes.c_long\n    return py_cfunctions.factorial(num)\n\ndef dotproduct(dim: int, a: np.array, b: np.array):\n    # Convert np.array to ctype doubles\n    a_data = a.astype(np.double)\n    a_data_p = a_data.ctypes.data_as(c_double_p)\n    b_data = b.astype(np.double)\n    b_data_p = b_data.ctypes.data_as(c_double_p)\n    py_cfunctions.dotproduct.restype = ctypes.c_double\n    # Compute result...\n    return py_cfunctions.dotproduct(dim,a_data_p,b_data_p)\n    \ndef elementwiseproduct(dim: int, a: np.array, b: np.array):\n    # Convert np.array to ctype doubles\n    a_data = a.astype(np.double)\n    a_data_p = a_data.ctypes.data_as(c_double_p)\n    b_data = b.astype(np.double)\n    b_data_p = b_data.ctypes.data_as(c_double_p)\n\n    py_cfunctions.elementwiseproduct.restype = np.ctypeslib.ndpointer(dtype=ctypes.c_double,shape=(dim,))\n    # Compute result...\n    return py_cfunctions.elementwiseproduct(dim,a_data_p,b_data_p)\n\n# so_file genreated with:\n# cc -fPIC -shared -o cfunctions.so cfunctions.c\n\nso_file = 'MY_PATH/cfunctions.so'\npy_cfunctions = ctypes.CDLL(so_file)\nc_double_p = ctypes.POINTER(ctypes.c_double)\npy_cfunctions.factorial.argtypes = [ctypes.c_int] \npy_cfunctions.elementwiseproduct.argtypes = [ctypes.c_int, c_double_p, c_double_p]\npy_cfunctions.dotproduct.argtypes= [ctypes.c_int, c_double_p, c_double_p]\n\nAnd that’s it! You now can import py_cfunctions and call\nfactorial(), dotproduct() and\nelementwiseproduct().\n\n\n\n",
    "preview": {},
    "last_modified": "2025-02-25T22:52:24+01:00",
    "input_file": "how-to-call-a-c-function-from-python.knit.md"
  },
  {
    "path": "posts/2021-07-22-the-vc-dimension/",
    "title": "The VC dimension.",
    "description": "A quick explanation of the VC dimension.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2021-02-14",
    "categories": [
      "Machine Learning",
      "Classification"
    ],
    "contents": "\nWhen talking about binary classification, an\nhypothesis is a function that maps an input from the entire\ninput space to a result: \\[\nh:\\mathcal{X}\\to\\{-1,+1\\}\n\\] The number of hypotheses \\(\\vert\\mathcal{H}\\vert\\) can be\ninfinite.\nA dichotomy is a hypothesis that maps from an input from the\nsample size to a result:\n\\[\nh:\\{\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_N\\}\\to\\{-1,+1\\}\n\\]\nThe number of dichotomies \\(\\vert\\mathcal{H}(\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_N\n)\\vert\\) is at most \\(2^N\\),\nwhere \\(N\\) is the sample size.\ne.g. for a sample size \\(N = 3\\) we\nhave at most \\(8\\) possible\ndichotomies:\n        x1 x2 x3\n1       -1 -1 -1\n2       -1 -1 +1\n3       -1 +1 -1\n4       -1 +1 +1\n5       +1 -1 -1 \n6       +1 -1 +1\n7       +1 +1 -1\n8       +1 +1 +1\n\nThe growth function is a function that counts the\nmost dichotomies on any \\(N\\)\npoints. \\[\nm_{\\mathcal{H}}(N)=\\underset{\\mathbf{x}_1,\\dots,\\mathbf{x}_N\\in\\mathcal{X}}{max}\\vert\\mathcal{H}(\\mathbf{x}_1,\\dots,\\mathbf{x}_N)\\vert\n\\] This translates into choosing any \\(N\\) points and laying them out in\nany fashion in the input space. Determining \\(m\\) is equivalent to looking for such a\nlayout of the \\(N\\) points that yields\nthe most dichotomies.\nThe growth function satisfies: \\[\nm_{\\mathcal{H}}(N)\\le 2^N\n\\] This can be applied to the perceptron. For example, when \\(N=4\\), we can lay out the points so that\nthey are easily separated. However, given a layout, we must then\nconsider all possible configurations of labels on the points, one of\nwhich is the following:\n\n\nThis is where the perceptron breaks down because it cannot\nseparate that configuration, and so \\(m_{\\mathcal{H}}(4)=14\\) because two\nconfigurations—this one and the one in which the left/right points are\nblue and top/bottom are red—cannot be represented. For this reason, we\nhave to expect that for perceptrons, \\(m\\) can’t be \\(2^4\\).\nThe VC ( Vapnik-Chervonenkis ) dimension of a\nhypothesis set \\(\\mathcal{H}\\) ,\ndenoted by \\(d_{VC}(\\mathcal{H})\\) is\nthe largest value of \\(N\\) for which\n\\(m_{\\mathcal{H}}(N)=2^N\\) , in other\nwords is “the most points \\(\\mathcal{H}\\) can shatter”\nWe can say that the VC dimension is one of many measures\nthat characterize the expressive power, or capacity, of a hypothesis\nclass.\nYou can think of the VC dimension as “how many points can this model\nclass memorize/shatter?” (a ton? \\(\\to\\) BAD! not so many? \\(\\to\\) GOOD!).\nWith respect to learning, the effect of the VC dimension is that if\nthe VC dimension is finite, then the hypothesis will generalize:\n\\[\nd_{vc}(\\mathcal H)\\ \\Longrightarrow\\ g \\in \\mathcal H \\text { will\ngeneralize }\n\\]\nThe key observation here is that this statement is independent\nof:\nThe learning algorithm\nThe input distribution\nThe target function\nThe only things that factor into this are the training examples, the\nhypothesis set, and the final hypothesis.\nThe VC dimension for a linear classifier (i.e. a line in 2D,\na plane in 3D etc…) is \\(d+1\\)\n(a line can shatter at most \\(2+1=3\\)\npoints, a plane can shatter at most \\(3+1=4\\) points etc…)\nProof: here\nHow many randomly drawn examples sufﬁce to guarantee error of at most\n\\(\\epsilon\\) with probability at least\n(1−\\(\\delta\\))?\n\\[\nN\\ge\\frac{1}{\\epsilon}\\left(4\\log\\left(\\frac{2}{\\delta}\\right)+8VC(H)\\log_2\\left(\\frac{13}{\\epsilon}\\right)\\right)\n\\]\nPAC BOUND using VC dimension: \\[\nL_{true}(h)\\le\nL_{train}(h)+\\sqrt{\\frac{VC(H)\\left(\\ln\\frac{2N}{VC(H)}+1\\right)+\\ln\\frac{4}{\\delta}}{N}}\n\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2025-02-25T22:53:07+01:00",
    "input_file": "the-vc-dimension.knit.md"
  },
  {
    "path": "posts/2021-07-22-generative-adversarial-networks-original-formulation/",
    "title": "Generative Adversarial Networks (Original Formulation)",
    "description": "\"Generative Adversarial Networks\" (Goodfellow et al.) Paper overview.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2020-12-14",
    "categories": [
      "Computer Vision",
      "Machine Learning"
    ],
    "contents": "\nLet’s start with a question: what is Generative\nModeling?\nGenerative modeling is an unsupervised\nlearning task in machine learning that involves\nautomatically discovering and learning the regularities or patterns in\ninput data in such a way that the model can be used to generate or\noutput new examples that plausibly could have been drawn from the\noriginal dataset.\nGANs are a clever way of training a generative model by framing the\nproblem as a supervised learning problem with\ntwo sub-models: the generator model that we\ntrain to generate new examples, and the\ndiscriminator model that tries to classify\nexamples as either real (from the domain) or fake (generated).\nFormulation\nGenerator \\(\\mathcal{G}\\):\nproduces realistic samples e.g. taking as input some random noise. \\(\\mathcal{G}\\) tries to fool the\ndiscriminator\nDiscriminator \\(\\mathcal{D}\\)\nthat takes as input an image and assess whether it is real or generated\nby \\(\\mathcal{G}\\)\nBoth \\(\\mathcal{D}\\) and \\(\\mathcal{G}\\) are conveniently chosen as\nMLPs. The generative process depends on two networks:\n\\(\\mathcal{D}\n=\\mathcal{D}(\\mathbf{x},\\theta_d)\\)\n\\(\\mathcal{G}\n=\\mathcal{G}(\\mathbf{z},\\theta_g)\\)\n\\(\\theta_g\\) and \\(\\theta_d\\) are the network parameters,\n\\(\\mathbf{x}\\in\\mathbb{R}^n\\) is an\ninput image (either real or generated by \\(\\mathcal{G}\\)) and \\(\\mathbf{z}\\in\\mathbb{R}^d\\) is some random\nnoise to be fed to the generator. We suppose that \\(\\mathbf{x}\\) is sampled from a distribution\n\\(p_{data}\\) \\((\\)i.e. \\(\\mathbf{x}\\sim p_{data}(\\mathbf{x}))\\) and\n\\(\\mathbf{z}\\) is sampled from a\ndistribution \\(p_z\\) \\((\\)i.e. \\(\\mathbf{z}\\sim p_z(\\mathbf{z}))\\). Our\nDiscriminator’s output is to be seen as the probability that the input\nimage comes from the data and not from the generator: \\[\n\\mathcal{D}(\\cdot,\\theta_d):\\mathbb{R}^n \\to [0,1]\n\\]\nThe generator gives as output a generated image: \\[\n\\mathcal{G}(\\cdot,\\theta_d):\\mathbb{R}^d \\to \\mathbb{R}^n\n\\] A good discriminator is such that:\n\\(\\mathcal{D}(\\mathbf{x},\\theta_d)\\) is\nmaximum when \\(\\mathbf{x} \\in X\\)\n(i.e. \\(\\mathbf{x}\\) is sampled for the\nreal images dataset \\(X\\))\n\\(1 -\n\\mathcal{D}(\\mathbf{x},\\theta_d)\\) is maximum when \\(\\mathbf{x}\\) was generated by \\(\\mathcal{G}\\)\n\\(1 -\n\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)\\) is\nmaximum when \\(\\mathbf{z} \\sim\np_z(\\mathbb{z})\\)\nTraining \\(\\mathcal{D}\\) consists in\nmaximizing the binary cross-entropy:\n\\[\n\\underset{\\theta_d}{\\text{max}}\n\\left(\\\n\\mathbb{E}_{\\mathbf{x}\\sim p_{data}(\\mathbf{x})}\n[\\log\\mathcal{D}(\\mathbf{x},\\theta_d)]\n+\n\\mathbb{E}_{\\mathbf{z}\\sim p_z(\\mathbf{z})}\n\\log[1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)]\n\\ \\right)\n\\]\nWhere\n\\(\\mathcal{D}(\\mathbf{x},\\theta_d)\\) has to\nbe \\(1\\) since \\(\\mathbf{x} \\sim p_{data}(\\mathbf{x})\\),\nnamely images are real.\n\\(\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)\\)\nhas to be \\(0\\) since \\(\\mathcal{G}(\\mathbf{z},\\theta_g)\\) is a\ngenerated (fake) image.\nA good generator \\(\\mathcal{G}\\) is one that makes \\(\\mathcal{D}\\) fail:\n\\[\n\\underset{\\theta_g}{\\text{min}}\n\\left(\n\\underset{\\theta_d}{\\text{max}}\n\\left(\\\n\\mathbb{E}_{\\mathbf{x}\\sim p_{data}(\\mathbf{x})}\n[\\log\\mathcal{D}(\\mathbf{x},\\theta_d)]\n+\n\\mathbb{E}_{\\mathbf{z}\\sim p_z(\\mathbf{z})}\n\\log[1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)]\n\\\n\\right)\n\\right)\n\\]\nOptimizing \\(\\mathcal{D}\\) to\ncompletion in the inner loop of training is computationally prohibitive,\nand on finite datasets would result in overfitting. Instead, we\nalternate between \\(k\\) steps of\noptimizing \\(\\mathcal{D}\\) and one step\nof optimizing \\(\\mathcal{G}\\). This\nresults in \\(\\mathcal{D}\\) being\nmaintained near its optimal solution, as long as \\(\\mathcal{G}\\) changes slowly enough.\nLet’s schematize it:\nWe need to solve by an iterative numerical approach the min max game\nshown at \\((4)\\). In order to do so we\nalternate:\n\\(k\\)-steps of Stochastic\nGradient Ascent w.r.t. \\(\\theta_d\\) to\nsolve\n\\[\n  \\underset{\\theta_d}{\\text{max}}\n  \\left(\\\n  \\mathbb{E}_{\\mathbf{x}\\sim p_{data}(\\mathbf{x})}\n  [\\log\\mathcal{D}(\\mathbf{x},\\theta_d)]\n  +\n  \\mathbb{E}_{\\mathbf{z}\\sim p_z(\\mathbf{z})}\n  \\log[1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)]\n  \\\n  \\right)\n  \\]\n\\(1\\)-step of Stochastic\nGradient Descent w.r.t. \\(\\theta_g\\)\nbeing \\(\\theta_d\\) fixed:\n\\[\n  \\underset{\\theta_g}{\\text{min}}\n  \\left(\\\n  \\mathbb{E}_{\\mathbf{x}\\sim p_{data}(\\mathbf{x})}\n  [\\log\\mathcal{D}(\\mathbf{x},\\theta_d)]\n  +\n  \\mathbb{E}_{\\mathbf{z}\\sim p_z(\\mathbf{z})}\n  \\log[1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)]\n  \\\n  \\right)\n  \\]\ni.e. (the removed term does not depend on \\(\\theta_g\\))\n\\[\n  \\underset{\\theta_g}{\\text{min}}\n  \\left(\\\n  \\mathbb{E}_{\\mathbf{z}\\sim p_z(\\mathbf{z})}\n  [\\log(1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d))]\n  \\\n  \\right)\n  \\]\ni.e.\n\\[\n  \\underset{\\theta_g}{\\text{max}}\n  \\left(\\\n  \\mathbb{E}_{\\mathbf{z}\\sim\\phi_z}\n  [\\log\\mathcal{D}(\\mathcal{G}(\\mathbf{z},\\theta_g),\\theta_d)]\n  \\\n  \\right)\n  \\]\nThere is a reason why Goodfellow proposed to optimize \\(\\log(\\mathcal{D}(\\cdot))\\) instead of \\(\\log(1-\\mathcal{D}(\\cdot))\\). If we try to\ndescend the gradient of \\(\\log(1-\\mathcal{D}(x))\\), we notice that at\nthe beginning of the training process, when the generated samples would\nbe easily classified as “fake” (i.e. \\(\\mathcal{D}(x) \\sim 0\\)), there would be\ntoo few gradient in order to learn properly!\n\n\n\nWe have the following value function for our min-max problem:\n\\[\nV(\\mathcal{G},\\mathcal{D})\n=\n\\mathbb{E}_{\\mathbf{x}\\sim\np_{\\text{data}}(\\mathbf{x})}\\log(\\mathcal{D}(\\mathbf{x}))\\mathbf{x}+\\mathbb{E}_{\\mathbf{z}\\sim\np_{z}(\\mathbf{z})}\\log(1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z}))d\\mathbf{z}\n\\]\n\\[\n=\\int_{\\mathbf{x}}p_{\\text{data}}(\\mathbf{x})\\log(\\mathcal{D}(\\mathbf{x}))d\\mathbf{x}\n+\n\\int_{\\mathbf{z}}p_{\\mathbf{z}}(\\mathbf{z})\\log(1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z}))d\\mathbf{z}\n\\]\n\\[\n=\\int_{\\mathbf{x}}\n\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\n\\right)\nd\\mathbf{x}\n\\]\nThis last equality comes from the Radon-Nikodym Theorem of measure\ntheory and it’s sometimes referred as the Law Of The Unconscious\nStatistician (or LOTUS Theorem) since students have been accused of\nusing the identity without realizing that it must be treated as the\nresult of a rigorously proved theorem, not merely a definition (if you\nwant the full proof check this\nout! )\n\\[\nV(\\mathcal{G},\\mathcal{D})\n=\\int_{\\mathbf{x}}\n\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\n\\right)\nd\\mathbf{x}\n\\]\nLet’s first consider the optimal discriminator \\(\\mathcal{D}\\) for any given generator \\(\\mathcal{G}\\). The training criterion for\nthe discriminator \\(\\mathcal{D}\\),\ngiven any generator \\(\\mathcal{G}\\), is\nto maximize the quantity defined below:\n\\[\n\\underset{\\mathcal{D}}{\\text{argmax}}\\left(V(\\mathcal{G},\\mathcal{D})\\right)\n=\\underset{\\mathcal{D}}{\\text{argmax}}\\left(\\int_{\\mathbf{x}}\n\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\n\\right)\nd\\mathbf{x}\n\\right)\n\\]\nFor the individual sample \\(\\mathbf{x}\\) we derive \\(V(\\mathcal{G},\\mathcal{D})\\) w.r.t. \\(\\mathcal{D}(\\mathbf{x})\\) and we equal this\nquantity to \\(0\\) in order to find the\noptimal discriminator \\(\\mathcal{D}(\\mathbf{x})^{*}\\)\n\\[\n\\frac{d}{d\\mathcal{D}(\\mathbf{x})}\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\n\\right)\n=0\n\\]\n\\[\n\\frac{p_{data}(\\mathbf{x})}{\\mathcal{D}^{*}(\\mathbf{x})}\n-\n\\frac{p_{g}(\\mathbf{x})}{1-\\mathcal{D}^{*}(\\mathbf{x})}=0\n\\]\n\\[\n\\frac{p_{data}(\\mathbf{x})(1-\\mathcal{D}^{*}(\\mathbf{x}))-p_{g}(\\mathbf{x})\\mathcal{D}^{*}(\\mathbf{x})}{\\mathcal{D}^{*}(\\mathbf{x})(1-\\mathcal{D}^{*}(\\mathbf{x}))}\n=0\n\\]\n\\[\n\\frac{p_{data}(\\mathbf{x})-p_{data}(\\mathbf{x})\\mathcal{D}^{*}(\\mathbf{x})-p_{g}(\\mathbf{x})\\mathcal{D}^{*}(\\mathbf{x})}{\\mathcal{D}^{*}(\\mathbf{x})(1-\\mathcal{D}^{*}(\\mathbf{x}))}\n= 0\n\\]\n\\[\np_{data}(\\mathbf{x})-p_{data}(\\mathbf{x})\\mathcal{D}^{*}(\\mathbf{x})-p_{g}(\\mathbf{x})\\mathcal{D}^{*}(\\mathbf{x})\n= 0\n\\]\n\\[\np_{data}(\\mathbf{x})-\\mathcal{D}^{*}(\\mathbf{x})\\left(p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})\\right)\n= 0\n\\]\n\\[\n\\color{blue}{\\mathcal{D}^{*}(\\mathbf{x})\n=\n\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\n}\n\\]\nDoes this point represent a maximum? we have to check if the second\nderivative calculated in \\(\\mathcal{D}^{\\star}\\) is negative.\n\\[\n\\frac{d}{d\\mathcal{D}(\\mathbf{x})}\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\n\\right)\n=\n\\frac{p_{data}(\\mathbf{x})}{\\mathcal{D}(\\mathbf{x})}\n-\n\\frac{p_{g}(\\mathbf{x})}{1-\\mathcal{D}(\\mathbf{x})}\n\\]\n\\[\n\\frac{d^2}{d^2\\mathcal{D}(\\mathbf{x})}\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\n\\right)\n=\n\\frac{d}{d\\mathcal{D}(\\mathbf{x})}\\left(\\frac{p_{data}(\\mathbf{x})}{\\mathcal{D}(\\mathbf{x})}\n-\n\\frac{p_{g}(\\mathbf{x})}{1-\\mathcal{D}(\\mathbf{x})}\\right)\n\\]\n\\[\n\\frac{d}{d\\mathcal{D}(\\mathbf{x})}\\left(\\frac{p_{data}(\\mathbf{x})}{\\mathcal{D}(\\mathbf{x})}\n-\n\\frac{p_{g}(\\mathbf{x})}{1-\\mathcal{D}(\\mathbf{x})}\\right)\n=\n-\\frac{p_{data}(\\mathbf{x})}{\\mathcal{D}^2(\\mathbf{x})}\n-\n\\frac{p_{g}(\\mathbf{x})}{\\left(1-\\mathcal{D}(\\mathbf{x})\\right)^2} <\n0\n\\]\nThe quantity above is negative for every \\(\\mathcal{D}\\), \\(\\mathcal{D}^*\\) included, since \\(p_{data}(\\mathbf{x})\\) and \\(p_g(\\mathbf{x})\\) are between \\(0\\) and \\(1\\).\nWe then can plug \\(\\mathcal{D^{\\star}}\\) into \\(\\mathcal{V(G,D)}\\) and find the optimal\ngenerator \\(\\mathcal{G^{\\star}}\\)\nas:\n\\[\n\\int_{\\mathbf{x}}\\left(p_{\\text{data}}(\\mathbf{x})\n\\log(\\mathcal{D}^{\\star}(\\mathbf{x}))\n+\np_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}^{\\star}(\\mathbf{x}))\\right)d\\mathbf{x}\n\\]\n\\[\n\\mathcal{G}^{\\star}\n=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\\int_{\\mathbf{x}}\n\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log\\left(\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\n+\np_{g}(\\mathbf{x})\n\\log\\left(1-\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\n\\right)\nd\\mathbf{x}\n\\right)\n\\]\n\\[\n\\mathcal{G}^{\\star}\n=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\\int_{\\mathbf{x}}\n\\left(\np_{\\text{data}}(\\mathbf{x})\n\\log\\left(\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\n+\np_{g}(\\mathbf{x})\n\\log\\left(\\frac{p_{g}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\n\\right)\nd\\mathbf{x}\n\\right)\n\\]\n\\[\n\\mathcal{G}^{\\star}=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\n\\int_{\\mathbf{x}}\n\\left(\np_{\\text{data}}(\\mathbf{x})\n\\left(\\log 2 - \\log 2\\right) +\np_{\\text{data}}(\\mathbf{x})\n\\log\\left(\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\n\\\\\n+\np_{\\text{g}}(\\mathbf{x})\n\\left(\\log 2 - \\log 2\\right) +\np_{g}(\\mathbf{x})\n\\log\\left(\\frac{p_{g}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\n\\right)\nd\\mathbf{x}\n\\right)\n\\]\n\\[\n\\mathcal{G}^{\\star}=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\n-\\log\n2\\int_{\\mathbf{x}}\\left(p_g(\\mathbf{x})+p_{data}(\\mathbf{x})\\right)d\\mathbf{x}\n+\n\\int_{\\mathbf{x}}\np_{\\text{data}}(\\mathbf{x})\n\\left(\\log 2  +\n\\log\\left(\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\n\\right)\\right)d\\mathbf{x}\\\\\n+\n\\int_{\\mathbf{x}}\np_{\\text{g}}(\\mathbf{x})\n\\left(\\log 2  +\n\\log\\left(\\frac{p_{g}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)\\right)d\\mathbf{x}\n\\right)\n\\]\n\\[\n\\mathcal{G}^{\\star}=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\n-\\log 2 \\cdot (2)\n+\n\\int_{\\mathbf{x}}\np_{\\text{data}}(\\mathbf{x})\n\\log\\left(2\\cdot\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\n\\right)d\\mathbf{x}\\\\\n+\n\\int_{\\mathbf{x}}\np_{\\text{g}}(\\mathbf{x})\n\\log\\left(2\\cdot\\frac{p_{g}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\\right)d\\mathbf{x}\n\\right)\n\\]\n\\[\n\\mathcal{G}^{\\star}=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\n-\\log 2^2\n+\n\\int_{\\mathbf{x}}\np_{\\text{data}}(\\mathbf{x})\n\\log\\left(\\frac{p_{data}(\\mathbf{x})}{\\frac{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}{2}}\n\\right)d\\mathbf{x}\\\\\n+\n\\int_{\\mathbf{x}}\np_{\\text{g}}(\\mathbf{x})\n\\log\\left(\\frac{p_{g}(\\mathbf{x})}{\\frac{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}{2}}\\right)d\\mathbf{x}\n\\right)\n\\]\n\\[\n\\mathcal{G}^{\\star}=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\n-\\log 4\n+\n\\mathit{KL}\\left(p_{data}||\\frac{p_g+p_{data}}{2}\\right)\n+\n\\mathit{KL}\\left(p_{g}||\\frac{p_g+p_{data}}{2}\\right)\n\\right)\n\\]\n\\[\n\\color{blue}{\\mathcal{G}^{\\star}=\\underset{\\mathcal{G}}{\\text{argmin}}\\left(\n-\\log 4\n+\n2\\cdot\\mathit{JSD}(p_{data}||p_g)\\right)}\n\\]\nWhere the Kullback-Leibler divergence (KL) and the\nJenson-Shannon divergence (JSD) are quantities that\nmeasure the difference between two distributions and we know that \\(\\mathit{JSD}(p_{data}\\vert\\vert p_g)=0\\)\nonly when \\(p_{data} = p_g\\) !\n\\[\n\\implies V(\\mathcal{D}^{\\star}_{\\mathcal{G}},\\mathcal{G}) = -\\log 4\n\\]\nTheorem \\(1\\):\nThe global minimum of the virtual training criterion \\[V(\\mathcal{D}^{\\star}_{\\mathcal{G}},\\mathcal{G})\\]\nis achieved if and only if \\(p_{g}=p_{data}\\). At that point, \\(V(\\mathcal{D}^{\\star}_{\\mathcal{G}},\\mathcal{G})\\)\nachieves the value \\(−\\log 4\\).\nBesides, that was what we expected! We wanted our generator to learn\nthe same distribution which generated the data. If we know that\n\\(p_{data} = p_g\\) then it’s trivial to\nobserve that at the end of the training process the optimal\ndiscriminator will be forced to output \\(0.5\\) since it won’t be able to distinguish\nbetween real and fake samples anymore.\n\\[\n\\color{blue}{\\mathcal{D}^{\\star}(\\mathbf{x})\n=\n\\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x})+p_{g}(\\mathbf{x})}\n= \\frac{1}{2}}\n\\]\nBut does this converge?\nWell, as stated in the original Paper:\nIf \\(\\mathcal{G}\\) and \\(\\mathcal{D}\\) have enough capacity, and at\neach step of our algorithm, the discriminator is allowed to reach its\noptimum given \\(\\mathcal{G}\\) and \\(p_g\\) is updated so as to improve the\ncriterion \\[\n\\mathbb{E}_{\\mathbf{x}\\sim\np_{data}}\\left[\\log\\mathcal{D}^{\\star}_{G}(\\mathbf{x})\\right] +\n\\mathbb{E}_{\\mathbf{x}\\sim\np_{g}}\\left[\\log(1-\\mathcal{D}^{\\star}_{G}(\\mathbf{x}))\\right]\n\\]\nthen \\(p_g\\) converges to \\(p_{data}\\).\nProof:\nConsider \\(V(\\mathcal{G,D})=\nU(p_g,\\mathcal{D})\\) as a function of \\(p_g\\) as done in the above criterion.\nNote that \\(U(p_g,\\mathcal{D})\\)\nis convex in \\(p_g\\).\nThe subderivatives of a supremum of convex functions include the\nderivative of the function at the point where the maximum is attained.\nIn other words, if \\(f(x)=\\sup_{\\alpha\\in\\mathcal{A}}f_\\alpha(x)\\)\nand \\(f_\\alpha(x)\\) is\nconvex in \\(x\\) for every\n\\(\\alpha\\) , then \\(\\partial f_\\beta(x) \\in \\partial f\\)\nif \\(\\beta=\\arg\\sup_{\\alpha\\in\\mathcal{A}}f_\\alpha(x)\\).\nThis is equivalent to computing a gradient descent update for\n\\(p_g\\) at the optimal \\(\\mathcal{D}\\) given the\ncorresponding \\(\\mathcal{G}\\).\n\\(\\sup_\\mathcal{D}U(p_g,\\mathcal{D})\\)\nis convex in \\(p_g\\) with\na unique global optima as proven in Theorem \\(1\\), therefore with sufficiently\nsmall updates of \\(p_g\\), \\(p_g\\) converges to \\(p_x\\), concluding the proof.\nIn practice, adversarial nets represent a limited family of\n\\(p_g\\) distributions via the\nfunction \\(\\mathcal{G}(\\mathbf{z};\\theta_g)\\), and\nwe optimise \\(\\theta_g\\)\nrather than \\(p_g\\)\nitself. Using a multilayer perceptron to define \\(\\mathcal{G}\\) introduces multiple\ncritical points in parameter space. However, the excellent performance\nof multilayer perceptrons in practice suggests that they are a\nreasonable model to use despite their lack of theoretical\nguarantees.\n\n\n\n",
    "preview": {},
    "last_modified": "2025-02-25T22:52:44+01:00",
    "input_file": "generative-adversarial-networks-original-formulation.knit.md"
  },
  {
    "path": "posts/2021-07-22-lotus-theorem-in-original-gans-formulation/",
    "title": "LOTUS Theorem in original GANs formulation.",
    "description": "How do we apply the Law Of The Unconcious Statistician to Ian Goodfellow's original GANs value function formulation?",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2020-12-11",
    "categories": [
      "Computer Vision",
      "Machine Learning"
    ],
    "contents": "\nIn this brief we want to prove a passage of the original GANs Paper\nby Ian Goodfellow. Specifically, we want to prove that the following\nequation is satisfied:\n\\[\n\\int_{\\mathbf{z}}p_{z}(\\mathbf{z})\\log(1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z})))d\\mathbf{z}\n=\n\\int_{\\mathbf{x}}p_{g}(\\mathbf{x})\n\\log(1-\\mathcal{D}(\\mathbf{x}))\nd\\mathbf{x}\n\\] For a continuous random variable \\(\\mathbf{z}\\), let \\(\\mathbf{x} = \\mathcal{G}(\\mathbf{z})\\) and\n\\(\\mathbf{z}=\\mathcal{G}^{-1}(\\mathbf{x})\\)\nsuppose that \\(\\mathcal{G}\\) is\ndifferentiable and that its inverse \\(\\mathcal{G}^{-1}\\) is monotonic. By the\nformula for inverse functions and differentiation we have that \\[\n\\frac{d\\mathbf{z}}{d \\mathbf{x}}\\cdot\n\\frac{d \\mathbf{x}}{d \\mathbf{z}}\n=\n1\n\\] \\[\n\\frac{d\\mathbf{z}}{d \\mathbf{x}}\n=\n\\frac{1}{\\frac{d \\mathbf{x}}{d \\mathbf{z}}}\\\\\n\\] \\[\n\\frac\n{d\\mathbf{z}}\n{d\\mathbf{x}}\n=\n\\frac\n{1}\n{\\frac{d\\mathcal{G}(\\mathcal{G}^{-1}(\\mathbf{x}))}{d(\\mathcal{G}^{-1}(\\mathbf{x}))}}\n\\] \\[\n\\frac\n{d\\mathbf{z}}\n{d\\mathbf{x}}\n=\n\\frac\n{1}\n{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}\n\\] \\[\nd\\mathbf{z}\n=\n\\frac\n{1}\n{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}d\\mathbf{x}\n\\] so that by a change of variables we can rewrite everything in\nfunction of \\(\\mathbf{x}\\), \\[\n\\int_{-\\infty}^{\\infty}\np_{z}(\\mathbf{z})\n\\log\n\\left(\n1-\\mathcal{D}(\\mathcal{G}(\\mathbf{z}))\n\\right)\nd\\mathbf{z}\n=\n\\] \\[\n\\int_{-\\infty}^{\\infty}\np_{z}(\\mathbf{z})\n\\log\n\\left(\n1-\\mathcal{D}(\\mathbf{x})\n\\right)\n\\frac{1}{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}d\\mathbf{x}=\n\\] \\[\n\\int_{-\\infty}^{\\infty}\n\\color{blue}{p_{z}(\\mathbf{z})}\n\\log\n\\left(\n1-\\mathcal{D}(\\mathbf{x})\n\\right)\n\\color{blue}{\n\\frac{1}{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}}d\\mathbf{x}=\n\\] \\[\n\\int_{-\\infty}^{\\infty}\n\\color{blue}{p_{z}(\\mathbf{z})}\n\\color{blue}{\n\\frac{1}{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}}\n\\log\n\\left(\n1-\\mathcal{D}(\\mathbf{x})\n\\right)\nd\\mathbf{x}=\n\\] Now, notice that the cumulative distribution function \\(P_\\mathbf{X}(\\mathbf{x}):\\mathbb{R}^n\\to[0,1]\\)\nis \\(P_\\mathbf{X}(\\mathbf{x})=Pr(\\mathbf{X}<\\mathbf{x})=Pr(X_1\\leq\nx_1,X_2\\leq x_2,\\dots,X_n\\leq x_n)\\), we observe that:\n\\[\nP_\\mathbf{X}(\\mathbf{x})=\n\\] \\[\nPr(\\mathbf{X}<\\mathbf{x})=\n\\] \\[\nPr(\\mathcal{G}(\\mathbf{Z})\\leq\\mathbf{x})=\n\\] \\[\nPr(\\mathbf{Z}\\leq\\mathcal{G}^{-1}(\\mathbf{x}))=\n\\] \\[\nP_{\\mathbf{Z}}(\\mathcal{G}^{-1}(\\mathbf{x}))\n\\] From here \\[\nP_\\mathbf{X}(\\mathbf{x})=P_{\\mathbf{Z}}(\\mathcal{G}^{-1}(\\mathbf{x}))\\\\\nP_\\mathbf{X}(\\mathbf{x})=P_{\\mathbf{Z}}(\\mathbf{z})\n\\] We take the derivative w.r.t \\(\\mathbf{x}\\): \\[\n\\frac{dP_\\mathbf{X}(\\mathbf{x})}{d\\mathbf{x}}\n=\\frac{P_{\\mathbf{Z}}(\\mathbf{z})}{d\\mathbf{x}}\n\\] \\[\n\\frac{dP_\\mathbf{X}(\\mathbf{x})}{d\\mathbf{x}}\n=\\frac{P_{\\mathbf{Z}}(\\mathbf{z})}{d\\mathbf{z}}\\frac{d\\mathbf{z}}{d\\mathbf{x}}\n\\] \\[\np_{X}(\\mathbf{x}) =\np_{\\mathbf{z}}(\\mathbf{z})\\frac{1}{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}\n\\] For clarity we rename \\(p_{X}(\\mathbf{x})\\) as \\(p_{g}(\\mathbf{x})\\) since it represents the\ndistribution learned from our generator \\(\\mathcal{G}\\), we have \\[\n\\color{blue}{p_{g}(\\mathbf{x}) =\np_{\\mathbf{z}}(\\mathbf{z})\\frac{1}{\\mathcal{G}^{'}(\\mathcal{G}^{-1}(\\mathbf{x}))}}\n\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2025-02-25T22:52:54+01:00",
    "input_file": "lotus-theorem-in-original-gans-formulation.knit.md"
  },
  {
    "path": "posts/2021-07-20-a-boring-post-on-linear-regression/",
    "title": "A boring Post on Linear Regression",
    "description": "A visual and mathematical overview of Linear Regression.",
    "author": [
      {
        "name": "Andrea Bonvini",
        "url": "https://github.com/andreabonvini"
      }
    ],
    "date": "2020-09-12",
    "categories": [
      "Machine Learning",
      "Regression"
    ],
    "contents": "\nRegression is one of the main problems tackled by\nmachine learning.\nWe define regression analysis as a set of statistical\nprocesses for estimating the relationship between a dependent variable\n(often called the target variable) and one or more independent\nvariables (often called features). In Linear\nRegression we aim to find the best hyperplane that is able\nto predict the outcome variable starting from a set of\nfeatures.\nOne trivial example could be estimating the price (target\nvariable) of a house starting from the house’s dimension expressed\nin squared meters (\\(m^2\\)). In this\ncase, since we are just considering one feature (the dimension\nof the house expressed in \\(m^2\\)) our\nhyperplane will consist of a simple line.\n\n\nEach dot in the plot corresponds to a real data sample, namely a real\ncorrespondence among area and price. This is how our dataset would look\nlike\n\n\nOur goal consists in finding the line that better explains the data\nwe have been provided with.\n\n\nThis traduces in finding the best weights \\(\\textbf{w} = \\begin{bmatrix}w_0 \\\\\nw_1\\end{bmatrix}\\) such that \\(w_0+w_1\\cdot\\left(\\text{dimension in }m^2\\right)\n\\simeq\\text{Price}\\)\nNow it’s time to introduce some formalism.\nFor the \\(i_{th}\\) data sample we\ncall our target variable \\(t_i\\) , our\nfeatures \\(\\mathbf{x}_i\\) and the\nweights that we apply to our features \\(\\mathbf{w}\\). Note that \\(\\mathbf{x}_i\\) and \\(\\mathbf{w}\\) are column vectors , while\n\\(t_i\\) is just a scalar.\nSuppose we have \\(n\\) data samples\nand we consider \\(m\\) features, then we\ndefine:\nThe target vector\n\\[\n\\mathbf{t} = \\begin{bmatrix}t_1\\\\\nt_2\\\\\n\\vdots\n\\\\\nt_n\n\\end{bmatrix}\n\\]\nThe dataset\n\\[\n\\mathbf{X}=\\begin{bmatrix}\n\\mathbf{x_1}^T\n\\\\\n\\mathbf{x_2}^T\n\\\\\n\\vdots\n\\\\\n\\mathbf{x}_n^T\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 && x_{11} && \\cdots && x_{1m}\n\\\\\n1 && x_{21} && \\cdots && x_{2m}\n\\\\\n\\vdots && \\vdots && \\ddots &&\\vdots\n\\\\ 1 &&\\cdots && \\cdots &&x_{nm}\n\\end{bmatrix}\n\\]\nNote that the \\(1\\) at the beginning\nof each row are just to take in account the bias \\(w_0\\) (that would be the intercept in our\ntrivial example)\nAnd the weights\n\\[\n\\mathbf{w}=\n\\begin{bmatrix}\nw_0\n\\\\\nw_1\n\\\\\nw_2\n\\\\\n\\vdots\n\\\\\nw_m\n\\end{bmatrix}\n\\]\nThen our prediction (that from now on we’ll call \\(y\\) ) for the \\(i_{th}\\) sample will be\n\\[\ny_i = \\mathbf{w}^T\\mathbf{x}_i = \\begin{bmatrix}\nw_0\n&&\nw_1\n&&\nw_2\n&&\n\\cdots\n&&\nw_m\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1\n\\\\\nx_{i1}\n\\\\\nx_{i2}\n\\\\\n\\vdots\n\\\\\nx_{im}\n\\end{bmatrix}\n\\]\nAnd our prediction vector (which will have dimension \\(n\\times 1\\)) will be computed as\n\\[\n\\mathbf{y}=\\begin{bmatrix}\n1 && x_{11} && \\cdots && x_{1m}\n\\\\\n1 && x_{21} && \\cdots && x_{2m}\n\\\\\n\\vdots && \\vdots && \\ddots &&\\vdots\n\\\\\n1 &&\\cdots && \\cdots &&x_{nm}\n\\end{bmatrix}\\cdot\\begin{bmatrix}\nw_0\n\\\\\nw_1\n\\\\\n\\vdots\n\\\\\nw_m\n\\end{bmatrix}=\\mathbf{X}\\mathbf{w}\n\\]\nBut how can we find the optimal weights? We do that by minimizing the\nso-called Mean Squared Error \\(J(\\mathbf{w})\\).\n\\[\nJ(\\mathbf{w}) =\\\\\n\\frac{1}{N}\\left(\\mathbf{t}-\\mathbf{y}\\right)^T\\left(\\mathbf{t}-\\mathbf{y}\\right)=\\\\\\frac{1}{N}\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)^T\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)\n\\]\nWhere \\(\\epsilon_i\\) is just the\ndifference between the target values \\(t_i\\) and our predictions \\(y_i\\),\n\\[\n\\begin{bmatrix}\nt_1-y_1\n\\\\\nt_2-y_2\n\\\\\nt_3-y_3\n\\\\\n\\vdots\n\\\\\nt_n-y_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\epsilon_1\n\\\\\n\\epsilon_2\n\\\\\n\\epsilon_3\n\\\\\n\\vdots\n\\\\\n\\epsilon_n\n\\end{bmatrix}\n\\]\nTo have a visual understanding of what we’re talking about, the\nvarious \\(\\epsilon_i\\) corresponds to\nthe green segments in the image below.\n\nOur cost function is just the Mean Squared Error\n\\[\nJ(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^n\\epsilon_i^2\n\\]\nSince we would like to minimize this quantity, we derive with respect\nto \\(\\mathbf{w}\\) and set the\nderivative equal to \\(0\\) .\n\\[\nJ\\left(\\mathbf{w}\\right) =\n\\frac{1}{N}\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)^T\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)\\\\\\frac{\\partial\nJ(\\mathbf{w})}{\\partial \\mathbf{w}} =\n-\\frac{2}{N}\\mathbf{X}^T\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)\n=0\n\\]\nWhich is equivalent to\n\\[\n\\mathbf{X}^T\\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)=0\n\\]\nWe then isolate the weights\n\\[\n\\mathbf{X}^T\\mathbf{t}=\\mathbf{X}^T\\mathbf{X}\\mathbf{w}\\\\\n\\mathbf{w}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n\\]\nAnd that’s all!\nNow, before tackling the problems relative with such closed form\nsolution, it is useful to introduce the parameters-space,\ni.e. the space representing all the possible solutions \\((w)\\) of our problem: in our trivial\nexample this space corresponds to all the possible points \\(w:(w_0,w_1) \\in \\mathbb{R}^2\\), each of\nthis points traduces in a different predictor (line) in the\nfeatures-space as you can see in the animation below.\nLet’s talk now about some problems that can arise from the closed\nform solution\nThe inverse of \\(\\left(\\mathbf{X}^T\\mathbf{X}\\right)\\) is\ncomputationally expensive when the number of features is high, being the\ntemporal complexity of such inversion \\(\\mathcal{O}(m^3)\\) (Note that \\(\\left(\\mathbf{X}^T\\mathbf{X}\\right)\\) has\ndimensionality equal to \\((m+1)\\times\n(m+1)\\) )\nThe matrix \\(\\left(\\mathbf{X}^T\\mathbf{X}\\right)\\) isn’t\nalways invertible since \\(\\mathbf{X}^T\\mathbf{X}\\) could contain\nlinearly dependent rows, which implies a null determinant. In\nsuch case we’d have infinite solutions for \\(\\mathbf{w}\\).\nIn order to show this last drawback we’ll use a toy example:\nWe have to find a valid predictor for a dataset which contains just\nthree samples.\n\n\nWe solve by means of the closed form solution:\n\\[\n\\mathbf{X}\\mathbf{w}=\\mathbf{t}\n\\]\n\\[\n\\begin{bmatrix}1 && 1\\\\1 && 0\\\\1 &&\n-1\\\\\\end{bmatrix}\\cdot\\begin{bmatrix}\\mathbf{w}_0\\\\\\mathbf{w}_1\\\\\\end{bmatrix}=\\begin{bmatrix}4.1\\\\1.9\\\\0\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n\\]\n\\[\n\\mathbf{w} =\n\\begin{bmatrix}\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 && 1\\\\\n1 && 0\\\\\n1 && -1\\\\\n\\end{bmatrix}\n\\end{bmatrix}\n^{-1}\n\\cdot\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} =\n\\begin{bmatrix}\n3 && 0\\\\\n0 && 2\n\\end{bmatrix}\n^{-1}\n\\cdot\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} =\n\\frac{1}{6}\n\\begin{bmatrix}\n2 && 0\\\\\n0 && 3\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} =\n\\begin{bmatrix}\n\\frac{1}{3} && 0\\\\\n0 && \\frac{1}{2} \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 && 1 && 1\\\\\n1 && 0 && -1\\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} =\n\\begin{bmatrix}\n\\frac{1}{3} &&  \\frac{1}{3}  && \\frac{1}{3}\\\\\n\\frac{1}{2} &&  0  && -\\frac{1}{2}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4.1\\\\\n1.9\\\\\n0\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} =\n\\begin{bmatrix}\n2.00\\\\\n2.05\n\\end{bmatrix}\n\\]\nIn this case \\(\\mathbf{X}^T\\mathbf{X}\\) is invertible and,\nif we plot the cost function \\(J(\\mathbf{w})\\) in the parameter space, we\ncan see that \\(J(\\mathbf{w})\\) is a\nconvex function with one single minimum and a well defined\nbowl shape. This minimum corresponds to the point \\(\\mathbf{w} = \\begin{bmatrix} 2.00\\\\ 2.05\n\\end{bmatrix}\\), i.e. the blue dot which appears at the base of\nthe bowl.\nWhich corresponds to the following predictor line:\n\n\nThe above example is the ideal scenario, but it is not always the one\nyou’ll be dealing with. In fact there are some observations to be done\nregarding the dataset to be used:\nNow let’s compute the closed form solution for another example:\n\\[\n\\begin{bmatrix}m^2 && price\\\\2 && 2\\\\2 && 4\\\\2\n&& 6\\\\\\end{bmatrix}\n\\]\n\n\nHere our data samples are pretty bad since there doesn’t seem to be\nany correlation between the independent variable \\((m^2)\\) and the dependent variable \\((price)\\). We will observe that the\ninversion of the matrix \\(\\mathbf{X}^T\\mathbf{X}\\) becomes\nproblematic.\n\\[\n\\mathbf{X}\\mathbf{w}=\\mathbf{t}\n\\]\n\\[\n\\begin{bmatrix}1 && 2\\\\1 && 2\\\\1 &&\n2\\\\\\end{bmatrix}\\cdot\\begin{bmatrix}\\mathbf{w}_0\\\\\\mathbf{w}_1\\\\\\end{bmatrix}=\\begin{bmatrix}2\\\\4\\\\6\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n\\]\n\\[\n\\mathbf{w} =\\begin{bmatrix}\\begin{bmatrix}1 && 1 && 1\\\\2\n&& 2 && 2\\\\\\end{bmatrix}\\cdot\\begin{bmatrix}1 &&\n2\\\\1 && 2\\\\1 &&\n2\\\\\\end{bmatrix}\\end{bmatrix}^{-1}\\cdot\\begin{bmatrix}1 && 1\n&& 1\\\\2 && 2 &&\n2\\\\\\end{bmatrix}\\begin{bmatrix}2\\\\4\\\\6\\end{bmatrix}\n\\]\n\\[\n\\mathbf{w} = \\begin{bmatrix}  3&& 6\\\\6 &&\n12\\end{bmatrix}^{-1}\\cdot\\begin{bmatrix}1 && 1 && 1\\\\2\n&& 2 &&\n2\\\\\\end{bmatrix}\\cdot\\begin{bmatrix}2\\\\4\\\\6\\end{bmatrix}\n\\]\nAs you can see here we can’t invert \\[\n\\begin{bmatrix}\n3\n&&\n6\n\\\\\n6\n&&\n12\n\\end{bmatrix}\n\\] since its determinant would be \\(0\\) !\nBy plotting the cost function \\(J(\\mathbf{w})\\) we would obtain a sort of\nparabolic cylinder\nwhose minimum has infinite solutions (that we can find by\ngradient-based techniques).\nLastly it is opportune to remember that our dataset needs to have a\nnumber of samples greater than the number of parameters to be learnt.\nBeing \\(m\\) the number of features of\neach sample and \\(n\\) the number of\nsamples in our dataset, we need to satisfy the constraint \\(n>m+1\\). Let’s reason why through a\nsimple example. Suppose that you have been asked to find the model that\nbest explains a dataset made of \\(2\\)\nsamples \\((n=2)\\). Suppose that each\nsample is represented by \\(2\\) features\n\\((m=2)\\) and that we need to predict a\nvalue \\(t\\). In this scenario we are\nnot satisfying the constraint since \\(2\\not>2+1\\). What are the consequences?\nOf course we will get a perfect fit! There are infinite planes \\((w_0+w_1x_1+w_2x_2=0)\\) which pass through\ntwo points!\nWe’ll find at least a model that passes through all the data, more\nspecifically we’d find one solution in the \\(n=m+1\\) case, and infinite solutions in the\n\\(n<m+1\\) case. We have found a very\nbad family of models because they are an exact representation of the\ntraining data, which translates into a very obvious overfitting problem.\nGenerally we want \\(n >> m\\).\n\n\n\n",
    "preview": {},
    "last_modified": "2025-02-25T22:36:18+01:00",
    "input_file": {}
  }
]
